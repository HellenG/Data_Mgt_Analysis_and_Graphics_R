[
["index.html", "Essentials of Data Analysis and Graphics using R Preface", " Essentials of Data Analysis and Graphics using R Hellen Gakuruh 2017-07-14 Preface Welcome to “Essentials of Data Analysis using R”, this is the third and core book in our learning data analysis and graphics tutorial series. In this book we aim to get a good foundation on data analysis; that is, the essential concepts and methods in Statistics. In all our chapters, we will strive to understand the reasoning behind the calculations including assumptions made rather than memorizing formulas. In so doing we get the ability to make sound interpretation of results. In this book, we will begin by learning how to describe data before learning how to make inferences. Learning Outcomes Chapter One: Introduction Chapter Two: Descriptive Statistics Chapter Three: Exploratory Data Analysis (EDA) Chapter Four: Probability Chapter Five: Inferential Statistics Chapter Six: Time Series Analysis Chapter Seven: Survival Analysis Appendix A : Refresher Mathematics It’s a free E-book This book is completely free, but feel free to make any donation towards future book development. "],
["introduction.html", "Chapter 1 Introduction 1.1 Some statistical terms/concepts 1.2 Chapter organization 1.3 Basic Preliquiste", " Chapter 1 Introduction The world today presents us with vast amount of data, most of it generated from everyday activity and others emanating from surveys. However, most of this data is underutilized and we often tend to organize surveys before turning available data into useful information. One of the major reasons as to why most data is underutilized is lack of data analysis skills. Having data analysis skills not only helps to identify sources of data but also convert readily available data into useful information. It’s upon this bases that this book is written; this book is geared towards imparting data analysis concepts building them from a foundation level. Data analysis is often interchanged with statistics, however, a clear distinction exists between the two terms. Data analysis is a broad area which includes statistics; statistics on the other hand is derived from mathematics where it looks at a sample to draw conclusions about a population. In a sense, data analysis can be public health analysis, population analysis, business analysis, social media analysis and so on, in all these types of data analysis, statistics is core, but it is not the only aspect. In this book we take this broad approach by looking at data analysis from different fields or specialization. Our core focus will be statistics but specifically applied statistics (using statistical concepts to understand readily available data). Below we discuss a few statistical concepts we will be using during this book and later give a snapshot of what to expect in each of the chapters. 1.1 Some statistical terms/concepts In this section we go over some of the terms and concepts we will be using. 1.1.1 Variables A variable is an element or a factor that changes (varies) depending on some condition. Variables can be independent or dependent, discrete or continuous, quantitative or qualitative. When conducting a study, a dependent variable is that variable being studied while independent variables are all variables being manipulated by study implementers (for experimental studies) or out of control by study implementers but affect study/dependent variable. For example, in an experiment to measure efficacy of a new drug, independent variables could be other drugs addressing the same condition as the new drug while dependent variable would be efficacy level. It is quite common to see independent variables referred to as predictors, or explanatory variables. Dependent variables are also referred to as response or outcome variables. Variables are classified as discrete if they are numerical variables (based on numbers as opposed to qualitative) and they are count data or their numbers can take on certain values. Example of a discrete variable is number of R packages on CRAN, there is no possibility of having 10.5 packages, it can only be 10 or 11 packages. Continuous variables on the other hand are numerical variables but their values can take on any value withing a range for example shoe size, which can be 6, 6.5, 7 and so on, however, they can not be less than 0 and more than largest recorded foot size (16 or there about). Classification of variables as either quantitative or qualitative has a great influence on type of data analysis methods used. A quantitative variable is one whose values are numerical, for example anthropometric measures like weight and height, number of something like students in a class and so on. Both discrete and continuous variables are quantitative variables. Qualitative variables are variables whose values are not numerical but text-based, these variables measure some “type” or “level” or “quantity”. For example gender type (female or male), area code (as many as area of concern), educational level (high, medium, low), and residency (urban or rural). Qualitative variables are also known as categorical variables. 1.1.2 Measurement Levels A vital concept in data analysis is measurement levels of a dependent variable. Measurement levels are classifications used to indicated type of values a variable can have and therefore inform on type of measurement to be used. There are four measurement levels, these are “nominal”, “ordinal”, “interval” and “ratio”. Nominal level is a qualitative type and variables with this measurement level have categories with no defined/natural order. A good example is gender, in analysis being male or female has no difference and therefore treated same, however, educational level categorized as high medium and low has a natural order and therefore cannot be referred to as nominal. Ordinal level are categorical variables with natural order and each level is treated differently during analysis. Our earlier example on education level is a good example of an ordinal variable as it has ordered levels. Levels have an increasing or decreasing aspect to it and therefore analytical methods chosen have to appreciate this ordering. Note, in these levels, it is easy to say there is a difference between each level, but degree of this difference is not quantifiable, for example, we know medium education level is higher than “low” education level, but we do not know by how much. if we added some numerical value to these levels like 1 for low education, 2 for medium and 3 for high education level, we still cannot tell if the difference between 1 and 2 is the same as difference between 2 and 3. Interval and ratio measurement levels are similar in that they are both numerical scales where each level has the same measured distance from the other. The best way to understand these two measurement levels is to picture them on a number line with each level representing a position on the continuous number line. By this latter fact, we can say both interval and ratio variables are continuous variables. The difference between these two scales is value of zero; for interval scales, zero is arbitrary meaning it does not indicate “lack off/absence” of the variable whereas in ratio scale zero means lack off or absence of that variable. A classic example given for interval scales are temperature measured by degrees Fahrenheit/Celsius, a zero degree does not mean temperature does not exist but if this temperature is measured by Kelvins (unit of measure for temperature based on absolute scale) 1, then zero would mean no temperature and therefore become a ratio scale. 1.1.3 Distributions A very common term in statistics and by extent data analysis is “distribution”. Distribution is a description of relative values of observations an event/trial/experiment. For example, if we were to count the number of laptop types in a classroom, we would end up with relative values such as 10 HP’s, 9 Macs, 5 Dell, 2 Toshiba. Note, these values do not have any relationship with number of these machines in general population, they just represent “laptop distribution in a classroom”. We can also say “distribution” is a listing (or in some case a function as we shall later see), of all possible values and how often they occur. Distributions are often shown using a table or a graph. When we want to table distribution of one variable like number of laptops in a class by make, we use a “frequency table”. Frequency means number of occurrence of a certain observation like number of HP’s. Make Total HP 10 Macs 9 Dell 5 Toshiba 2 When describing distribution of more than one variable, we use contingency tables (also known as cross tabulation or cross tabs). For example, if we were to table number of laptops by make and by gender (how many Female/Males had a certain type of laptop), then we would have something like this: ## gender ## laptops Female Male ## Dell 3 2 ## HP 8 2 ## Mac 5 4 ## Toshiba 0 2 Some of the graphs used to display distributions include dot plot, bar charts, histograms, box plots, tally charts, pie charts and the like. Let’s discuss this graphs as we discuss graphical representation of descriptive summaries. 1.2 Chapter organization In this section we go through what to expect in each of the chapters in this book. Our aim here is to interlink the concepts and see how some concepts rely on knowledge of another. Following this introduction, we shall start off by discussing ways to describe univariate data. Here we shall distinguish between numerical and categorical data. We will conclude this chapter by demonstrating how to display descriptive summaries using R. A chapter on exploratory data analysis (EDA) follows descriptive statistics. This chapter introduces EDA as a technique to quickly look at data. It should be noted, in actual practice, EDA is usually performed as an initial activity and thus this chapter should have come before descriptive statistics. However, since this book also targets complete beginners in data analysis, then discussing EDA (which involves a lot of descriptive statistics) before descriptive statistics would not have been good for novice learners. Before discussing core of statistics (making inference), we will discuss probability as a prerequisite. This chapter will (re)introduce probability concepts like events, sample space, permutations and combinations (counting rules). We shall use classical examples like dice throws, playing cards and marbles an urn to just ensure we have the concepts right. With probability covered, we shall begin our discussion on inferential statistics. In this chapter, we shall discuss a critical concept in inferential statistics which is modeling assumptions (fully, non and semi parametric). We shall also discuss other concepts like central limit theorem and statistical propositions like point and interval estimates (CI). In our discussion we shall distinguish statistical propositions for means and proportions as well as difference between sample statistic and parameters. In addition we shall discuss hypothesis testing highlighting type one and type two errors as well as sensitivity analysis. We shall conclude with techniques for studying relationships between two or more variables(regression analysis). As we wind up this book, we shall have chapters discussing time and survival analysis. These chapters are included as data on time and survival issues are quite common especially in development and humanitarian sector (one of the target learners of this book). 1.3 Basic Preliquiste This book is intended as an introduction to data analysis and graphics, hence discussion is kept simple and concepts introduced gradually. It is however important to have basic knowledge of R (or another statistical program) for you to participate in examples and exercises. Mathematics is an inevitable prerequisite for statistics, and since a large part of data analysis relies on statistics, it is therefore important to have some basic mathematical concepts. It is advisable to go through appendix A on refresher mathematics and see what you need to refresh on. As with the rest of the book, mathematical concepts are re-introduced in a very basic and easy way. There is a list of handy and certainly (beginner friendly) resources added at the end of this chapter in case you need just a little bit more mathematics. Wikipedia↩ "],
["chap2.html", "Chapter 2 Introduction to Descriptive Statistics Goal What we shall cover 2.1 Descriptive Statistics Overview 2.2 Measures of Descriptive Statistics 2.3 Case Study 2.4 Exercises", " Chapter 2 Introduction to Descriptive Statistics Goal The main goal of this chapter is to introduce “Descriptive Statistics” as a foundation for data analysis. What we shall cover By the end of this chapter you should: have an understanding on concept of location/relative standing (quantiles and percentiles), center (mean, median and mode), variability/spread (range, variance and standard deviation), skewness and kurtosis know how to graphically display descriptive summaries as an addition or alternative to displaying numerical summaries 2.1 Descriptive Statistics Overview One of the first tasks a data analyst is tasked to do after a quick Exploratory Data Analysis is to describe variables in a given data. The main aim of this task is to understand information being passed by these variables, this is achieved by computing summaries of each variable and making visual displays. In this regard, when we say descriptive statistics, we mean numbers and graphs used to describe and summarize a given data. There could be many descriptive statistics computed and/or graphed to describe an individual variable, but we often report the most informative descriptive statistic per variable. So what are some of these descriptive statistics? Consider a numerical variable like scores of students in a class room, for this particular variable, what information would be of interest to us? Won’t it be informative to know average scores, how about range between the highest and lowest score, or the percentage of students in the lower or upper bounds (we call these outliers), won’t they be informative. It could also be quite informative to visualize position of each score. Based on this, we would compute some values to give us this information, these values are what we call descriptive statistics of a variable. In our report, we would not include all of these summaries, only those we found to be informative. For example, if we did not have outliers, then we would not report it, we can simply report on the average. We would also not include graphs on individual observation if it did not show an interesting pattern (clustering or presence of outliers). In this chapter, we will go over concepts in descriptive statistics (theoretically) and then follow up with a practical session. Our practical session will involve an actual data analysis of some data set followed up by a demonstration of how to write an analytical report. With that in mind, for our concept building section, we shall discuss two quantiles (percentiles and quartiles ), three measures of central tendency or location (mean, median, and mode), four measures of spread or dispersion (range, inter-quartile range(IQR), variance, and standard deviation) and finally two measures of shape of a data distribution (skewness, kurtosis). 2.2 Measures of Descriptive Statistics In this section we will begin by gaining theoretical knowledge on some of the most informative measures of descriptive statistics, these are: Quantiles: percentiles and quartiles Measures of central tendency: mean, median and mode Measures of spread/dispersion: range, inter-quartile, variance, and standard deviation Measures of distribution shape: skewness and kurtosis In all these measures, we will discuss their numerical and graphical representation and follow-up with a demonstration on how they are computed in R. 2.2.1 Quantiles In the most simplistic terms, quantiles are statistical measures which give values below and above a certain point. There are two commonly used measurements, these are percentiles and quartiles (this term is different from our title quantiles). Quantiles are used to inform on data distribution, for example, we could say 90% of all callers to a customer care center were satisfied with services offered or most students scored between the second quartile (median/50%) and third quartile (75%). Saying this rather actual values or scores can be quite meaningful as we would get a general picture of where an individual value/score lies within a group of observations. In general, we use quantiles when we want to describe an individual value in regards to other values. 2.2.1.1 Percentiles There are quite a number of definitions of percentiles, but the underlining concept behind them is that percentiles give a value below which a given percent of observations occur and the remaining percent of data occur above. To understand this, think of a number line with percentages from 1 to 99 (first value would be 1% and last value would be 99%), a score in the 25th percentile means there are 25% of the observations below it and 75% above it. Twentyfifth percentile With that understanding, suppose we were choosing a statistical program to use for our organization and we are told our preferred program R had a score of 286 out of a possible 300. This is good information but leaves us with a number of questions, top most being, how does 286 compare to scores for other programs. Percentiles can be handy here, but we need the entire data set to get a percentile. Therefore let’s use the following hypothetical data (distribution) to learn how to compute percentiles. 174 287 236 211 156 286 232 188 182 276 229 First thing we want to do is order our data set from lowest to highest value. 156 174 182 188 211 229 232 236 276 286 287 Then we want to compute proportions of each value, that is, get values between 0 and 1 of the same length with our data set. This should give us 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9 and 1. Our percentiles will be these proportions multiplied by 100. We can table this percentile as follows: Score Rank Percentile 156 1 0 174 2 10 182 3 20 188 4 30 211 5 40 229 6 50 232 7 60 236 8 70 276 9 80 286 10 90 287 11 100 From this table we can easily see score of 286 at the 90th percentile. This is certainly much more informative than just saying R scored 286 out of 300. Take note, percentile and percentage are two totally different terms. Saying someone scored 90 percent is not the same as being in the 90th percentile. As an example, there could be a number of scores like 85 to 92 in the 90th percentile but only a score of 90 percent can be 90 percent. Computing percentiles in R As mentioned before, we really do not need to memorize formulas or do manual computations, we just need to understand how to use them and then let statistical programs like R do the computation. In R, to get percentile of any value in a given distribution, we first have to tell R which data we will be using, sort the data and identify index of interested value, get quantiles with function quantile and then subset output of quantiles with index of interested value. For the quantile function, we will input proportions of each value or probability of observing each value. # Data scores &lt;- c(174, 286, 287, 236, 211, 156, 232, 188, 182, 276, 229) # Index of interested score rank &lt;- which(sort(scores) == 286) # Percentiles of all scores p &lt;- quantile(scores, probs = seq(0, 1, length.out = length(scores))) p ## 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% ## 156 174 182 188 211 229 232 236 276 286 287 # Percentile for interested score cat(&quot;\\n&quot;, names(p[rank]), &quot;\\n&quot;) ## ## 90% Please read up on function ?quantile in R to understand available algorithms for computing percentiles, there are nine of them. 2.2.1.2 Quartiles Quartiles (with an “r” not an “n”) are similar to percentiles except that in quartiles we use fractions of the data instead of percentages. That is, both percentiles and quartiles divide data, however, percentiles divide data such that a certain percent of data lie below a give percent and the rest above while quartiles divide data such that a certain fraction of data lie above and the rest below. To understand these two terms better, let’s first get fractions of a data set and then see how they differ from percentiles. To obtain sample fractions of a given data set we begin by ordering the data set or getting ordered statistics. These ordered statistics are the quantiles and their fractions can obtained by computing their proportion (each value divided by variable length minus 1). \\[fractions = \\sum[\\frac{x_i}{(n-1)}]\\] Where: \\(x_i\\) = value \\(n\\) = number of observations For our students scores data, we can compute their fractions as shown in the table: ## Quantile Sample_fraction ## 1 156 0.0 ## 2 174 0.1 ## 3 182 0.2 ## 4 188 0.3 ## 5 211 0.4 ## 6 229 0.5 ## 7 232 0.6 ## 8 236 0.7 ## 9 276 0.8 ## 10 286 0.9 ## 11 287 1.0 Notice our fractions are different from our percentiles: ## Quantile Quartile Percentile ## 1 156 0.0 0 ## 2 174 0.1 10 ## 3 182 0.2 20 ## 4 188 0.3 30 ## 5 211 0.4 40 ## 6 229 0.5 50 ## 7 232 0.6 60 ## 8 236 0.7 70 ## 9 276 0.8 80 ## 10 286 0.9 90 ## 11 287 1.0 100 There are four quarters often reported for a variable, this quarters as the name suggest partition data into four equal parts. There quarters can be quite informative as it can show unique features of the data like data concentration and isolated values at extreme points (outliers). It might not be appropriate to compute quartiles if data is multi-modal (it has more than one data concentration), but let’s discuss this limitation when we are discussing mode under measures of central tendency. There are three cut-off points that divide a data set into four equal parts, these are Q1 (first quartile), Q2 (second quartile), and Q3 (third quartile). Q1 splits the lowest 25% of data from the highest 75% of data, this is the same as the 25th percentile. Q2 splits data into halves, this is the same as the 50th percentile or as we shall discuss later, the median of a distribution. Q3 splits top 25% of data from lower 75% of data, this is the same as the 75th percentile. Quantiles: Percentiles and Quartiles Going back to our scores data set, looking at the fractions for our quantiles (ordered statistics), we cannot find a value where 25% of data are below and 75% are above, we also can’t find a value where 25% of data are above and 75% are below. However we can find a value where 50% are above and 50% are below, this is score 229. We therefore can get Q2 but not Q1 and Q3. To get these missing values we need to use a mathematical concept called linear interpolation. Linear interpolation simply means getting new data point given some values. In our case, the first new point we want is a score that cuts off data such that 25% are above and 75% are below. Looking at our table with quantiles and their fractions, we see 0.25 (25/100) is between 0.2 and 0.3, so we know the score we seek is between 182 and 188. We now need to interpolate this score using these four pieces of information. To interpolate this score, we need to determine type of change as well as rate of this change 2. Change between these two points is an increase as scores increased from 182 to 188 (difference of 6) and fractions increased from 0.2 to 0.3 (difference of 0.1). We can compute rate of increase by dividing change in scores by change in fractions, that is 60. Since the score we seek is between 182 and 188, then we expect rate of increase from score of 182 with fraction of 0.2 to this unknown score with a fraction of 0.25 to be a fraction of 60. This fraction is exactly the difference between 0.25 and 0.2 which is 0.05. So, rate of change from point 0.2,182 to our unknown point is 3, if we add this to 182 we get 185. We can therefore conclude that the score that cuts off values such that 25% are below and 75% are above is 185. Using the same line of reasoning, we can establish that 256 (236 + (0.75 - 0.7) * ((276 - 236)/(0.8 - 0.7))) cuts off values such that 25% are above it and 75% are below it. Let’s look at how to compute these values in R. Getting quartiles in R In R, we can still use function quantiles to get our quartiles, in these case inputting proportions for the three quantiles: quantile(scores, seq(0.25, 0.75, 0.25)) ## 25% 50% 75% ## 185 229 256 We can also get this and other information using function “summary” and “fivenum”. Note function “fivenum” means Tukey’s five number summary, it’s output is unnamed vector, it can be useful for additional computation. summary(scores) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 156.0 185.0 229.0 223.4 256.0 287.0 fivenum(scores) ## [1] 156 185 229 256 287 2.2.1.3 Graphical Display for Quantiles There are about four graphs used to display quantiles, these are: Box plot QQplots Empirical Shift function plots and Symmetry plots On this section we will look at the first two displays. Box plots Box plots or more appropriately box-and-whisker plots are one of the most informative graphical displays for distribution even though they have of late been superseded by fancier displays; their simplicity make them stand test of time. Box-and-whisker plots are best used to show outliers (values occurring at extreme points) and comparing two or more distributions. To draw a box-and-whiskers plot, draw a box from Q1 to Q3 noting Q2 with a vertical line. This box is called Inter-quartile Range (IQR) and it represents 50% of data (75% - 25%). Draw whiskers as lines extending 1.5 times IQR below Q1 and above Q3. Box-and-whiskers plot Value 1.5 is an arbitrary number with no specific meaning behind it, however, it but serves it’s purpose in identifying outliers. Constructing box-and-whisker graphs by hand As an example, suppose we had the following hypothetical values for students scores; ## 80 77 83 64 80 81 75 83 71 86 81 76 68 84 70 24 17 9 20 99 97 To draw a box-and-whiskers plot for this distribution, we first get it’s quartiles: ## 25% 50% 75% ## 68 77 83 Then we compute IQR and whiskers length. We compute whiskers as 1.5 times IQR below Q1 and above Q3. Whiskers are lines extending from both ends of Q1 and Q3, they are referred to as lower and upper whiskers. IQR is computed as the difference between Q3 and Q1. For our hypothetical data set, IQR = 15. Our whiskers are computed as: Lower whisker 1.5 time IQR is 22.5. Subtracting 22.5 from Q1 which was 68 we get 45.5. Our lower whisker will extend from Q1 to 45.5, all values below this are outliers, these are 9, 17, 20 and 24. Upper whisker For the upper whisker, we will add Q3 (83) to 22.5 giving us 105.5. Since we do not have scores above 99, then we will draw our whisker from Q3 to our highest score which is 99. Within this information, we can now draw our box plot. Box plot for scores Using R to plot box-and-whiskers In R, plotting box-and-whisker is just one function call, “boxplot”. boxplot(scores2, col = &quot;grey90&quot;, ylab = &quot;Scores&quot;, pch = 21, bg = 4, horizontal = TRUE) title(&quot;Box plot for student&#39;s scores&quot;) Figure 2.1: Box plot in R Interpreting box-and-whisker plot From our plot, it’s clear to see most students performed well as they clustered around average score of 77, however, there are four students who performed worse than other students. Quantile plots These plots display sample fractions against quartiles they correspond to. To draw these we just need to compute the fractions and plot them. Using our scores data set, we can get the following fractions: ## [1] 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 ## [15] 0.70 0.75 0.80 0.85 0.90 0.95 1.00 We will plot fractions we have just computed on the x-axis and our ordered scores/quartiles on the y-axis. We will plot a line passing though all points. Due to interpolation, drawing this plot by hand might not be a good idea, therefore we will use R. Quantile plots in R There is no function to call for this plot, but since it is a line graph, standard “plot()” should do the trick. plot(quants, sort(scores2), type = &quot;l&quot;, ann = FALSE) title(&quot;Quantile plot in R&quot;, xlab = &quot;Sample fractions&quot;, ylab = &quot;Quartiles&quot;) # Add points to show how linear interpolants points(quants, sort(scores2), pch = 21, bg = 4) Figure 2.2: Q-plot in R 2.2.1.3.1 Quantile-Quantile (QQ) Plots QQ plots are graphical displays for comparing two data sets, these data sets can either be two observations or one observation and one theoretical data set. Quantiles of observation one are plotted against quantiles of observation two/theoretical data set. Patterns of these points are used to Assess whether distributions being compared are similar Compare shapes of distribution Assess goodness of fit As an example, let’s add a second class scores with these values. ## 93 81 75 78 53 70 78 92 76 98 67 76 82 74 61 72 71 93 73 68 93 83 85 96 81 79 76 72 87 75 71 75 96 85 88 We want to compare this distribution with that of our first class. To do this we compute sample fractions of both observations. But before we do that, take note these two classes do not have the same size, class one has 21 scores and class two has 35. Since we want to plot them on the same axis, we need to standardize their axis by taking number of fractions for each sample to be equal to highest value between the two observations. Therefore, our first task is to get highest number between the two observations which is 35 (length of second class), then we compute there fractions. This should give us 0, 0.03, 0.06, 0.09, 0.12, 0.15, 0.18, 0.21, 0.24, 0.26, 0.29, 0.32, 0.35, 0.38, 0.41, 0.44, 0.47, 0.5, 0.53, 0.56, 0.59, 0.62, 0.65, 0.68, 0.71, 0.74, 0.76, 0.79, 0.82, 0.85, 0.88, 0.91, 0.94, 0.97 and 1. Now we can get quantiles for our classes using our computed fractions. Since we are using sample size for the second class (35), then quantiles for second class would simply be ordered statistics of it’s class scores. However, for the first class we need to interpolate their quantiles. We have seen how to interpolate these values, hence we will use R to make our work easier. Using R Let’s compute quantiles for scores of first and second class. # Get number of fractions n &lt;- max(length(scores2), length(scores3)) # Compute quantiles quantileClass1 &lt;- quantile(scores2, seq(0, 1, length.out = n)) cat(&quot;Quantiles for first class:\\n&quot;, quantileClass1, &quot;\\n\\n&quot;) ## Quantiles for first class: ## 9 13.70588 17.52941 19.29412 21.41176 23.76471 45.17647 64.47059 66.82353 68.58824 69.76471 70.47059 71.23529 73.58824 75.23529 75.82353 76.41176 77 78.76471 80 80 80.35294 80.94118 81 81.23529 82.41176 83 83 83.47059 84.11765 85.29412 88.58824 95.05882 97.82353 99 quantileClass2 &lt;- quantile(scores3, seq(0, 1, length.out = n)) cat(&quot;Quantiles for second class:\\n&quot;, quantileClass2) ## Quantiles for second class: ## 53 61 67 68 70 71 71 72 72 73 74 75 75 75 76 76 76 78 78 79 81 81 82 83 85 85 87 88 92 93 93 93 96 96 98 We can now plot these two samples using plot function. But you should know that R has a handy function which we can call with our two distributions and it will do all the calculations and then make QQ plots for us. This function is “qqplot”. Let’s compare qqplots generated using our computations and those from qqplot function. op &lt;- par(&quot;mfrow&quot;) par(mfrow = c(1, 2)) plot(quantileClass1, quantileClass2, ann = FALSE, pch = 21, bg = 4) title(&quot;Computed QQ plot&quot;, xlab = &quot;Class 1&quot;, ylab = &quot;Class 2&quot;) qqplot(scores2, scores3, ann = FALSE, pch = 21, bg = 4) title(&quot;Using &#39;qqplot()&#39;&quot;, xlab = &quot;Class 1&quot;, ylab = &quot;Class 2&quot;) par(op) ## NULL These plots look the same, now we need to interpret it. Interpreting QQ plots There are at least three distribution properties a QQ plot can tell us. This are, skewness, tailness and modality. It should however be noted that distributions with small sample size are not often clear as in our case (sample size of 21 and 35). In general, if points on a QQ plot lie on the line x=y, then the two distributions are said to be similar. If the points form a line but not necessarily lie on x=y, then they are said to be linearly related and generally come from the same probability distribution. We will discuss probability distributions and their implications in chapter three. Given this information, let’s add x=y line to our plot. qqplot(x = scores2, y = scores3, xlab = &quot;Class 1&quot;, ylab = &quot;Class 2&quot;, main = &quot;QQ plot in R&quot;, pch = 21, bg = 4) lines(x = 1:99, y = 1:99) To draw x=y line we used function line parsing to it values forming x=y. From this line we know that scores of class one and class two do not form a linear relationship. We can thus conclude they do not have similar distributions. Though not to clear (due to small sample size), there seems to be a bi-modal (two peaks) given the fact that we see a sort of “s” shape. These two peaks are concentration of points at point 20,70 and 80,80. Something we can see from our graph are tailness or isolated values at extreme point, this could be an indicator of outliers (values away from expected). It is useful to note QQ plots are not reported, they are more applicable as an Exploratory Data Analysis technique (an analysts tool so to say), that is, they are more suitable in guiding data analysis rather than being a finding to be reported. We shall revisit QQ plots when discuss probability distribution, at that point we would have discussed some of the issues we have mentioned like skewness and modality. 2.2.2 Measures of Central Tendency To best understand measures of central tendency or location, think of our first example on scores, we wanted to make an informed select of one statistical program among a number of programs. To this end we were told our preferred program, R, scored 286 out of 300. From our discussion on quantiles we discovered that this meant it was in the 90th percentile. That’s certainly good information, however, if you are an astute analyst, then you would want to know where the other programs are located in the distribution. More specifically, you would want to know distance of 286 from center of the distribution. Measure of central tendency is the answer to this. They summarize data to a single useful and representative information. There are three commonly used measures of central tendency, these are “mean”, “median” and “mode”. In this section we get to look at each one of them while noting their applicability. 2.2.2.1 Mean Mean and specifically arithmetic mean indicates center of a distribution. It’s computed as sum of all values divided by number of values. So, if you have a variable, mean is the summation of all values in that variable divided by number of elements in that variable. Mean is more appropriate for numerical variables (discrete 3 and continuous variables 4), but not qualitative or categorical data. 2.2.2.1.1 Mean for numeric data Going back to our first example on scores on statistical programs, we compute mean as total of all values divided number of all values, that is, sum of divided by 11, giving us 223.3636364. Mathematical notation Based on the notion that mean is the sum of all values divided by number of values, then, given values x1, x2, x3, …, Xn, mean is: \\[\\frac{x_1 + x_2 + x_3 + ... + x_n}{n}\\] This is mathematically expressed as: \\[\\bar{x} = \\frac{\\sum {x_1, x_2, x_3, ..., x_n}}{n}\\] Where: \\(\\bar{x}\\) is Sample mean \\(\\sum\\) is Greek capital letter sigma meaning “sum of” \\(n\\) is sample size This mathematical expression is often reduced to: \\[\\bar{x} = \\frac{\\sum{x}}{n}\\] In statistics, it’s important to distinguish between population parameters 5 and sample statistics 6. Mathematical expression given above is a sample statistic, if we were dealing with entire population, then population mean would be given by: \\[\\mu = \\frac{\\sum{X}}{N}\\] Where: \\(\\mu\\) is population mean \\(X\\) are observations \\(N\\) is population size Computing Mean in R Getting mean in R is just one function call, we use function “mean”. mean(scores2) ## [1] 67.85714 Do take note, if data contains missing values or NA’s, you need to tell R by setting argument “na.rm” to TRUE, otherwise output would be NA. Univariate Frequency Distributions When we have a discrete variable with few unique values or continuous variable with known ranges, then it useful to convert them grouped data. Grouping data involves categorization or batching together observations. Grouping not only helps describe similar observations but it also helps to see underlying distribution like average, spread, skewness, modality or peakness, and extreme or isolated values. Grouped data is often presented in frequency tables. This table can be used for grouped and ungrouped data. Ungrouped data are often unique vales of a discrete variable. Frequency tables are also called frequency distributions as they tabulate frequencies along side their corresponding observation. Frequency is the number of times an observation occurs. With that understanding, let’s look at two examples of frequency distributions, one will be for ungrouped data and the other for grouped data. For our first example on ungrouped data, let’s consider the following data set, it’s a list of responses to a question asked to analyst on how many times they have used R in the last week. {0, 0, 1, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5} From this data, we can see there are a number of repetitive values or few distinct values. Based on this fact, we can summarize these data by counting number of occurrence of each unique value (0, 1, 2, 3, 4 and 5) and tabulate them as follows. Usage Frequency 0 2 1 1 2 1 3 6 4 11 5 9 What we have just created is an ungrouped frequency table. Now let’s look at grouped frequency distributions. Suppose we have the following data on number of years some of the most popular programs have been in existence: {1, 4, 6, 7, 7, 8, 9, 12, 12, 12, 13, 15, 15, 16, 17, 17, 18, 19, 19, 19, 20, 20, 21, 21, 22, 22, 23, 23, 24, 25} There are few terms or concepts we need to appreciate as we construct frequency tables for grouped distributions. These are: Class: Range of values like “1-5” or “6-7”. These can also be considered as sub-set of a data distribution. Class size: It is the number of values in a class, for example a class of “1-5” has 5 values 1, 2, 3, 4, and 5. Class limits: These are the minimum and maximum values of a class, for example 1 and 5 for class “1-5”. These values can be specified as upper and lower limits. Class boundaries: These are also called true class limits and computed as an average of sum of lower limit of one class and upper limit of a subsequent class. As an example, if we have three classes “1-5”, “6-10” and “11-15”, we can compute class boundaries for the first two classes as (5 + 6)/2 = 5.5 and (10 + 11)/2 = 10.5. Notice, we are adding half a point to each upper boundary, therefore, we can just do the same to other classes beginning from before first class and ending right after last class; that is 0.5, 5.5, 10.5, and 15.5. Class width/interval: These are difference between upper and lower boundaries of any class for example 5 which is 5.5 - 0.5. It’s also the lower limits of two consecutive classes or the upper limits of two classes like 5 which results from 6 - 1. Class mark/midpoint: This is the middle value in a class. It’s computed as an average of upper and lower limits of a class or difference of upper and lower boundaries. When constructing a frequency distribution table there are a few issues that need to be agreed on, these include number of classes and class width. It’s important to ensure we do not have too many or too few classes as it will obscure certain feature of our distribution or make it hard for us to interpret frequency distribution. It’s also important for us to consider class width as we do not want to end up with too many empty classes than necessary. There are couple of formulas out there on estimating class size/width, there are also those that recommend class sizes of either 2, 5, or 10. I suggest using the latter recommendation but guided by data. For example, for our data which has values from 1 to 25, we might want to have a class width of 5 thereby having a total of 5 classes. Taking classes of width 2 might make our frequency distribution too big as we would have about 12 classes and some leftover. Having class width of 10 on the other hand would mean having only two or three classes, this might be a bit small. So our 5 classes seams ideal. Now that we know how many classes we will have and their width, we can construct our classes bearing in mind that they need to be unique (a value can only have one possible class it belongs to). Based on this we can have the following classes and number of observations that fall in those classes. Years Frequency 1 - 5 2 6 - 10 5 11- 15 6 16- 20 9 21- 25 8 What we have above is a grouped frequency distribution table. With this brief introduction to frequency distributions, let’s now see how to compute their descriptive statistics. 2.2.2.1.2 Mean for frequency distributions In this section we will look at how to compute averages for ungrouped and grouped distributions. 2.2.2.1.3 Mean for ungrouped distributions To learn how to compute mean for ungrouped distributions, let’s build up from our understanding of mean for non-frequency distribution. For these non-frequency distributions, we defined mean as sum of all values divided by number of values. Now, for ungrouped mean, we need to begin by reconstructing number of values by multiplying observations by their frequencies, and then sum them up before dividing by number of values or total frequencies. As an example, let’s revisit our data on responses from analyst. Usage Frequency 0 2 1 1 2 1 3 6 4 11 5 9 We compute mean for this data by multiply each usage (observation/value) with it’s frequency, then sum them up and finally divide by number of responses (number of analysts); that is, \\[\\frac{(0*2) + (1*1) + (2*1) + (3*6) + (4*11) + (5*9)}{2+1+1+6+11+9}\\] We should get mean as 3.6666667. Based on this finding we can conclude that, on average analysts in our organization used R about 3.7 times last week. We can mathematically express this computation as: \\[\\bar{x} = \\frac{{\\sum\\limits^{n}_{i=1}}{f_ix_i}}{\\sum\\limits^{n}_{i=1}{f_i}}\\] Where: n = number of unique observations or number of rows in frequency table f = frequency x = an observation in a frequency table or simply as: \\[\\bar{x} = \\frac{\\sum{fx}}{\\sum{f}}\\] Computing mean for ungrouped distribution in R In R, we can compute mean for distribution with “mean” function. To generate a frequency table we use function “table”. table() does not produce a very presentable table, so we will transform it into a data frame with “as.data.frame” function thereby giving us a table similar to what we manually constructed. ## [1] 3.666667 ## ungpd1 ## 0 1 2 3 4 5 ## 2 1 1 6 11 9 ## Usage Freq ## 1 0 2 ## 2 1 1 ## 3 2 1 ## 4 3 6 ## 5 4 11 ## 6 5 9 2.2.2.1.3.1 Mean for grouped distributions Let’s use our second example on frequency distribution to compute mean for a grouped distribution. This was our distribution: Years Frequency 1 - 5 2 6 - 10 5 11- 15 6 16- 20 9 21- 25 8 We have just discussed mean for ungrouped mean as summation of products of observations and frequencies divided by total frequency. We are going to use this definition with a slight amendment and that’s what we consider to be our observations. When we were dealing with ungrouped data it was easy for us to recreate our original values by multiplying observations by their frequencies, however, for grouped distributions we can’t do this. This is because we can simply not know exact value of any frequency within a class (range of values). So what we can do is go for the next best thing which is an estimate. This estimate is a class midpoint or a class mark. Once we have these mid points, then we can compute mean just as we did with ungrouped data. To show each computation, let’s use our frequency table and add column on midpoint and product of midpoints and frequencies. Years Midpoints Frequencies Product 1 - 5 3 2 6 6 - 10 8 5 40 11 - 15 13 6 78 16 - 20 18 9 162 21 - 25 23 8 184 Total 30 470 Mean for this distribution is thus 470 divided by 30 which is 15.6666667. We can therefore conclude that average number of years statistical packages have been in existence is about 15.7 years. We can use this average with number of years R has been in existence which is 24 (from 1993 to 2017); looks like R has some mileage over most programs (hypothetically speaking). There two things we need to appreciate as we conclude this section on mean for grouped data, these are; Mean for grouped data is an estimate: Unlike mean for ungrouped distributions, mean for grouped data is an approximation as it uses midpoints rather than actual values/observations. It is therefore important to collect responses with ungrouped values as it is easier to group observations during data analysis than it is to reconstruct actual values from classes. Don’t use mean for frequency distributions with open groups: Open groups like “15+” or “65 and above”, should use mode as a measure of central tendency. This is because it is not possible to compute midpoint for an infinite class. Computing grouped mean in R Unfortunately there is no one function for calculating grouped mean in R so we have to go through a number of steps to compute this mean. # Data years &lt;- factor(c(&quot;0-5&quot;, &quot;6-10&quot;, &quot;11-15&quot;, &quot;16-20&quot;, &quot;21-25&quot;), ordered = TRUE) freq &lt;- c(2L, 5L, 6L, 9L, 8L) gpd1 &lt;- data.frame(Years = years, Freq = freq) gpd1 ## Years Freq ## 1 0-5 2 ## 2 6-10 5 ## 3 11-15 6 ## 4 16-20 9 ## 5 21-25 8 # Number of observations n &lt;- sum(gpd1$Freq) # Midpoint midpoint &lt;- c((5+0)/2, (10+6)/2, (15+11)/2, (20+16)/2, (25+21)/2) gpd1[3] &lt;- midpoint names(gpd1)[3] &lt;- &quot;Midpoint&quot; gpd1 ## Years Freq Midpoint ## 1 0-5 2 2.5 ## 2 6-10 5 8.0 ## 3 11-15 6 13.0 ## 4 16-20 9 18.0 ## 5 21-25 8 23.0 # Product of midpoints and frequency gpd1[4] &lt;- gpd1$Freq * gpd1$Midpoint names(gpd1)[4] &lt;- &quot;Product&quot; gpd1 ## Years Freq Midpoint Product ## 1 0-5 2 2.5 5 ## 2 6-10 5 8.0 40 ## 3 11-15 6 13.0 78 ## 4 16-20 9 18.0 162 ## 5 21-25 8 23.0 184 # Mean sum(gpd1$Product)/n ## [1] 15.63333 2.2.2.2 Median Median is basically the middle observation in an ordered distribution. To get this middle value, we have to determine if distribution has an even or an odd number of observations. 2.2.2.2.1 Median for odd numbered distributions For odd numbered observations, middle value is rather easy to locate, it is that value which splits a distribution such that there are equal number of values before it and after it. For example, a distribution with 21 observation would have the eleventh observation as it’s median since there ten values before it and another ten after it. Basically , median for an odd numbered distribution is number of observations divided by two and then raised to the nearest whole like 21/2 = 10.5, 10.5 to nearest whole is 11. Using this reasoning we can generate our own formula for computing median for odd numbered distribution: \\[Median_{(odd)} = data[round(\\frac{n}{2})]\\] Where: data = distribution [] = subset notation round = raise number to next whole number (digits = 0) n = number of observation in distribution Now let’s get median for our data on scores for statistical programming languages. First we order our data from the lowest value to the highest value: ## 156 174 182 188 211 229 232 236 276 286 287 Since number of elements in this data set is odd (11), we can use our formula that is, median is round(11/2) which is,229. Computing median in R In R, median for numerical distribution is one function call whether it’s an odd numbered distribution or even. median(scores) ## [1] 229 2.2.2.2.2 Median for even numbered distributions For even numbered distributions, median is an average of the two middle values. For example, a distribution with 20 observations would have it’s median as an average of the tenth and eleventh observation. To get these two middle values, we get half the number of distribution like 20/2 and half the number of distribution plus one like (20/2 + 1). As before, we can generate our own formula for computing median for even numbered distribution as: \\[Median_{(even)} = \\frac{{data[\\frac{n}{2}]+data[\\frac{n}{2}+1]}}{2}\\] Where: data = distribution [] = subset n = number of observation in the distribution Now, using our scores data set, let’s add a score of 234 to make an even numbered distribution. This is how it looks when ordered: ## 156 174 182 188 211 229 232 234 236 276 286 287 Our data now has twelve values, using our derived formula, we can compute median as \\[Median_{(even)} = \\frac{scores[\\frac{12}{2}]+scores[\\frac{12}{2}+1]}{2}\\] This should output 230.5. Another way to look at median for even numbered distribution is number of distribution (n) plus one divided by two. \\[Median_{(even)} = \\frac{n + 1}{2}\\] Above formula is certainly simpler but not as intuitive as our formula. 2.2.2.2.3 Median for frequency distributions Like median for non-frequency distribution, computation for median for frequency distributions depends on whether total frequency is odd or even. Since we now know difference between median for odd and even numbered distribution, in this section we will focus on getting to locate median of frequency distribution by using our two data sets on responses from analysts and years statistical programs have been in existence. By and large, median for ungrouped and grouped distributions go through the same processes. We first determine if we are dealing with an odd or an even numbered distribution by getting sum of all frequencies, then using appropriate formula, compute location of median, and finally identify observation or class containing the median by cummulating frequencies. Let’s see how this actually works. 2.2.2.2.3.1 Ungrouped distributions Using our data analyst response data, let’s determine its median. We begin by finding out if it’s an odd or even numbered distribution by summing frequencies (2, 1, 1, 6, 11, and 9). This should give us 30, an even number. Since it’s an even number we will use our second formula to locate position of our median. Median is the observation at position 15.5 ((30+1)/2). To identify observation at this position we need to generate cumulative frequencies and the best way to do this is to add a column to our frequency distribution table. Usage Frequency Cumulative frequency 0 2 2 1 1 3 2 1 4 3 6 10 4 11 21 5 9 30 From our cumulative frequencies, we can see 15.5 is in the fourth observation, hence median is 4. Locating median for ungrouped distributions in R Median for ungrouped distributions is computed the same way as non-frequency distributions, using function “median”. ungpd1 ## [1] 0 0 1 2 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 median(ungpd1) ## [1] 4 2.2.2.2.3.2 Grouped distributions Median for grouped distribution is exactly the same as that of ungrouped distributions. That is, we begin by identifying whether we have an odd or an even number of distribution, compute location of our median and finally identify class with that position. Our total frequency is 30, same as before so we know we are looking for a class with the fifteen point five observation. We generate cumulative frequencies. Years Frequencies Cumulative frequency 1 - 5 2 2 6 - 10 5 7 11 - 15 6 13 16 - 20 9 22 21 - 25 8 30 From these (cumulative frequencies) we find 15.5 is in the fourth class, hence median class is “16-20”. Locating median for grouped distribution in R Base R does not have a function to compute median for grouped data, but it is not hard to compute it. For odd number of distribution, we can get median of our distribution’s indices, round it up and subset this value from our data. For even number, we can simply get median of our data. # Median for odd numbered grouped distribution dat &lt;- rep(as.character(years), freq) # Generate data dat[round(median(seq_along(dat)))] ## [1] &quot;16-20&quot; # Median for even numbered grouped distribution dat[length(dat)+1] &lt;- &quot;0-5&quot; # Add a value to make distribution even median(dat) ## [1] &quot;16-20&quot; 2.2.2.3 Mode Mode is the most frequently occurring value or category. This is the only measure of central tendency suitable for categorical or qualitative data. This is also the only measure of central tendency which could have more than one value or none at all. A distribution can have no mode in which case there are repeating/uniform observations, or have one mode thus called “unimodal”, or two modes thus called “bimodal” or more than two modes thus called “multimodal”. 2.2.2.3.1 Mode: numerical distributions For discrete distributions, to get the most frequently occurring value we need to generate frequencies and then determine which observation has the highest frequency. For continuous distributions, we need to group/categorize observations, we will discuss these distributions in our section on grouped distributions. As an example of discrete distributions, let’s look at situations where we do not have a mode, have one mode (unimodal), two modes (bimodal) and where we have more than two modes (multimodal). Uniform Distribution Uniform distributions have no mode which means all observations are equal. Here is an example of a data set with no mode. ## 65 65 65 65 65 66 66 66 66 66 67 67 67 67 67 68 68 68 68 68 69 69 69 69 69 70 70 70 70 70 We can establish lack of mode using a frequency table. Value Frequency 65 5 66 5 67 5 68 5 69 5 70 5 All observations have the same frequency. Unimodal distributions Unimodal distributions have one peek or one most frequently occurring observation. For example, the following distribution: ## 64 65 67 66 64 66 65 65 64 68 66 65 67 64 65 66 64 65 65 64 65 66 65 65 64 64 67 65 64 65 64 66 66 65 66 64 64 63 66 66 67 63 65 66 65 65 66 66 65 65 66 66 66 65 64 64 68 66 64 65 65 66 65 67 65 65 65 66 66 65 66 64 65 65 64 66 63 66 65 64 66 66 64 66 65 65 66 66 64 63 66 67 64 64 64 68 65 64 66 65 We can create the following frequency distribution Values Frequency 63 4 64 24 65 33 66 30 67 6 68 3 From this table, it is clear to see the most frequently occurring value is 65 as it has the highest number of observations (frequency of 33). Bimodal distributions Bimodal distributions have exactly two modes. That is, they have two most frequently occurring value. We can see this from the following distribution ## 39 39 40 40 40 40 40 40 41 41 41 68 69 69 69 70 70 70 70 70 70 71 71 This distribution has the following frequency distribution: Value Frequency 39 2 40 6 41 3 68 1 69 3 70 6 71 2 The two modes in this distribution are 40 and 70 as each has six observations. Multimodal distributions Multimodal distributions have more than two modes, here is an example of a distribution with three modes. multimodal &lt;- c(bimodal, rep(72, 6)) cat(multimodal) ## 39 39 40 40 40 40 40 40 41 41 41 68 69 69 69 70 70 70 70 70 70 71 71 72 72 72 72 72 72 From the following frequency distribution, we can see there are three modes, 40, 70 and 72. Value Frequency 39 2 40 6 41 3 68 1 69 3 70 6 71 2 72 6 Getting mode of a distribution in R R does not have a function to get statistical mode, the “mode” function in R is used to do something else (get internal storage type). However, getting this value is rather easy with knowledge of what mode is. To get mode we table (make a frequency table) our values, find maximum frequency using function “which.max” and return value using “name” function. # Data set.seed(583) mode1 &lt;- round(rnorm(100, 65)) # Frequency table table(mode1) ## mode1 ## 63 64 65 66 67 68 ## 4 24 33 30 6 3 # Mode names(which.max(table(mode1))) ## [1] &quot;65&quot; 2.2.2.3.2 Mode: Frequecy distribitions In this section we discuss how to get mode for grouped and ungrouped distribution. 2.2.2.3.2.1 Mode: ungrouped distributions Getting mode for this distribution is the same as getting mode for non-frequency distribution, actually even easier since we have frequencies, we only need to determine which is the highest frequency. Using our data analyst responses, we can get mode as 4 since it had the highest frequency (11). In R we only need to use function “which.max” to get mode. # Frequency table (as a dataframe) ungpd1Tab ## Usage Freq ## 1 0 2 ## 2 1 1 ## 3 2 1 ## 4 3 6 ## 5 4 11 ## 6 5 9 # Mode ungpd1Tab[which.max(ungpd1Tab$Freq), 1] ## [1] 4 ## Levels: 0 1 2 3 4 5 2.2.2.3.2.2 Mode: Grouped distributions Mode for grouped data is applicable for both categorical distributions as well as continuous distributions. For continuous distributions, categories/groups need to be constructed first. From these groups we can get modal class the same way we got mode for ungrouped frequency, that is, identify class with highest frequency. For our data on number of years statistical programs have been in existence, we can easily locate mode as the fourth class “16-20” which has 9 observations. Locating mode for grouped distribution in R Using R, we again use function “which.max” and subset class (years). gpd1$Years[which.max(gpd1$Freq)] ## [1] 16-20 ## Levels: 0-5 &lt; 11-15 &lt; 16-20 &lt; 21-25 &lt; 6-10 Here is a function for determining modal class given a continuous distribution and breaks. Breaks are cutoff points to which a distribution will be grouped. mode_grouped &lt;- function(x, breaks, class = TRUE) { n &lt;- length(breaks) nms &lt;- sapply(1:(n-1), function(i) paste(breaks[i], &quot;-&quot;, breaks[i+1])) freq &lt;- lapply(1:(n-1), function(i) which(x &gt;= breaks[i] &amp; x &lt; breaks[i+1])) names(freq) &lt;- nms if (class) { names(which.max(sapply(freq, length))) } else { which.max(sapply(freq, length)) } } 2.2.2.4 Comparison of measures of central tendecy We have just concluded a good discussion on measures of central tendency. From it we can compute mean, median and mode of any numeric and frequency distribution. This is certainly great, but do we need to report on all of them? Certainly not, each measure has it’s own merits and demerits as well as it’s applicability. Let’s discuss these aspects. Central tendency for qualitative distributions When dealing with qualitative or grouped data, mode is the most appropriate measure of central tendency. Reason, think of a variable such as educational level with high, medium and low, would it make sense to say average level of education is 10.6, what would ten mean and more specifically, what would a point six indicate? Won’t it be more informative to hear “most respondents have high education”. Mean and median When data has some extreme values, median is more appropriate than mean. Basic reason for this is that mean uses all values in a distribution while median uses positions of these values. Think of it this way, you can’t compute mean without knowing values in a distribution but you can say where median value is located by just knowing how many values a distribution has. For example, in the following distribution, we have eleven values from 53 to 64. This distribution does not have extreme values. ## 59 59 60 60 61 61 61 61 62 62 62 Mean for this distribution is 60.7272727 and median is 61, a difference of 0.2727273. Difference between mean and median is small and any can be used to report centrality of distribution but most analysts in this case would report on mean. Now let’s add just one extreme value (2) and assess it’s impact on mean and median. ## 2 59 59 60 60 61 61 61 61 62 62 62 Now our distribution begins from 2 not 53. When we compute mean we get 55.8333333 and when we compute median we get 61, a difference of 5.1666667. For this distribution, which average would be more appropriate, mean of about 56 or median of about 61? Comparison of mean and median with extreme value As shown in the figure above, mean would certainly not be an accurate measure of centrality for this distribution, it is influenced by an extreme value (2). Therefore, when reporting averages for distributions with extreme values, it is meaningful to report median as opposed to mean. 2.2.2.5 Summary - Mean, Median, Mode Mean is also called average, it is computed as sum of all values divided by number of values. Median is center of a distribution; the value at the middle of a distribution when it is arranged in order. Mode is the most frequently occurring value in a distribution. Mean, median and mode provide us with a descriptive value for our distribution what we would call a representative value. All three measures can be used to describe numerical distributions (discrete and continuous), but mean and median are more appropriate. When a numerical distribution has extreme values, it is best to use median as a representative value. For any distribution, there can only be one mean and one median, but there could zero, one (uni-modal), two (bimodal) or more modes (multimodal). When writing a data analysis report we only report the most appropriate statistic. Sometimes it can be quite informative to present data in frequency tables. These are tables summarizing observations with how many time they occur (frequency). Frequency distributions can either be ungrouped or grouped. Grouped means observations have been categorized or bundled together in form of classes. These classes comprise range of values, how these classes are constructed is important and some considerations must be made. It’s important as analysts to always try to collect data as ungrouped distribution as it is easier to construct groups pre-analysis rather than reconstruct values during analysis. Grouped statistics are estimates and with all estimates, they are not as accurate. 2.2.3 Measures of dispersion (variablity/spread) Having established a representative value for our data, the next thing we may wish to know is how spread out our data points are and more specifically how far away they are from our representative value. Why you might ask, well, think of our first data set with scores on statistical program’s performance. We found out our preferred program R with a score of 286 is in the 90th percentile, that meant it had performed better than 90% of the programs. We also have a reported average of about 223 which tells us R is way ahead of average score. This is certainly informative, but as analyst we are abound to want more information. For example we might want to know where the other scores are located, are they clustered around our average score or are they dispersed. Another question we might have is, since 286 is way above mean of 223, could it be an extreme value and what was the minimum and maximum score. Now this is where measure of dispersion comes in. Other than answering these questions, measures of dispersion can be used to compare distributions with similar measures of central tendency. For example, two distributions can have the same mean but one distribution can have it’s values more spread out from the mean than the other distribution. When we say dispersion, we mean how spread out our values are in the distribution. Some of the statistics used to measure dispersion are range, inter-quartile range and standard deviation. In this section we discuss how to compute these measures of dispersion. 2.2.3.1 Range Range is the simplest measure of dispersion, it is described as the difference between minimum and maximum value. This difference is often reported along side these minimum and maximum values. In general, range tells us how spread out our values are. For example, to compute range for our data on scores for statistical programs, we begin by arranging values from lowest to the highest value. From this ordered distribution we subset first and last value which become our minimum and maximum values. Range is thus their difference. We can therefore establish maximum score as 287 and minimum score as 156 there difference is 131. When reporting this range, we can do it this way 131 (156 - 287). Computing range in R In R, we get range using function “range”, it’s output is a vector with minimum and maximum values. To get difference between the two, use function “diff”. # Minumum and maximum values range(scores) ## [1] 156 287 # Range diff(range(scores)) ## [1] 131 Range of 131 (156 - 287) out of a possible range of 300 (0 - 300) (possibility of zero to maximum score) tells us scores are not quite spread out. In comparison to R’s score of 286, we can now say R is almost the highest rated program. In general, range is useful when we want to look at an entire distribution. It is especially informative when comparing dispersion of two or more distribution. For example, we could look at range of two or more class scores to see how well each class did. Range however is influenced by extreme values, extreme values increase range of a distribution. Take for instance a distribution with values 2, 55 and 65, range for this distribution is 63 (2, 65), this dispersion is wider because of value 2, if we excluded it we would get a range of 10 (55, 65). Because of this shortfall, inter-quartile range are often reported. 2.2.3.2 Inter-quartile Range Inter-quartile range (IQR) is a range of where 50% of a distribution lie. For distributions with outliers, it is best to report it’s IQR than it’s range. IQR is simply third quartile minus first quartile (Q3 - Q1). Q1 is the 25th percentile or location below which 25% of values lie and Q3 is the 75th percentile or location above which 25% of data lie. We have discussed how to get quartiles as well as how to show interquartile range when constructing box plots. Here let’s discuss how IQR can inform us on spread of a distribution. From our scores on statistical programs, we can compute Q3 as 256 and Q1 as 185 therefore IQR is 71. So we can report 50% of our data lie between 185 and 256. We can also show this using a box plot. boxplot(scores, horizontal = TRUE, col = &quot;grey90&quot;) title(&quot;Distribution of scores&quot;, xlab = &quot;scores&quot;) Figure 2.3: Scores distribution From this box plot, we can also establish there are no outliers. Note, IQR is a range and not a scalar (single value), hence it’s best to report both range and difference like “131 (156 - 256)” or simply the range alone “156, 256” but not “131”, it’s simply not communicative. Like range, IQR is not the best measure of dispersion as it does not take into account other values in a distribution other than quartiles, minimum and maximum values. A more robust measure of dispersion is standard deviation. 2.2.3.3 Variance and Standard Deviation One of the questions we sort to find out when we started this section on measures of dispersion is how far our data points are from the mean, to answer this question we use standard deviation which is derived from variance. Quite simply, standard deviation is the average distance of values of a distribution from it’s mean. The greater this distance, the greater the standard deviation and it’s dispersion. The shorter this distance, the short is standard deviation and it’s dispersion. In general, standard deviation try’s to establish an indicator of where each data point is from the mean. Hence we compute standard deviation (which we shall refer to as “sd”) as summation of difference of each data point from it’s mean divided by total number of values (basically mean of the distances from the mean). But before we sum these differences, we need to take note that summing deviations from the mean would result to zero since values above the mean would cancel values below the mean. To overcome this, we need to turn negative deviations to positive values. Two way to turn these values to non-negative values is by getting absolutes or squares. Squaring is preferred as it has some mathematical properties which are easier to work with, hence, before summing deviations we need to square them first. Once we take an average of the sum of squared differences, we would have computed a measure of dispersion called “variance”. Variance is therefore the average/mean sum of squared deviations. Variance is a squared deviation of data points from the mean, but what we need are the deviations. To get these deviations we take square root of variance which becomes our standard deviation. Using our score’s data set, let’s go step by step to compute variance and standard deviation before seeing how easy it is to get both statistics in R. Step 1: Compute mean We compute mean as 223.3636364. Step 2: Get squares difference from the mean Our squared difference are: cat(paste(paste0(&quot;(&quot;, scores, &quot;-&quot;, round(mean(scores), 2), &quot;)^2&quot;), &quot;=&quot;, round((scores - mean(scores))^2, 2), collapse = &quot;\\n&quot;)) ## (174-223.36)^2 = 2436.77 ## (286-223.36)^2 = 3923.31 ## (287-223.36)^2 = 4049.59 ## (236-223.36)^2 = 159.68 ## (211-223.36)^2 = 152.86 ## (156-223.36)^2 = 4537.86 ## (232-223.36)^2 = 74.59 ## (188-223.36)^2 = 1250.59 ## (182-223.36)^2 = 1710.95 ## (276-223.36)^2 = 2770.59 ## (229-223.36)^2 = 31.77 Step 3: Compute variance (average sum of squared deviations) Sum of squared deviation is 2.109854510^{4} and average sum of squared deviations is 1918.0495868. Step 4: Compute standard deviations Standard deviation is simply square root of variance which is 43.795543. Before we see how to compute variance and standard deviation in R, there is an important issue to note when computing variance, this is bias and unbiased variance. Biased and unbiased variance When dealing with sample data, our goal is to estimate true values of a population. So when dealing with sample statistics (estimates), we expect it to represent population parameters from where it came from. When we say “expect”, we mean that on repeated experiments, on average the estimator equals true parameter. This is often referred to as expectation of an estimator. When an expected estimator does not reflect it’s true population parameter then it is said to be biased. An expected estimator is said to be unbiased if it equals true population parameter. Mathematically, it has been proven that expectation sample mean is an unbiased estimator of population mean. It’s also been proven that sample variance is a biased estimator of population variance. The latter occurs as sample variance tends to underestimate variance. To correct a biased sample variance, a normalization factor is used. This factor increases mean squared deviations (MSE) by dividing squared deviations with “n - 1” instead of “n”. This technique is called “Bessel’s correction” and it is used when population mean is unknown. In summary, when population mean is unknown, we compute sample variance as sum of squared deviations divided by number of values minus one. We can express this mathematically as: \\[S^2 = \\frac{\\sum\\limits^{n}_{i=1}(x_i - \\bar{x})^2}{n - 1}\\] Where: \\(S^2\\) = Sample variance \\(x_i\\) = Sample observations (values) \\(\\bar{x}\\) = sample mean \\(n\\) = Number of observations (values) Population variance remains the same, that is: \\[\\sigma^2 = \\frac{\\sum\\limits^{N}_{i=1}(X_i - \\mu)^2}{N}\\] Where: \\(\\sigma^2\\) = Population variance \\(X_i\\) = Population values \\(\\mu\\) = Population mean \\(N\\) = Total population (sum of all population values) Based on preceding discussion, assuming our scores are sample data and we do not know population parameters, then correct sample variance is 2109.85 to the nearest 2 decimal places (2dp)) and standard deviation is 45.93 (2dp). Computing variance and standard deviation in R To compute variance in R we use function “var”. Note, R uses (corrected) sample variance. To compute sample standard deviation we simply use function “sd”, which is the same as square rooting variance “sqrt(var)”. # Variance var(scores) ## [1] 2109.855 # Sd, either sqrt(var(scores)) ## [1] 45.93315 # Or simply sd(scores) ## [1] 45.93315 Interpreting Standard Deviations Note, here we want to interpret standard deviation and not variance. Variance is not meaningful, we only use it to compute standard deviation. The whole point of computing standard deviation is to measure a distributions dispersion from it’s average. A high standard deviation means distribution is spread out, a low standard deviation means values are centered around the mean. For our scores data set, we found a standard deviation of about 46, this is certainly high, hence we can say values in this distribution are spread out. When reporting, we report both mean and standard deviation like this, “Scores had a mean of about 223 with a standard deviation of 46”. 2.2.3.4 Displaying Dispersion There are a number of graphical displays that show a distribution’s dispersion, some of them include dot plot, stem-and-leave plot and histogram. A dot plot is one of the simplest plots to develop, it displays values in a distribution. It involves plotting values on a scale, these values are stacked-up to look like bar charts (vertical rectangle boxes). This plot is used when there few data points(roughly around 30 values). For data sets with more values, stem-and-leave or histogram is appropriate. One advantage a dot plot has over most graphs is it’s preservation of numerical information (it is easy to see each value/data point). There are different variants of dot plots, original dot plot stems from 1880’s, called “Wilkinson’s dot plot” and a newer dot plot suggested by Cleveland ( a variation of a bar plot). A stem-and-leave plot shows frequencies for which values occur in a distribution. They are sort of table where each value is split into a “stem” (the first digits) and a “leaf” (usually the last digits). They show ordered distribution which makes it easy to read and interpret. They are most appropriate for distributions with about 15 to 150 data points. Anything below 15 would not show distribution well and anything above 150 might be to clustered. Histograms, these are perhaps the most widely used graphical displays for continuous distributions. A histogram is graph with rectangular boxes representing either frequencies or proportions. Rectangles represent “bins” which are range of values or intervals split from a distribution. The number of bins a histogram can have depend on a distribution, there is no ideal number as long as the bins show different features of the data. One of the advantages of histograms is it’s ability to show distributions with many values. In this section we will use original dot plot to show distribution of our variables. Dot Plot To generate a dot plot, draw a number line or a scale from minimum to maximum value and then mark a dot on each number there is a value. If there are more than one value, then stack the dots on top of each other. Let’s look at two distribution with the same mean but different standard deviations. We are interested in seeing how standard deviation informs us of a distribution’s spread. ## First distribution: 49, 50, 55, 56, 56, 59, 67, 70, 71, 72, 72, 75, 75, 76 and 82. Mean is about 66 and Standard deviation is about 11. ## Second distribution: 64, 64, 65, 65, 65, 65, 66, 66, 66, 67, 67, 67, 67, 67 and 67. Mean is about 66 and Standard Deviation is 1. If we draw these two distribution using the same scale, we should see something like this: Dot plots Unfortunately R does not have a function to generate this kind of plot, function “dot chart” produces a Cleveland’s dot plot which is basically a bar chart with dots instead of rectangular boxes. But not to worry, we can easily generate our own plot given our understanding of plot function (discussed in chapter eight of “R Essentials”). # Set canvas to 2 rows and 1 column op &lt;- par(&quot;mfrow&quot;) par(mfrow = c(2, 1)) # Generate frequency indices y1 &lt;- c(rep(1, 4), 2, rep(1, 5), 2, 1, 2, rep(1, 2)) y2 &lt;- c(1:2, 1:4, 1:3, 1:6) # First plot plot(scores4, y1, type = &quot;n&quot;, axes = FALSE, ylim = c(0.5, 2.5), xlab = &quot;Scores&quot;, ylab = &quot;&quot;) axis(side = 1, at = seq(45, 85, by = 1), labels = FALSE) axis(side = 1, at = seq(45, 85, by = 5), labels = FALSE, lwd = 2) mtext(seq(50, 80, by = 5), side = 1, line = 1, at = seq(50, 80, 5)) axis(side = 2, labels = FALSE, tick = FALSE, lwd = 0) text(x = 80, y = 3, labels = paste(&quot;SD = &quot;, round(sd(scores4))), xpd = TRUE) abline(v = mean(scores4), col = 4) points(x = scores4, y = y1, pch = 20) # Second plot plot(scores5, y2, type = &quot;n&quot;, axes = FALSE, xlab = &quot;Scores&quot;, ylab = &quot;&quot;, xlim = range(scores4)) axis(side = 1, at = seq(45, 85, by = 1), labels = FALSE) axis(side = 1, at = seq(45, 85, by = 5), labels = FALSE, lwd = 2) mtext(seq(50, 80, by = 5), side = 1, line = 1, at = seq(50, 80, 5)) axis(side = 2, labels = FALSE, tick = FALSE, lwd = 0) text(x = 80, y = 5, labels = paste(&quot;SD = &quot;, round(sd(scores5)))) abline(v = mean(scores4), col = 4) points(x = scores5, y = y2, pch = 20) Figure 2.4: dotplots par(mfrow = op) From our two plots, it is clear to see how they differ in terms of dispersion. First plot with sd of about 11 is more spread out than second plot with sd of about 1 despite them having a mean of about 66. 2.2.4 Shape of distributions Shape of a distribution is an important concepts in statistics as it informs on observations balance around it’s center and presence of outliers. This balance around it’s mean (symmetry) and presence or absence of outliers is often used to determine statistical methods of inference. Based on this fact, this section aims to introduce a quantitative measure for shape of sets of points called “moments”, more specifically we will look at the third (skewness) and fourth (kurtosis) moments. In statistics, moments are measures used to describe a distributions shape in terms of it balance from a central point (mean). Statistical moments can show a distribution that is symmetric or asymmetric. It can also inform us which side of the distribution has trailing observation as well as symmetric distributions with heavy tails. Moments have incremental measures referred to as orders and they begin from Zero up to higher order moments. The first five statistical moments are most commonly used to describe a distribution’s shape. These moments are: Zeroth moment - Total distribution First moment - Mean Second moment - Variance Third moment - Skewness Fourth moment - Kurtosis Statistical moments are based on concept of probability, for example the zeroth moment is a probability distribution function equal to one. That is, summation of all chances of observing a value in the distribution will total to 1. For example, for a discrete variable like a coin toss, there are only two outcomes, heads or tails, the chance of observing either a head or a tail for a fair coin is equal, that is 1/2 (or 0.5). Summation of these two chances equals 1. For a continuous variable such as having a student with a given height is not easily computed as that of a discrete variable. This is because continuous variables can take on any value within a range. When we reach chapter four on statistical inference and specifically probability, we will get to know “probability density function” which uses integrals to compute probability of continuous variables. At this point what we should grasp is that for both discrete and continuous variable summation of all possible chances equals 1. With the understanding that statistical moments are based on probability, when we talk of mean we are referring to mean of a discrete variable given \\(\\sum_xP_{(x)}\\) and mean for a continuous variable given by \\(\\int xf_{(x)}d_{x}\\). We will not get into defining these two function as we will get to discuss them at length later, but is important to know what we mean when we say first moment. An important issue to know as we discuss statistical moments is “expectations” as we shall often refer to them. This concept is also a probability concept which simply means a predicted “average”. To understand this concept, think of dice throws, if we threw a pair of dice many times, then in the long-run the chance (probability) of observing a pair of numbers like 2 and 3 would normalize to a certain value. That is, on average, given the many throws, we expect to see numbers 2 and 3 say 20% of the time. This average is what we call “expected value” and we will later note that this is based on “laws-of-large-numbers”. Mathematically, expectation is denoted by \\({\\mathbb E}\\), so when we write \\({\\mathbb E}(x)\\) (expectation of x), we simply mean average of variable \\(x\\). Each of the moments is defined by a mathematical formula that is a geometric series 7 and more specifically an exponential function. Exponential function are basically a base number raised to a certain number like \\(2^4\\). Exponential functions tend to grow over time or in this case values, example, take base 2 and raise it to exponents 0 through to 5, you should get 1, 2, 4, 8, 16, 32. The basic idea of letting these values grow is to be able to give more weight to values that are far from the mean. By doing so, we are able to determine just how far they are and how much they weigh (weight of the tails). It is with this reasoning that we raise zeroth moment with 0, first moment (mean) with 1, second moment (variance) with 2, third moment with 3 and fourth moment with 4. The basic formula for a moment is: sth moment = \\[\\frac{(x_1^s + x_2^s + x_3^s + ... +x_n^s)}{n}\\] Where: s = Number of moment (like 1 for mean) n = Total number of observations This formula can also be expressed simply as \\({\\mathbb E}(x)^s\\). In statistics, our values \\(x\\) are in reference to a particular point, hence they will be subtracted from this reference point. Zeroth moment Zeroth moment as mentioned earlier would sum-up to one. It is given by \\({\\mathbb E}((x - \\bar{x})^0)\\) (expectation of deviations raised to 0) or \\(\\sum(x - \\bar{x})^0\\). It is good to note that 0 is considered point of equilibrium (perfect balance) for any distribution. This point of equilibrium is referred to as origin. First moment As mentioned, first moment is the mean and it is defined as a “raw moment”. Without going into too much detail, raw moments are in reference to “origin” zero. So mean (first moment about origin) is given by: \\(\\bar{x}_1^` = {\\mathbb E}[(x - 0)^1]\\) = \\({\\mathbb E}[(x)^1]\\) \\[\\therefore \\bar{x} = {\\mathbb E}(x)\\] The symbol \\(`\\) on top of \\(\\bar{x}\\) means it a raw moment. Second moment At this point we should be able to see a pattern in these moments. Each moment is raised to an exponent the same as it’s number and so far we have been making reference to the origin (distances from 0). Now, from second moment we will start making reference to mean rather than it’s origin. That is, distance of each observation from it’s mean. We are doing this because the rest of the other moments relate only to spread and shape rather than it’s location (where it is from origin). It is for this reason (getting distances from the mean) that second, third, fourth and higher moments are defined as central moments. Therefore we can express second moment as \\({\\mathbb E}[(x-\\mu)^2]\\) for population variance and \\({\\mathbb E}[(x-\\bar{x})^2]\\) for sample variance. Alternatively, these can be expressed as: Population Variance: \\[\\sigma^2 = \\frac{\\sum(x - \\mu)^2}{n}\\] Unbiased sample variance: \\[s^2 = \\frac{\\sum(x - \\bar{x})^2}{n-1}\\] Third moment Third moment is Skewness, it measures a distribution’s symmetry. This moment will tell us if there are trailing values at either extremes of a distributions or not. We will discuss this a little more below, but here let’s look at it’s formula as a way to complete our discussion on statistical moments. Following our second moment, we know this moment should be raised to three and it should be referenced to our average (\\(\\mu\\) or \\(\\bar{x}\\)). Therefore third moment is given by \\(m_{3} = {\\mathbb E}[(x-\\mu)^3]\\) or simply \\[m_{3} = \\frac{\\sum(x-\\mu)^3}{n}\\] Where: \\(m_3\\) = Third moment \\(x\\) = An observation \\(\\mu\\) = Mean \\(n\\) = Number of observations Notice \\(m_3\\) is the average cubed distance from the mean, for variance it was average squared distance from the mean. By taking cubes, we ensure direction of these distances are maintained, that is observation below the mean would be negative and those above the mean would be positive. For a perfectly symmetric distribution, values below the mean and those above the mean would cancel each other thereby resulting to zero. However, if values below or above are more than the other, then we would have a asymmetric (skewed) distribution. When making comparison between different distributions with different size of unit of measure, this moment is bound to give unreliable measure. To understand this, take an example of the following two test scores: ## First test score: 74, 79, 41, 58, 97, 93, 40, 61, 46, 49, 59, 54, 94, 82 and 70 ## Second test score: 37, 39.5, 20.5, 29, 48.5, 46.5, 20, 30.5, 23, 24.5, 29.5, 27, 47, 41 and 35 The first test score is out of 100, and the other is out of fifty, but test score two is actually half of test score 1 so we should expect \\(m_3\\) to be the same for both distributions. But this is not so as: ## m3 for first test score is 1474.977 ## m3 for second test score is 184.3721 Difference between these two \\(m_3\\) comes from unequal size of unit of measure (100 and 50). To correct this we use a statistical concept known as “standardization”. Standardization basically means creating unit-less measures which are used to compare observations of different units or unit sizes. In this case we will use standard deviation (average distance from mean) to standardize deviations. That is, we will take each deviation as a fraction of standard deviation. We will then take the cube route of these standardized deviation before summing the up. By doing so we would have “standardized moments”. Mathematically we can express this standardized moments as: \\[m_3 = \\frac{{\\mathbb E}[(x-\\mu)^3]}{\\sigma^3}\\] Using this new standardized measure, we should get \\(m_3\\) for both scores as 0.2032448. Fourth moment Like \\(m_3\\), \\(m_4\\) or fourth moment is a standardized moment, meaning deviations are fractions of distributions standard deviation. Thus it is express as: \\[m_4 = \\frac{{\\mathbb E}[(x-\\mu)^4]}{\\sigma^4}\\] For standardized moments, they are expressed in terms of standard deviations from mean. By taking fourth power means we are eliminating negative deviation, but at the same time giving more weight to extreme values than those close to the mean. In particular, values that are less than one standard deviation away will have lower \\(m_4\\) but values than are 1 standard deviation away will have higher \\(m_4\\). For example, a value that is 0.5 standard deviation away will have \\(m_4\\) of 0.0625 while a value that is 1.5 standard deviation away will have \\(m_4\\) of 5.0625. 2.2.4.1 Skewness Skewness in very basic terms is tendency of some values in a distribution to be located at the extreme points (minimum/maximum) of it’s distribution. When a distribution has most of it’s values centered around it’s average but has trailing values either to the left or right, then it is said to be skewed. Trailing values are often referred to as tails (from a visual point of view). Skewness also means there is lack of symmetry (they are asymmetrical). Symmetry means there is a perfect balance to the left and to the right of a distribution’s average: two half’s of the distribution are a mirror image of each other. Symmetric distribution have their measures of center (mean, median and mode) equal or almost equal. Graphically, they are often referred to as “bell-shaped” curves. There are two ways to tell whether a distribution is skewed, by visual inspection and by computation. Histograms, density plots and bar plots are often used to visualize presence and direction of skewness while computation is used to inform on amount and direction of skewness. A skewed distribution can either be positive or negatively skewed. A positively skewed distribution is one whose bulk of values are to the left of the distribution with a trail of values to the right (has a right tail). For this distribution, mean is usually higher than the mode. A negative skewed distribution is one whose bulk of values are to the left of the distribution and has a tail to the right. Negative skewed distributions have a higher mode than mean. Skewness is an important concept in statistics as most statistical inferences are based on assumption of perfect (or near perfect) symmetry for example the normal distribution which forms the largest portion of data analysis. When checking for skewness, it’s important to begin by establishing a distribution’s mode. That is, find out if it has a mode and if so, how many. For uni-modal distribution, we check for presence of skewness by displaying distribution and making some computations. From visual inspections, we know if bulk of the data is to the left and it’s right tailed, then it’s positively skewed. If bulk of data is to the right and it’s left tailed then it’s negatively skewed. If on the other hand both sides of a distribution’s average are equal, then distribution is not skewed, it symmetric. Let’s now discuss these two methods of determining skewness (display and computation), we begin with displaying distributions. 2.2.4.1.1 Displaying Skewness The best way to display skewness is by using histogram or kernel density plots for continuous distributions and bar plots for discrete observations. The reason histograms are more appropriate for continuous distributions is that their bins are continuous (rectangular bars touch each other). On the other hand bar plots are more appropriate for discrete observations as their bars do not touch meaning they are not continuous. As an example, we will show a symmetric continuous distribution using the following hypothetical distribution of sampled human heights in inches. ## 68, 68.0147058823529, 68.0294117647059, 68.0441176470588, 68.0588235294118, 68.0735294117647, 68.0882352941177, 68.1029411764706, 68.1176470588235, 68.1323529411765, 68.1470588235294, 68.1617647058823, 68.1764705882353, 68.1911764705882, 68.2058823529412, 68.2205882352941, 68.2352941176471, 68.25, 68.2647058823529, 68.2794117647059, 68.2941176470588, 68.3088235294118, 68.3235294117647, 68.3382352941177, 68.3529411764706, 68.3676470588235, 68.3823529411765, 68.3970588235294, 68.4117647058823, 68.4264705882353, 68.4411764705882, 68.4558823529412, 68.4705882352941, 68.4852941176471, 68.5, 68.5147058823529, 68.5294117647059, 68.5441176470588, 68.5588235294118, 68.5735294117647, 68.5882352941177, 68.6029411764706, 68.6176470588235, 68.6323529411765, 68.6470588235294, 68.6617647058823, 68.6764705882353, 68.6911764705882, 68.7058823529412, 68.7205882352941, 68.7352941176471, 68.75, 68.7647058823529, 68.7794117647059, 68.7941176470588, 68.8088235294118, 68.8235294117647, 68.8382352941177, 68.8529411764706, 68.8676470588235, 68.8823529411765, 68.8970588235294, 68.9117647058823, 68.9264705882353, 68.9411764705882, 68.9558823529412, 68.9705882352941, 68.9852941176471, 69, 69.0147058823529, 69.0294117647059, 69.0441176470588, 69.0588235294118, 69.0735294117647, 69.0882352941177, 69.1029411764706, 69.1176470588235, 69.1323529411765, 69.1470588235294, 69.1617647058823, 69.1764705882353, 69.1911764705882, 69.2058823529412, 69.2205882352941, 69.2352941176471, 69.25, 69.2647058823529, 69.2794117647059, 69.2941176470588, 69.3088235294118, 69.3235294117647, 69.3382352941177, 69.3529411764706, 69.3676470588235, 69.3823529411765, 69.3970588235294, 69.4117647058823, 69.4264705882353, 69.4411764705882, 69.4558823529412, 69.4705882352941, 69.4852941176471, 69.5, 69.5147058823529, 69.5294117647059, 69.5441176470588, 69.5588235294118, 69.5735294117647, 69.5882352941177, 69.6029411764706, 69.6176470588235, 69.6323529411765, 69.6470588235294, 69.6617647058823, 69.6764705882353, 69.6911764705882, 69.7058823529412, 69.7205882352941, 69.7352941176471, 69.75, 69.7647058823529, 69.7794117647059, 69.7941176470588, 69.8088235294118, 69.8235294117647, 69.8382352941177, 69.8529411764706, 69.8676470588235, 69.8823529411765, 69.8970588235294, 69.9117647058823, 69.9264705882353, 69.9411764705882, 69.9558823529412, 69.9705882352941, 69.9852941176471, 70, 67, 67.037037037037, 67.0740740740741, 67.1111111111111, 67.1481481481482, 67.1851851851852, 67.2222222222222, 67.2592592592593, 67.2962962962963, 67.3333333333333, 67.3703703703704, 67.4074074074074, 67.4444444444444, 67.4814814814815, 67.5185185185185, 67.5555555555556, 67.5925925925926, 67.6296296296296, 67.6666666666667, 67.7037037037037, 67.7407407407407, 67.7777777777778, 67.8148148148148, 67.8518518518518, 67.8888888888889, 67.9259259259259, 67.962962962963, 68, 70, 70.037037037037, 70.0740740740741, 70.1111111111111, 70.1481481481482, 70.1851851851852, 70.2222222222222, 70.2592592592593, 70.2962962962963, 70.3333333333333, 70.3703703703704, 70.4074074074074, 70.4444444444444, 70.4814814814815, 70.5185185185185, 70.5555555555556, 70.5925925925926, 70.6296296296296, 70.6666666666667, 70.7037037037037, 70.7407407407407, 70.7777777777778, 70.8148148148148, 70.8518518518518, 70.8888888888889, 70.9259259259259, 70.962962962963, 71, 66, 66.25, 66.5, 66.75, 67, 71, 71.25, 71.5, 71.75 and 72 . This distribution has a mean of about 69, a standard deviation of about 1.09, and a modal class of 68 - 70. Ploting Histograms and Density plots To plot a histogram as we mentioned in “R Essentials” chapter eight, we need to begin by defining our bins (intervals to split data). As noted in that book, there is no set way for splitting data, and it is best to try a number of different intervals until a good pattern forms. For this data, we have a range of values between 66 and 72, so we want to split these values in such a way that we have a good picture of our distribution. One good cutoff point is to split data every after 1 value beginning with 66 up to 72. Basically, split data to breaks of 66, 67, 68, 70, 71 and 72. Based on this, we can form classes that begin from first break up to but not including next break. Therefore we can table our classes with their frequencies in the following distribution table: ## intervals trail ## [1,] &quot;66 - 67&quot; &quot;5&quot; ## [2,] &quot;67 - 68&quot; &quot;28&quot; ## [3,] &quot;68 - 70&quot; &quot;137&quot; ## [4,] &quot;70 - 71&quot; &quot;28&quot; ## [5,] &quot;71 - 72&quot; &quot;5&quot; We can now draw a histogram from this table. A histogram is basically a plot of breaks against frequencies (or proportions). Number of frequencies (or proportions) are shown using rectangular boxes referred to as “bins”. Each bin represents an interval between breaks, like the first bin will be between 66.5 and 67 with a height of 6 and the second will be between 67 and 67.5 with a height of 11. hist(heights1, breaks = breaks, ann = FALSE) title(&quot;Histogram&quot;, xlab = &quot;Sampled Heights&quot;, ylab = &quot;Frequency&quot;) abline(v = median(heights1), col = 4) Figure 2.5: Histogram of height1 data We can easily plot a density plot instead of a histogram. A density plot is a curve formed by estimating values between two observations. Estimation is done using a statistical technique called “kernel smoothing”, which is heavily based on probability and more specifically “probability density function”. Density plot are a better display of shape of a distribution than histograms because they are not affected by “binning”. Histograms involve a lot of trial-and-error to get the right bin size and number to display shape of distribution well. Therefore, it’s preferable to use these plots when interest is to visualize shape. Since we shall cover probability in chapter four, let’s take the basic understanding that a density plot will fit a curve over all point thereby giving us a visual of shape of our distribution. With that understanding, we might not accurately draw a density plot free hand and therefore we can only see this plot using a statistical program like R. In R, we use function “density” to compute density estimates and plot them with “plot” function. In our subsequent chapter on Exploratory data analysis, we shall see how to add color to such plots but for now let’s just see how to plot them. dens &lt;- density(heights1) plot(dens, ann = FALSE) title(&quot;Density plot in R&quot;) segments(x0=median(heights1), y0 = 0, x1 = median(heights1), y1 = dens$y[which.max(dens$y)], col = 4) Figure 2.6: Density plot of height1 data Now let’s look at a positively skewed distribution. For this, we shall use the following distributions. trail2 &lt;- c(10:2, rep(1, 7)) breaks2 &lt;- seq(50, 66, 1) n &lt;- length(breaks2) values &lt;- seq(min(breaks2), max(breaks2), 0.1) classes2 &lt;- lapply(1:(n-1), function(i) values[values &gt;= breaks2[i] &amp; values &lt; breaks2[i+1]]) heights2 &lt;- unlist(lapply(1:(n-1), function(i) sample(classes2[[i]], trail2[i], replace = TRUE))) cat(heights2) ## 50.7 50.9 50 50.6 50.6 50.9 50.3 50.8 50.5 50.8 51.1 51.3 51 51.7 51.9 51.5 51.4 51.1 51.8 52.2 52.8 52.4 52.5 52.3 52.1 52 52.2 53.7 53 53.1 53.2 53 53.1 53.4 54.8 54.6 54.5 54.8 54.2 54.7 55.2 55.7 55.2 55.5 55.7 56.2 56.7 56.3 56.3 57 57.8 57.4 58.2 58.5 59.2 60.5 61.1 62.1 63.2 64.1 65.8 Using R, let’s plot a histogram and a density plot side-by-side just to see how well they show shape of a distribution. op &lt;- par(&quot;mfrow&quot;) par(mfrow = c(1, 2)) hist(heights2, main = &quot;Histogram&quot;) dens2 &lt;- density(heights2) plot(dens2, main = &quot;Density plot&quot;) par(mfrow = op) From our plots, the tail to the right of the peak/hump/cluster clearly suggest a positive skewed distribution. Great, how about a negatively skewed distribution, here is a distribution which will show us how it looks like. heights3 &lt;- rep(91:36, c(1:10, 20, 36, 20, 10:3, rep(2, 5), rep(1, 30))) cat(heights3) ## 91 90 90 89 89 89 88 88 88 88 87 87 87 87 87 86 86 86 86 86 86 85 85 85 85 85 85 85 84 84 84 84 84 84 84 84 83 83 83 83 83 83 83 83 83 82 82 82 82 82 82 82 82 82 82 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 79 78 78 78 78 78 78 78 78 78 78 77 77 77 77 77 77 77 77 77 76 76 76 76 76 76 76 76 75 75 75 75 75 75 75 74 74 74 74 74 74 73 73 73 73 73 72 72 72 72 71 71 71 70 70 69 69 68 68 67 67 66 66 65 64 63 62 61 60 59 58 57 56 55 54 53 52 51 50 49 48 47 46 45 44 43 42 41 40 39 38 37 36 dens3 &lt;- density(heights3) plot(dens3) Bar plots Suppose we had a discrete distribution, histogram would not be ideal as discrete values are not continuous, instead bar plot would be more appropriate. Let’s look at an example. We are given the following distribution with ages of students in a class. ages &lt;- rep(11:19, c(5, 15, 25, 50, 73, 50, 25, 15, 5)) cat(ages) ## 11 11 11 11 11 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 19 19 19 19 19 Mean of this distribution is about 15, median is 15 and mode is 15. With these statistics, we can easily say our distribution is symmetric based on our current understanding of symmetry. But let’s visualize this with a bar plot. Bar plots are quite similar to histograms with exception that bars represent actual observations rather than classes or breaks and bars do not touch each other as data is not continuous. To draw these plots we begin by making frequency tables. Value Frequency 11 5 12 15 13 25 14 50 15 73 16 50 17 25 18 15 19 5 From our frequency table we can establish that we have uni-modal distribution as we have one mode; 15 with 73 observations. Now we can plot (unique) observations against their frequencies. In R, we can produce a bar plot with function&quot; bar plot&quot; inputting a frequency table generated by “table” function. tabages &lt;- table(ages) barplot(tabages) title(&quot;Bar plot of ages data set&quot;, xlab = &quot;ages&quot;, ylab = &quot;Frequency&quot;) Figure 2.7: barplot of ages data There is a lot of debate on use of bar plot, in general, these plots are seen as easy to generate but have a high data to ink ratio, they basically use to much ink. Hence an alternative is “Cleveland’s dot plot”. tabages_mat &lt;- matrix(tabages) colnames(tabages_mat) &lt;- &quot;Ages&quot; dotchart(tabages_mat, bg = 4) title(&quot;Dot plot a bar plot alternative&quot;, xlab = &quot;Frequencies&quot;) Figure 2.8: Cleverland’s dot plot 2.2.4.1.2 Computating Skewness There are a number of ways to compute skewness, one of them is using Pearson’s moment coefficient of skewness or simply moment coefficient of skewness. This is the third statistical moment we discussed earlier. Other are: Pearson’s first skewness coefficient (mode skewness) Pearson’s second skewness coefficient (median skewness) Quantile-based measures Groeneveld &amp; Meeden’s coefficient L-moments Distance skewness Medcoupe 8 In this section we shall compute skewness based on the third statistical moment of skewness as we have discussed it at length. You can use a calculator or any other statistical program to compute skewness of height 1 through 3 and that of ages data set, but I prefer to use R as we can easily program our own functions. Here are some function definition for computing skewness in R. These are m3 and standardized m3. # To compute third statistical moment m3 &lt;- function(x) { n &lt;- length(x) dif &lt;- (x - mean(x))^3 sum(dif)/n } # To compute standardized thrid statistical moment m3_std &lt;- function(x) { s &lt;- sd(x) m3(x)/(s^3) } With these functions we can now compute skewness of data set “height1” (continuous, symmetric), height2 (continuous, positively skewed), height3 (continuous, negatively skewed) and ages (discrete, symmetric). # Skewness of height 1 data set skew_1 &lt;- m3_std(heights1) skew_1 ## [1] 0 # Skewness of height 2 data set skew_2 &lt;- m3_std(heights2) skew_2 ## [1] 1.193713 # Skewness of height 3 data set skew_3 &lt;- m3_std(heights3) skew_3 ## [1] -1.798546 # Skewness for ages data set skew_4 &lt;- m3_std(ages) skew_4 ## [1] 0 Interpreting Computed Skewness First thing to determine with computed skewness is whether a distribution is symmetric or asymmetric. Symmetric distributions have a skewness of zero while asymmetric distributions can be any other positive or negative number. For other values other than 0, a negative value means it’s negatively skewed while a positive value means it’s positively skewed. More specifically, according Bulmer 1979 9, If skewness is less than -1 or greater than +1 then distribution is “highly skewed”. If skewness is between -1 and -1/2 or between +1/2 and 1, then the distribution is “moderately skewed”. If skewness is between -1/2 and +1/2, the distribution is “approximately symmetric”. The good thing about R is that it’s a programming language and we can program a function to interpret for us skewness instead of us trying to figure out which interval our value lies. We can therefore define the following function: skewness_interpreter &lt;- function(x) { if(x == 0) { return(&quot;symmetric (not skewed)&quot;) } else if (x &gt; -0.5 &amp; x &lt; 0.5) { return(&quot;approximately symmetric&quot;) } else if (x &lt;= -0.5 &amp; x &gt;= -1) { return(&quot;moderately (negatively) skewed&quot;) } else if (x &gt;= 0.5 &amp; x &lt;= 1) { return(&quot;moderately (positively) skewed&quot;) } else if (x &lt; -1 | x &gt; 1) { if (x &lt; -1) { return(&quot;highly negatively skewed&quot;) } else { return(&quot;highly positively skewed&quot;) } } else { return(&quot;Can&#39;t interpret that, I need one numerical value.&quot;) } } Given the aforementioned computation interpretation and function, then we can say distribution 1 (height1) with skewness of 0 is symmetric (not skewed). Distribution 2 (height2) with skewness of 1.1937133 is highly positively skewed and distribution 3 (height3) with skewness -1.7985459 is highly negatively skewed. Ages data set with skewness of 0 is perfectly symmetric (not skewed) around it’s mean. 2.2.4.2 Kurtosis We have seen how to describe shape of a distribution as being symmetric (no skewness) or asymmetric (skewed) and for skewed distributions we have a basic understanding of how to identify left and right skewness. But what if both sides are skewed like one side has a long tail and another has a short but fat tail, skewness can not tell us this (unless we plot it). This is because both sides would cancel each other. To determine “tailness” or outliers on both sides of a distribution, we need to use another measure of descriptive statistic called “Kurtosis”. In very simple terms, Kurtosis is a measure of combined effects of a distribution’s tails in terms of the entire distribution. This measure originated with a Statistician called “Karl Pearson”. In deriving this measure, Pearson targeted extreme values by taking the fourth power of deviations from the mean. This is our fourth statistical moment discussed earlier. Do note, given our discussion on the fourth moment, it is quite clear why Kurtosis is a measure of “tailness” rather than peakness as it is defined in many (older) statistical literature. Basically, taking the fourth power means we are looking at extreme values. Ultimately as we compute Kurtosis, we should be able to say whether it is “Mesokurtic”, “Leptokurtic” or “Platykurtic”. Mesokurtic distributions are “bell-shaped” or symmetrical like the normal distribution. This distribution has kurtosis of 3. A leptokurtic distribution has a kurtosis of greater than 3. The word “Lepto” means “slender”, shape-wise it has fatter tails. Example of distributions which display this distribution are: Student’s t-distribution, Laplace distribution, Exponent distribution, Poisson distribution and Logistics distribution. During our chapter on probability, we shall discuss more on these and other distributions. Platykurtic comes from the word “platy” and “kurtic”. Word platy means “broad” while “kurtic” means kurtosis. This distributions has a kurtosis of less than three. Shape-wise, they have thinner tails. They include: continuous and discrete uniform distributions, and Bernoulli distributions. Kurtosis are often computed in reference to symmetric (ideal) distribution. As mentioned, these distributions have kurtosis of 3, hence kurtosis is often reported in reference to value 3: This is what is called “excess kurtosis”. In terms of excess kurtosis, a mesokurtic distribution will be three, a leptokurtic distribution would have more than three (hence positive excess kurtosis) and a platykurtic distribution would have less than three (negative excess kurtosis). 2.2.4.2.1 Displaying Kurtosis To show kurtosis we will use density plots. Below are examples of a Mesokurtic and Leptokurtic distribution. x &lt;- seq(-4, 4, length.out = 100) n_dist &lt;- dnorm(x) t_dist &lt;- dt(x, 1) plot(x, n_dist, type = &quot;l&quot;) title(&quot;Comparison of normal and t distribution&quot;) lines(x, t_dist, col = 4) legend(&quot;topright&quot;, inset = 0.01, legend = c(&quot;Normal&quot;, &quot;t-distribution&quot;), lwd = 2, lty = 1, col = c(&quot;black&quot;, 4)) Figure 2.9: Mesokurtic and Leptokuurtic Distributions As noted, normal or symmetric distribution is a good example of a Mesokurtic distribution and in this plot it is depicted with a black density plot. T-distribution is a good example of a Leptokurtic distribution as it is slender and has fatter tails than the normal distribution. Uniform distribution is a distribution with constant probability. That is, chance of observing any one of the values is the same throughout. This distribution has no skewness as it one of the symmetric group of distributions. Graphically it is quite broad hence it is platykurtic in terms of kurtosis. u_dist &lt;- runif(200, 55, 89) hist(u_dist) Figure 2.10: Display of a platykurtic distribution 2.2.4.2.2 Computing Kurtosis Kurtosis is measured by the fourth standardized statistical moment which we defined as \\(m_{4}std = \\frac{m4}{\\sigma^4}\\). We can compute this statistic in R using the following defined functions: m4 &lt;- function(x) { n &lt;- length(x) m &lt;- mean(x) sum((x - m)^4)/n } m4_std &lt;- function(x) { s &lt;- sd(x) m4(x)/s^4 } Using this functions we should get the following kurtosis for variables, height1, height2, height3 and ages. kurt1 &lt;- m4_std(heights1); kurt1 ## [1] 2.828369 kurt2 &lt;- m4_std(heights2); kurt2 ## [1] 3.874731 kurt3 &lt;- m4_std(heights3); kurt3 ## [1] 5.768516 kurt4 &lt;- m4_std(ages); kurt4 ## [1] 2.884805 Recall it is more meaningful to describe tailness (kurtosis) in terms of symmetric distribution which has kurtosis of three, something we called “excess kurtosis”. Then let’s compute these measures. excess1 &lt;- kurt1 - 3; excess1 ## [1] -0.1716313 excess2 &lt;- kurt2 - 3; excess2 ## [1] 0.8747308 excess3 &lt;- kurt3 - 3; excess3 ## [1] 2.768516 excess4 &lt;- kurt4 - 3; excess4 ## [1] -0.1151954 2.2.4.2.2.1 Interpreting Kurtosis To interpret excess kurtosis we will follow the same scheme we used for skewness though this is really not documented. Value of zero means it “mesokurtic” If value is not zero but it is in between -0.5 and 0.5 is “approximately mesokurtic” If value is between -0.5 and -1 (inclusive of -0.5) then it is “moderately platykurtic” If value is between 0.5 and 1 (inclusive of 0.5), then it is “moderately leptokurtic” If value is -1 and below, then it is “platykurtic” Finally if value is 1 and above it is leptokurtic Here is a simple function for interpreting excess kurtosis in R. excess_interpreter &lt;- function(x) { if(x == 0) { return(&quot;mesokurtic&quot;) } else if (x &gt; -0.5 &amp; x &lt; 0.5) { return(&quot;approximately mesokurtic&quot;) } else if (x &lt;= -0.5 &amp; x &gt; -1) { return(&quot;moderately platykurtic&quot;) } else if (x &gt;= 0.5 &amp; x &lt; 1) { return(&quot;moderately leptokurtic&quot;) } else if (x &lt;= -1) { return(&quot;platykurtic&quot;) } else { return(&quot;leptokurtic&quot;) } } With our function, variable 1 (height1) with -0.1716313, can be said to be approximately mesokurtic, that means it almost symmetrical (this is totally true, isn’t it). Variable 2 (height2) can be said to be moderately leptokurtic which means it has a slender and fatter tails, something similar to t-distribution (fatter tail must be from it’s right skewness which happens to be positive). Variable 3 (height3) can be said to be leptokurtic, this means it’s broader and has thinner tail (which is correct as we know it is negatively skewed). Variable 4 (ages) can be said to be approximately mesokurtic means it’s almost symmetric (this is also true given our discussion on skewness). With that we have covered most of the basics of descriptive statistics. Now let’s apply it in the following case study. 2.3 Case Study As our first case study, let’s suppose a friend of our has given us a data set to analyse given the fact we have been studying some data analysis. Our friend is a waiter in one of the leading restaurant. She has given us a (sample) data set with tips received over some months. Based on this sample, our friend is interested to know if tips received is related to any of the other variables collected, that is total bill, gender, smoking habits, day of the week, time and size of party. Since we want to practice what we have learnt, we take our friend’s challenge and promise to deliver a small report on our finding (at least the descriptive part). For us to accomplish our task, we will need to do three things; Get to know our data Identify measures and graphs to report on and Write report Please note this case study will be done in R. Okay, let’s get going. 2.3.1 Getting to know our data Our data set comes from a package called “reshape2”. If you do not have the package installed, please install it first or run the following code. This code will find out if you have the package and if not it will install it before loading it (making it available for this session). # Check if package is avalaible, if not install if (!any(installed.packages()[,&quot;Package&quot;] == &quot;reshape2&quot;)) { install.packages(&quot;reshape2&quot;) } # Load package library(reshape2) We are going to use data set called “tips” and our first mission is to have a quick look at it. There many way to do so in R like taking a look at the first or last six rows using “head” or “tail” function. Checking objects type for each variable with function “class”, and establishing their lengths with “length” function. A much easier and compact way to view all this information is by using function “str”. str(tips) ## &#39;data.frame&#39;: 244 obs. of 7 variables: ## $ total_bill: num 17 10.3 21 23.7 24.6 ... ## $ tip : num 1.01 1.66 3.5 3.31 3.61 4.71 2 3.12 1.96 3.23 ... ## $ sex : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 1 2 2 2 1 2 2 2 2 2 ... ## $ smoker : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ day : Factor w/ 4 levels &quot;Fri&quot;,&quot;Sat&quot;,&quot;Sun&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... ## $ time : Factor w/ 2 levels &quot;Dinner&quot;,&quot;Lunch&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ size : int 2 3 3 2 4 4 2 4 2 2 ... We can clearly see that our data frame has 244 observations with 7 variables. There are three numeric variables (two of which are continuous) and four categorical (qualitative) variables. This data set contains the following variables: tips in dollars bill in dollars sex of the bill payer whether there were smokers in the party day of week time of day and size of the party Since our interest is to report on tips and it’s relation with other factors, then our variable of interest (dependent/response) is called “tip” which is a numeric and specifically a continuous variable. Before we do anything else, we need to understand this variable. We would like to know it’s measure of central tendency, how disperse it’s values are to it’s average, is it skewed and how much excess kurtosis does it have. 2.3.1.0.1 Describing response variable To get descriptive information about our variable of interest, we can run individual calls to “mean”, “media”, and “quantiles”, but their is a much more efficient function for that, this is “summary”. summary(tips$tip) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 2.000 2.900 2.998 3.562 10.000 From output received we know minimum amount received in form of tips is $1 and maximum amount received is $10. Mean tip received is about $2.998 and median tip is about $2.9. Since this is a numeric variable, we might not need to compute mode as either mean or median is sufficient to report on average of this variable. Now, between mean and median, there doesn’t look like there much of a difference between the two measures (about $0.098) so we can actually use mean as an average for this variable. We can confirm that mean is ideal measure of central tendency for this variable by establishing that this variable is not heavily skewed or has kurtosis close to symmetry. # Skewness tip_skew &lt;- m3_std(tips$tip); tip_skew ## [1] 1.447482 # Excess kurtosis tip_excess &lt;- m4_std(tips$tip) - 3; tip_excess ## [1] 3.495977 We have gotten a skewness of 1.4474824 which is highly positively skewed and excess kurtosis of 3.4959771 which is leptokurtic. Based on these two pieces of information, we know that this variable is skewed to the right and has fatter tails. This thus nullify’s our earlier indication that data is not skewed. So between mean and median, despite the small noticeable difference, it is more appropriate for us to report on median as the measure of central tendency for this variable. We can visualize this with the following histogram. From our summary, we know 50% of the tips are between $2 and $3.56. This can best be shown using a boxplot. boxplot(tips$tip, col = &quot;grey90&quot;, horizontal = TRUE) Our box plot also shows us what we already know from skewness that our variable has outliers on the higher end of our distribution. This is certainly good information to be reported. Now let’s look at dispersion, since we have established this variable is skewed, then we expect standard deviation to be high. This is based on the fact that standard deviation is affected by skewness. Therefore when we compute we get a standard deviation of about $1.38 (standard deviation is in the same units as our variable). This value might seem small considering a standard symmetric distribution has a standard deviation of 1, but looking at our values, this might be actually big. Hence, dispersion is best described by quantiles. In general, when measure of central tendency is mean, then standard deviation is it’s most appropriate measure of dispersion. When median is used as measure of central tendency, then quantiles are the most appropriate measure of dispersion. So in this case we can report that 25% of tips are below $2, 50% are between $2 and $3.56 and 25% are above $3.56. With that we should have adequately described this variable. Now let’s describe the other variables often called “independent”, “predictors” or “explanatory” variables. 2.3.1.0.2 Other variables In this section we will describe the other factors by whether they are numeric or categorical. Numerical variables Let’s describe the other two numerical variables, these are “total_bill” and “size”. We can describe both variables by running functions “summary” (to establish it average and quantiles), skewness, kurtosis and standard deviation. Here is a script for describing “total_bill”: # To determine mean, median and quantiles summary(tips$total_bill) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 3.07 13.35 17.80 19.79 24.13 50.81 # To compute skewness total_skew &lt;- m3_std(tips$total_bill); total_skew ## [1] 1.119318 # Interpretation of skewness skewness_interpreter(total_skew) ## [1] &quot;highly positively skewed&quot; # To compute Kurtosis total_excess &lt;- m4_std(tips$total_bill); total_excess ## [1] 4.135065 # Interpreting kurtosis excess_interpreter(total_excess) ## [1] &quot;leptokurtic&quot; # To compute standard deviation sd(tips$total_bill) ## [1] 8.902412 Variable “total_bill” has a mean of about $19.79 and a median of about $17.8. Since “total bill” can be said to be highly positively skewed and leptokurtic, then we will report on median as our measure of central tendency rather than mean. With this fact then we would not report on standard deviation but rather the quantiles where 25% lie below $13.35, 50% are between $13.35 and $24.13 ans the rest (25%) are above $24.13. We can visualize this variable with a boxplot (see quantiles and outliers) and a histogram (see entire distribution). Here is a histogram of this distribution We can run a similar script for variable “size”: summary(tips$size) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.00 2.00 2.00 2.57 3.00 6.00 size_skew &lt;- m3_std(tips$size); size_skew ## [1] 1.430128 size_excess &lt;- m4_std(tips$size); size_excess ## [1] 4.633712 skewness_interpreter(size_skew) ## [1] &quot;highly positively skewed&quot; excess_interpreter(size_excess) ## [1] &quot;leptokurtic&quot; sd(tips$size) ## [1] 0.9510998 By just looking at our summary we can tell this variable does not have to many unique values. We have a minimum value of 1 first quantile of 2 which is the same as median, a third quantile of 3 and a maximum value of 6. Let’s see how many unique values there are for this variable: # Number of unique values length(unique(tips$size)) ## [1] 6 When we have a discrete variable with few unique values such as this one, then it is wise to convert the variable to a categorical (qualitative) variable. For categorical variables, mean and median are not appropriate: the only appropriate descriptive statistic for categorical distribution is mode and best presented in a table format (frequency table for univariate and contingency tables for bivariate distributions). So we need to convert our “size” variable to a categorical variable. We can do this in R with function “factor”. Please note, it is advisable not to overwrite original variable when transforming it. Therefore here we will transform size to a factor variable called “size2” and add it to our data set. # Converting a numerical variable to categorical tips$size2 &lt;- factor(tips$size, ordered = TRUE) # Confirm variable is included in our dataset str(tips) ## &#39;data.frame&#39;: 244 obs. of 8 variables: ## $ total_bill: num 17 10.3 21 23.7 24.6 ... ## $ tip : num 1.01 1.66 3.5 3.31 3.61 4.71 2 3.12 1.96 3.23 ... ## $ sex : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 1 2 2 2 1 2 2 2 2 2 ... ## $ smoker : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ day : Factor w/ 4 levels &quot;Fri&quot;,&quot;Sat&quot;,&quot;Sun&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... ## $ time : Factor w/ 2 levels &quot;Dinner&quot;,&quot;Lunch&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ size : int 2 3 3 2 4 4 2 4 2 2 ... ## $ size2 : Ord.factor w/ 6 levels &quot;1&quot;&lt;&quot;2&quot;&lt;&quot;3&quot;&lt;&quot;4&quot;&lt;..: 2 3 3 2 4 4 2 4 2 2 ... Now we can describe with the other categorical variables. Categorical variables Let’s begin this section with our recently added variable “size2”. Our initial interest should be establishing a measure of central tendency for which we have agreed should be mode. To get mode we need to get frequencies of each unique observation or factor. We can easily do this in R by making a frequency table using function “table”. table(tips$size2) ## ## 1 2 3 4 5 6 ## 4 156 38 37 5 4 We can now establish the most frequently occurring observation by using function “which.max” and subsetting it from the table. size2_tab &lt;- table(tips$size2) size2_mode &lt;- size2_tab[which.max(size2_tab)] names(size2_mode) ## [1] &quot;2&quot; From our output, we can now say that based on this data, the average size of party is 2. We can show this using a bar plot. Now let’s look at the other categorical variables. Here’s a script describing gender composition of bill payers. sex_tab &lt;- table(tips$sex) sex_tab2 &lt;- (sex_tab/nrow(tips))*100; sex_tab2 ## ## Female Male ## 35.65574 64.34426 sex_mode &lt;- sex_tab2[which.max(sex_tab2)]; sex_mode ## Male ## 64.34426 From our output, we can note that this sample data had more male bill payers than female bill payers. That is because male payers comprised of 64% (157) and females 36% (87)). We can do the same for variable “smoker”. # Number of smokers and non-smokers smoker_tab &lt;- table(tips$smoker) # Frequency as percentages smoker_tab2 &lt;- smoker_tab/nrow(tips)*100; smoker_tab2 ## ## No Yes ## 61.88525 38.11475 # Most frequent smoking habit smoker_mode &lt;- smoker_tab2[which.max(smoker_tab2)]; smoker_mode ## No ## 61.88525 Here is a script to describe day of the week tips were received. # Sample Days day_tab &lt;- table(tips$day) # Frequency as percentages day_tab2 &lt;- day_tab/nrow(tips)*100; day_tab2 ## ## Fri Sat Sun Thur ## 7.786885 35.655738 31.147541 25.409836 # Most observed day day_mode &lt;- day_tab2[which.max(day_tab2)]; day_mode ## Sat ## 35.65574 Finally we can describe variable time with following script. # Service time time_tab &lt;- table(tips$time) # Frequency as percentages time_tab2 &lt;- time_tab/nrow(tips)*100; time_tab2 ## ## Dinner Lunch ## 72.13115 27.86885 # Most frequent dinning time time_mode &lt;- time_tab2[which.max(time_tab2)]; time_mode ## Dinner ## 72.13115 We now have described all our variables and know what to include in our report. But there is more to descriptive statistics than just univariate analysis. We can look at relationship between our response variable and other variable, this called bivariate analysis. 2.3.1.0.3 Bivariate analysis Our aim in this section is to see how to describe two variable in terms of their relationship. We shall not go into details and certainly we shall not make any inferences (generalizing to the entire population). When describing two numerical and continuous variables such as tips and total bill, it is best to display them on a scatter plot. A scatter plot is graph of two variable with one being mapped on the x-axis (often the predictor/explanatory/independent variable) and the other on the y-axis (often the response variable). When one variable is numerical (often the response) and the other is categorical (independent/explanatory/predictor), then a bivariate boxplot (or it’s alternatives) is quite ideal. Descriptive measures can also be generated for each category of the categorical variable. When two variables are categorical, then two-way/contingency tables are quite idea. In addition, these variables can be presented using bar plots or more efficient dot plot. Let’s look as these relationships beginning with relationship between tipping and total bill. Since they are both numerical we can use a scatter plot to visualize their relationship. What we are interested to see is if the points have some form of pattern, if they do then we can say they have a relationship. We might not be accurate of this relationship without actually measuring it but we should be able to see it. We will discuss measures of bivariate relationships in our chapter on inferential statistics. In R, we use function “plot” to make a scatter plot plot(tips$total_bill, tips$tip, ann = FALSE, pch = 20) title(&quot;Relationship between tips and total bill&quot;, xlab = &quot;Total bill ($)&quot;, ylab = &quot;Tips ($)&quot;) From visual inspection, we can say there seems to be a relationship between tip’s and total bill since there is pattern being formed by the points. This points seem to be increasing for both tips and total bill. We can therefore report on this as a positive relationship where tip’s increase as total bill increases. At this point we can not tell our good friend how much the increase is or if it strong, but we certainly can show it exists. Something we might be interested to see as well is whether tips given depend on amount paid as total bill. That is, see if proportion of tip to total bill has a relationship to total bill. To do this we need to create a new variable by computing proportion of each tip to it’s total bill. tips$prop_tip &lt;- tips$tip/tips$total_bill plot(tips$total_bill, tips$prop_tip, ann = FALSE, pch = 21, bg = &quot;grey50&quot;) title(&quot;Realtionship between proportion of tips and total bill&quot;, xlab = &quot;Total bill ($)&quot;, ylab = &quot;Tips ($)&quot;) There doesn’t seem to be a strong relationship, in fact there seem to be a slight decrease meaning the more the bill the less likely tips would be high. This is an interesting finding which we should include in our report. Now let’s look at relationship between tip’s and gender. We can get summary statistics of each gender by using function “tapply” (or it’s wrapper function “by”). biva_sum1 &lt;- tapply(tips$tip, tips$sex, summary); biva_sum1 ## $Female ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 2.000 2.750 2.833 3.500 6.500 ## ## $Male ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.00 2.00 3.00 3.09 3.76 10.00 From this we can see on average (median), male bill payers tend to give higher tips than female bill payers. But we have to remember we had more male bill payer than female bill payer. Hence it would be interesting to know if observed difference still holds when weight of the two subgroups is taken into account. biva_len1 &lt;- tapply(tips$tip, tips$sex, length) biva_n1 &lt;- biva_len1[[&quot;Female&quot;]] + biva_len1[[&quot;Male&quot;]] prop_fem &lt;- biva_len1[[&quot;Female&quot;]]/biva_n1 prop_mal &lt;- biva_len1[[&quot;Male&quot;]]/biva_n1 tips$tip_sex_prop &lt;- ifelse(tips$sex == &quot;Female&quot;, tips$tip * prop_fem, tips$tip * prop_mal) biva_sum2 &lt;- tapply(tips$tip_sex_prop, tips$sex, summary); biva_sum2 ## $Female ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.3566 0.7131 0.9805 1.0103 1.2480 2.3176 ## ## $Male ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.6434 1.2869 1.9303 1.9880 2.4193 6.4344 Looks like male bill payers are still better tippers. We can show this relationship with two box plots for comparison. In R, we still use function “boxplot” to plot two box-and-whiskers plots, but this time we use a tilde (~) between the two variable. A tilde is used to indicate relationship between variable as a formula. For bivariate relationship, this formula is shown as “response~predictor”, that is y ~ x (y given x), where y is numerical and x is categorical. boxplot(tips$tip_sex_prop~tips$sex, col = &quot;grey90&quot;, pch = 21, bg = 4, horizontal = TRUE) title(&quot;Comparison of male and female tippers&quot;, xlab = &quot;Tips ($)&quot;) Based on this data we can say on average male tippers are better and they have a number who give high tips than female tippers. With that background on how to present bivariate variables, let’s run down relationship of tip’s and the other explanatory variables and in each say if there is a relationship to report on. biva_len2 &lt;- tapply(tips$tip, tips$smoker, length) tips$tip_sm_prop &lt;- ifelse(tips$smoker == &quot;Yes&quot;, tips$tip*(biva_len2[[&quot;Yes&quot;]]/nrow(tips)), tips$tip*(biva_len2[[&quot;No&quot;]]/nrow(tips))) biva_sum3 &lt;- tapply(tips$tip_sm_prop, tips$smoker, summary); biva_sum3 ## $No ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.6189 1.2377 1.6957 1.8515 2.1691 5.5697 ## ## $Yes ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.3811 0.7623 1.1434 1.1468 1.4026 3.8115 boxplot(tips$tip_sm_prop~tips$smoker, col = &quot;grey90&quot;, pch = 21, bg = 4, horizontal = TRUE) title(&quot;Tipping by smoking status&quot;, xlab = &quot;Tips ($)&quot;, ylab = &quot;Smoking Status&quot;) Relationship between tipping and day of the week. biva_len3 &lt;- tapply(tips$tip, tips$day, length) day_props &lt;- biva_len3/nrow(tips) tip_day &lt;- tapply(tips$tip, tips$day, function(i) i) tips$tip_day_prop &lt;- unlist(sapply(1:4, function(i) tip_day[[i]]*day_props[[i]])) biva_sum4 &lt;- tapply(tips$tip_day_prop, tips$day, summary); biva_sum4 ## $Fri ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.3456 0.4284 0.7131 0.8022 1.0697 1.6651 ## ## $Sat ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.3146 0.5640 0.8736 0.9567 1.1400 3.5656 ## ## $Sun ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.07787 0.36538 0.71490 0.78632 1.09016 2.39963 ## ## $Thur ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.3761 0.6230 0.8025 0.9576 1.0996 3.2090 boxplot(tips$tip_day_prop~tips$day, col = &quot;grey90&quot;, pch = 21, bg = 4, horizontal = TRUE) title(&quot;Tipping by day of service&quot;, xlab = &quot;Proportion of Tips ($)&quot;, ylab = &quot;Day&quot;) Relationship of tips by time of service biva_len4 &lt;- tapply(tips$tip, tips$time, length) time_props &lt;- biva_len4/nrow(tips) tip_time &lt;- tapply(tips$tip, tips$time, function(i) i) tips$tip_tm_prop &lt;- unlist(sapply(1:2, function(i) tip_time[[i]]*time_props[[i]])) biva_sum5 &lt;- tapply(tips$tip_tm_prop, tips$time, summary); biva_sum5 ## $Dinner ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.3484 1.0229 1.6230 1.8302 2.3370 6.4918 ## ## $Lunch ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.4013 0.7193 1.5364 1.8158 2.5246 7.2131 boxplot(tips$tip_tm_prop~tips$time, col = &quot;grey90&quot;, pch = 21, bg = 4, horizontal = TRUE) title(&quot;Tipping by time of service&quot;, xlab = &quot;Proportion of Tips ($)&quot;, ylab = &quot;Time&quot;) Relationship between tips and size of party biva_len5 &lt;- tapply(tips$tip, tips$size2, length) size_props &lt;- biva_len5/nrow(tips) tip_size &lt;- tapply(tips$tip, tips$size2, function(i) i) tips$tip_sz_prop &lt;- unlist(sapply(1:nlevels(tips$size2), function(i) tip_size[[i]]*size_props[[i]])) biva_sum6 &lt;- tapply(tips$tip_sz_prop, tips$size2, summary); biva_sum6 ## $`1` ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.3033 0.8814 1.2531 1.1020 1.4737 1.5984 ## ## $`2` ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.01639 0.59937 1.27869 1.25094 1.77098 3.61230 ## ## $`3` ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.01639 0.52398 1.03574 1.05608 1.52557 2.74918 ## ## $`4` ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.2118 0.7582 1.2787 1.4573 2.0331 3.7402 ## ## $`5` ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.5451 0.5451 0.7173 0.6952 0.7480 0.9207 ## ## $`6` ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.959 0.959 1.119 1.151 1.311 1.407 boxplot(tips$tip_sz_prop~tips$size2, col = &quot;grey90&quot;, pch = 21, bg = 4, horizontal = TRUE) title(&quot;Tipping by size of party&quot;, xlab = &quot;Proportion of Tips ($)&quot;, ylab = &quot;Size of party&quot;) Looking at all these relationships, we can certainly include them in our report as they seem to give us some good information. So far we have quite a bit of useful information to report, but is that all we can report, how about some multivariate, could there be an interesting relationship between tips and two other variables? Let’s find out. 2.3.1.0.4 Describing multivariate relationships One of the first two variables we would be interested in are gender and smoking status, we want to know how they relate to tips received. mva_props &lt;- prop.table(tapply(tips$tip, list(tips$sex, tips$smoker), length)); mva_props ## No Yes ## Female 0.2213115 0.1352459 ## Male 0.3975410 0.2459016 tip_sex_sm &lt;- tapply(tips$tip, list(tips$sex, tips$smoker), function(i) i) tips$tip_m1_prop &lt;- unlist(sapply(1:4, function(i) tip_sex_sm[[i]]*mva_props[[i]])) mva_sum1 &lt;- tapply(tips$tip_m1_prop, list(tips$sex, tips$smoker), summary, simplify = FALSE) mva_sum1[[1]]; mva_sum1[[2]]; mva_sum1[[3]]; mva_sum1[[4]] ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.2177 0.5330 0.8428 0.9957 1.3089 3.5779 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.1352 0.4869 0.6877 0.8273 1.1066 2.3852 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.2705 0.4969 0.7832 0.8986 1.0549 2.4590 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.1488 0.4622 0.7512 0.7939 0.9836 3.0134 We can visualize our numeric summaries using box plot. This time our formula would be in the form of “y~x1+x2” boxplot(tip_m1_prop~sex+smoker, data = tips, yaxt = &quot;n&quot;, col = &quot;grey90&quot;, pch = 21, bg = 4, horizontal = TRUE) axis(2, 1:4, rep(c(&quot;Female&quot;, &quot;Male&quot;), 2), cex.axis = 0.7) mtext(c(&quot;Non-smoker&quot;, &quot;Smoker&quot;), 2, 2.5, at = c(1.5, 3.5), cex = 0.7) title(&quot;Tips received by gender and smoking status&quot;, xlab = &quot;Proportion of tips ($)&quot;) Figure 2.11: Tips by gender and smoking Our plot is certainly informative as we can see on average bill pates from smoking table tipped more than those from non-smoking table. It is also evident male bill payers generally tipped better. Now let’s look at tips received in relation to day of the week and time of day. mva2 &lt;- tapply(tips$tip, list(tips$day, tips$time), length) mva2 &lt;- ifelse(is.na(mva2), 0, mva2) mva2_props &lt;- prop.table(mva2) tip_dy_tm &lt;- tapply(tips$tip, list(tips$day, tips$time), function(i) i) tips$tip_m2_prop &lt;- unlist(sapply(1:8, function(i) tip_dy_tm[[i]]*mva2_props[[i]])) mva_sum2 &lt;- tapply(tips$tip_m2_prop, list(tips$day, tips$time), summary, simplify = FALSE) mva_sum2[[1]]; mva_sum2[[2]]; mva_sum2[[3]]; mva_sum2[[4]] ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.3146 0.4386 0.6685 0.8364 1.0748 2.1108 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.3125 0.5337 0.7987 0.9344 1.1095 3.5656 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0123 0.2273 0.7898 0.7754 1.1092 2.7027 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.8075 0.8075 0.8075 0.8075 0.8075 0.8075 mva_sum2[[5]]; mva_sum2[[6]]; mva_sum2[[7]]; mva_sum2[[8]] ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.3400 0.3912 0.5750 0.8389 1.2500 1.6750 ## NULL ## NULL ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.3700 0.6230 0.8534 0.9409 1.1018 2.3176 boxplot(tip_m2_prop~time+day, data = tips, xaxt = &quot;n&quot;, col = c(&quot;grey90&quot;), pch = 21, bg = 4, las = 1) title(&quot;Tips received by day and time of service&quot;, ylab = &quot;Tips&quot;) axis(1, 1:8, labels = FALSE) text(1:8, par()$usr[3]-0.3, labels = rep(c(&quot;Dinner&quot;, &quot;Lunch&quot;), 4), srt = 45, adj = 1, xpd = TRUE) mtext(c(&quot;Fri&quot;, &quot;Sat&quot;, &quot;Sun&quot;, &quot;Fri&quot;), side = 1, line = 3, at = c(1.2, 3.2, 5.2, 7.2)) Figure 2.12: Tips by day and time of service Like before we see something to report as regards relationship between tips, day and time of service. With that we now have enough information to report on. In the next section we go over all analysis done and see what we want to include in our report. 2.3.2 Findings to report Given our analysis thus far, we want to include the following in our report: Univariate description of all variables in terms of their measures of central tendency and dispersion All bivariate relationship between response and predictor variables (they all showed some interesting findings) The two multivariate descriptive analysis done (they showed some useful relationship) In our report, we will alternate between measure and displaying graphs, in other cases we shall show both, it just depends with additional value each will have (we want to strike a balance between comprehensive report and short-and-to-the-point kind of report). 2.3.3 Writing an Analytical Report In this section we are going to write a brief report on our findings. When writing a report, it is important that we know what we want to report on by undertaking a comprehensive data analysis as we have just done. It is usually ideal to begin with a shell, that is a a report with some headings, that way we are guided on flow of how to report our findings. First part of a report is an executive summary, though this section is the first, it is usually written as the last. Main report begins with descriptive statistics before proceeding to inferential or other statistics. Under descriptive statistics, we begin with findings on univariate data before reporting on bivariate and multivariate data. How sections are named depend on audience/purpose of the report. If it is a student paper, then it will follow a given format, if not, then we can craft heading to suite findings to be discussed. Both numerical summaries and graphical display are used in an analytical report. However, displays should not be many, actually it is good to show only those that have significant information than their numerical summary. But note, graphs are easier to interpret even for people who can’t read. With that said, here is a sample of a report we can give our friend. 2.3.3.1 Data Analysis Report on Tipping Data Executive Summary Data used for this report had 244 observations and 7 variables. This data had information on tips received by a restaurant waiter over a period of a few months. Other than tips received, other variables collected included total bills paid, gender of bill payer, presence of smokers in the party, day of the week served, time of service and size of party. Due to length differences in categorical groups, relationship between tips received and each group was analysed as proportions rather than actual values. Tips received during the months of observation ranged between 1 and 10 dollars and had an average of 2.90 dollars. Total bills paid during this time averaged to 17.80 dollars. There were more male bill payers (64%) than female bill payers (36%). About sixty two percent of the tables served had no smokers while about thirty eight percent had a smoker in their party. Saturday had the highest observations (about 36%) followed by Sunday (about 31%), then Thursday (about 25%) and finally Friday (about 8%). Between dinner and lunch, most observations were made during dinner time which comprised of about 72% of the observations and 28% were observed during lunch time. Size of party ranged between 1 and 6 with 2 being the most observed size. Based on this sample, there was zero to slight negative association between tips received and total bills paid. In terms of relationship between tips received and gender of bill payer, on average, male bill payers tend to be better tippers than female bill payers. Association between tips received and presence of smokers in the party revealed that smoking tables on average tipped more than non-smoking tables. When tips received was associated to day of the week, Saturday was noted as a better tipping day than the other three days (Thursday, Sunday and Friday). Relationship between tips received and size of party showed that parties with 2 or 4 patrons on average tipped better. When tips received was associated to both gender of bill payers and presence of smokers in party, an interesting finding was noted, unlike before, here female bill payers in both smoking and non smoking tables tend to be better tippers than than their male counterparts. Associating tips received by day and time of serve revealed that more tips were received on Friday lunch time than any other time. 2.3.3.1.1 About the data Data used in this report regards tips received by a restaurant waiter. Other variables collected included total bills paid in dollars, sex of the bill payer, smoking habit, day served (Friday, Saturday, Sunday or Thursday), time of day observation was made (Dinner or Lunch) and size of party (between 1 and 6). In total there were seven variables collected. Variable of interest was taken to be “tips received” and the other variables were considered as possible associative factors. Variables tips and total bill are numerical and continuous variables, while variable sex, smoking habit, day of the week, and time of service are categorical variables. Size of party was the only discrete variable in this data set. In all, there were 244 observations. 2.3.3.1.2 Tips and other factors Tips received ranged between $1 and $10 and had an, average of $2.9. Twenty five percent of tips received were below $2 and another twenty five percent were above $3.56. The graph below shows distribution of tips. Figure 2.13: Distribution of tips received Average total bill paid were $17.8. Fifty percent of these bills were between $13.35 and $24.13. Out of the top twenty five percent, there were a significant number of high bills paid. For this sample, there were more male bill payer than female bill payers, this is because male bill payers comprised of about 64% and females comprised of about 36%. Of all the tables served most had no smokers, 62% while 38% had smokers in their party. There were four service days Friday, Saturday, Sunday and Thursday. Day served most was Saturday (36%) followed closely by Sunday (31%), then Thursday (25%) and finally Friday (8%). Between dinner and lunch, most served time was dinner which accounted for about 72%. Lunch accounted for the rest of time which was about 28%. Based on this sample, size of party ranged between 1 and 6 with an average of 2. 2.3.3.1.3 Relationship between tips and other factors This section presents findings on relationship between tips received and other factors. 2.3.3.1.3.1 Tips and total bills When proportions of tips to their total bill are compared to total bill there seems to be a zero to slight negative relationship. Figure 2.14: Tips and total bill Tips and gender of bill payer On average, male bill payers tend to be better tippers than female bill payers. This can be seen on the graph below. Tips and presence of smokers Between smoking and non smoking tables, when weighted, smoking table had a higher average ($1.14) of tipping than non-smoking tables ($1.7) Tips and day of the week Out of the four service days, taking into account number of observation per each day, on average Saturday was the best tipping day followed by Thursday, then Sunday and finally Friday. Tips and time of service There wasn’t much difference between lunch time and dinner service, they had almost the same average (after taking into account weight of both groups), however, lunch time tips were more varied than tips from dinner service. Tips and size of party As regards tips received given size of party, in cognizant of distribution of each group, parties with 2 or 4 patrons on average gave better tips followed by parties with 1, 6, 3 and 5 patrons. There was noticeable variability in a number of these groups (size of party). Tips in relation to gender and presence of smokers Interestingly, when tips is broken down to gender and smoking status, female bill payers in both smoking and non-smoking tables tend to be better tip givers than male bill payer. Although with quite a bit of variability, on average, female bill payers from non-smoking table tend to give the most tips. Tips in relation to day and time of service Based on this sample, there were no tips received on Saturday and Sunday lunch time, there was also only one observation on Friday dinner time. Otherwise taking into consideration the difference in groups, on average there were more tips received on Friday lunch time than any other time. 2.3.4 Deliverables It is absolutely important to remember to handover the following documents when submitting an analytical report. Actual finished report (PDF, word, or combination of the two) Script used for final report (in R, this would ideally be a self-contained R markdown document with it’s source documents) Script used to do preliminary analysis. This is the script used to run all analysis even those not reported. This could be an exploratory data analysis script or a simple self-generating script running down all pre-identified analysis (in R, this would typically be a “.R” file). Final Data set (this data set could contain transformed variables), reason for adding this is because a client might change their data set for some reason and therefore will not be able to reproduce analysis done. It’s also a away of documenting changes made pre and during analysis. Files submitted have to be well titled, more so if they are source files and exploratory data analysis scripts. This will ensure the one receiving the documents can easily identify them and use them as need be. 2.4 Exercises Rate of change is change of one variable given change in another variable.↩ Discrete variables are numerical variables whose values take on certain values. Simply put, these are whole numbers or numbers without decimal notation hence cannot be divided for example number of students in a classroom, or number of cats in a household.↩ Continuous variables are numerical variables whose values can take any value within a range. These values have fractions or decimal places, for example normal human temperature is said to be between 36.1 and 37.5 centigrade or between 96.9 and 99.5 Fahrenheit.↩ Describes entire a population, they often unknown↩ Describes a fraction of a population or a sample and used to estimate population parameter↩ Geometric series is series of constant ratios between successive terms (Wikipedia).↩ https://en.wikipedia.org/wiki/Skewness#Other_measures_of_skewness↩ Bulmer, M.G (1979) Principles of Statistics (Dover). New York: Dover↩ "],
["math.html", "A Refresher Mathematics A.1 Ratios, Proportions, Percentages and Rates A.2 Introduction to Set Theory A.3 Basic Algebra A.4 Operations on Polynomials A.5 Factoring Polynomials A.6 Scientific Notation A.7 Rational Exponents and Radicals A.8 Linear Equations and Inequalities in One Variable A.9 Quadratic Equation A.10 Other Pre-Calculus Topics A.11 Elementary Functions A.12 Graphs and Transformations A.13 Introduction to Calculus References", " A Refresher Mathematics In this appendix, we go through a few mathematical concepts useful in understanding statistics. You can read this in one session or refer to specific sections as need be. Concepts we intend to refresh on are: Ratios, Proportions, Percentages and Rates Introduction to Set theory Basic algebra Operations on Polynomials Factoring Polynomials Scientific Notation Rational Exponents and Radicals Linear Equations and Inequalities in One Variable Quadratic Equations Other Pre-Calculus Topics Elementary Functions Graphs and Transformations Introduction to Calculus A.1 Ratios, Proportions, Percentages and Rates Ratios, proportions, percentages and rates are some of the most widely used mathematical concepts in reporting statistical outputs. They involve simple calculations but if not well understood they might lead to mis-reporting. It is for this reason that we go through these concepts in this section. A.1.1 Ratios Ratios are basically comparison of two numbers called terms, for example a comparison of the number of girls to boys in a classroom. They can also be viewed as a relationship between two numbers. Ratios are expressed as \\(a \\text{ to } b\\), \\(a \\text{ per } b\\), \\(a:b\\) or simply as a fraction. As an example, let us report number of female smokers to male smokers from this data. (tab1 &lt;- table(tips$sex, tips$smoker)) ## ## No Yes ## Female 54 33 ## Male 97 60 m &lt;- tab1[1, 2]/tab1[2, 2] Here are ways of reporting: There are 33 female smokers to 60 male smokers, or even better There is one female smoker for every 1.55 male smoker A.1.1.1 Proportions Proportions are fractions/parts of a whole. In statistics, proportions are used to quantify or determine representation of a given category/observations in a variable. For example, a teacher in a class would be interested in knowing fraction/proportion of girls/boys in a class. Given data shown below, let us try and compute proportions of each gender by their smoking habits. (gender.smoker &lt;- table(tips$sex, tips$smoker)) ## ## No Yes ## Female 54 33 ## Male 97 60 We should get this pt &lt;- prop.table(table(tips$sex, tips$smoke)); pt ## ## No Yes ## Female 0.2213115 0.1352459 ## Male 0.3975410 0.2459016 A.1.1.2 Percentages Percentages express part of a whole as a part of 100, simply put they are proportions multiplied by 100. From our previous example, we can get percentages by multiplying proportions by 100. perc &lt;- pt * 100; perc ## ## No Yes ## Female 22.13115 13.52459 ## Male 39.75410 24.59016 A.1.1.3 Rates Rates are ratios (comparison of two values) whose terms are measured in different units. For example, in athletics, we could measure athletes speed in terms of distance covered by time like 52 kilometers for four hours (52km per 4hrs). Rates can be reduced to unit rate which are rates expressed as quantity of 1. In our example, athletes speed can be noted as 13km per hour (13km/h). A.2 Introduction to Set Theory In our probability chapter, we will use a lot of set notation and concepts. For that reason in this section we refresh on elementary concepts of set theory. We shall review two issues: Set Properties and notation Set Operations A.2.1 Set Properties and Notation A set is a collection of objects or things. Sets are denoted by capital letters such as A, B and C. Objects in a set are called elements or members of a set. \\(\\in\\) notation is used to show an element is a member of a set, for example \\(c \\in A\\) which means \\(c\\) is an element in set \\(A\\). \\(\\notin\\) is used to show an element is not a member of a given set. A set without any element is referred to as an empty or null set. For example a set of all negative values in 1:10. Empty sets are denoted with \\(\\varnothing\\) notation. To refer to a complete set, it’s elements are enclosed in curly brackets for example “{1, 2, 3, 4, 5}” for set with numbers 1 through five. Other than listing all values in a set, a rule can be used to indicate properties of a set it contains, for instance a set of all values above a given number. Let us expound on this, suppose we had data on heights of children and we wanted only those above a certain height, we can express this in set notation by using a set builder like this: \\[\\{height | height_{x} &gt; 3ft 2inc\\}\\] Where: {} are used to denote a set (of variable height) | means “such that” (in other setting “|” can be replaced with “:”) This expression is read as, “a set of all heights such that height is greater than 3 feet and 2 inches”. Therefore variable height must contain height above 3ft 2inc. For each rule a listing can generated, listing are possible elements meeting rule condition. If a listing continues indefinitely then “…” can be used to show this pattern of continuity. For example: Rule Listing {x x is an alphabetical letter} {x x^2 = 9} {x x is an even number} First two examples are referred to as finite sets (elements can be counted and there is an end) while the second is referred to as infinite sets (there is no end to counting elements). If each element in a set is in another set for example all elements of set A are in set B, then set A is a subset of set B. Note, a set can also be a subset to it’s self, in this case they are said to be equal. Symbols used to denote these relationships are \\(\\subset\\) for subset, \\(=\\) for equality (two set have equal elements), \\(\\notin\\) for not a subset, and \\(\\neq\\) for two sets without same elements. It’s been been proven a null or empty (\\(\\varnothing\\)) set is a subset of all sets though this proof is beyond scope of this section. Set of all elements under consideration is called a universal set denoted by \\(U\\). A.2.2 Set Operations There are four basic set operations, these are: + Union + Intersection + Complement and + Difference Set operations are best shown using a venn diagram. A Venn diagram is a display showing all possible logical relationships between a finite collection of different sets. These diagrams consist of overlapping circles within a rectangle. Overlapping area indicates similar elements and rectangle indicates universal set. Union set Union of sets is a combination of all elements of sets under consideration. For example, union of set A with elements {a, b, c} and set B with elements {c, d, e} is {a, b, c, d, e}. Note we only have unique values in output of a union. Symbolically, A union B can be shown as \\(A \\cup B\\), where \\(\\cup\\) denotes union. Diagrammatically this can be shown as: Shaded area: A union B Intersection Intersect is a set of elements that are all sets of interest, basically a set of similar non-unique elements. Symbolically this can be shown as; \\(A \\cap B\\) where \\(\\cap\\) means intersection. Diagrammatically this can be show as: Shaded area: A intersect B Sets A and B are said to be disjoint if they share no similar element or \\(A \\cap B = \\varnothing\\). Disjointed sets Complement A complement is a set of elements not contained in a set of interest. For example, a universal set \\(U\\) contains all elements, and set \\(A\\) contains a few elements from this universal set, all elements in \\(U\\) and not in \\(A\\) are complements of \\(A\\). Complement set are denoted with \\(&#39;\\) for instance \\(A&#39;\\) which is read as “\\(A\\) prime”. Using a Venn diagram this can be shown as: Shaded area: complement set Difference Set A set containing only elements not contained in another set; unique elements. For example \\(A - B\\) are all elements in \\(A\\) not contained in \\(B\\). Shaded area: A - B A.3 Basic Algebra Understanding probability theory requires some basic knowledge of algebra which we will use to compute different probabilities. In this regard, in this section we shall look at core concepts in algebra like: Set of real numbers Real number line Real number properties and Fraction properties A.3.1 Set of real numbers A number system is a writing system used to express numbers; they are mathematical notations for representing numbers of a given set. There are several number systems but most often used number system is the “real number” system. A real number can be viewed as any number with a decimal representation. Table below shows set of all real numbers and some important subsets. Symbol Name Description Examples \\(\\mathbb{N}\\) Natural numbers Counting numbers (also positive integers) 1, 2, 3, … \\(\\mathbb{Z}\\) Integers Natural numbers, their negatives, and 0 …, -2, -1, 0, 1, 2, … \\(\\mathbb{Q}\\) Rational numbers Numbers which can be represented as a/b, where a and b are integers and b \\(\\neq\\) 0; decimal representations are repeating or terminating -4, 0, 1, 25, \\(\\frac{-3}{5}\\), \\(\\frac{2}{3}\\), 3.67, -0.33\\(\\overline{3}\\), 5.2727\\(\\overline{27}\\) \\(\\mathbb{I}\\) Irrational numbers Numbers which can be represented as non-repeating and non-terminating decimal numbers \\(\\sqrt{2}\\), \\(\\pi\\), 1.414213…, 2.71828182… \\(\\mathbb{R}\\) Real numbers Rational and irrational numbers Source: Barnett, R.A. A.3.2 Real number line All real numbers can be positioned as a point on a line referred to as a “real number line”. Each point on a real number line corresponds to one real number, this real number is called a coordinate of the point. Origin is the point with coordinate 0. Left side of a real number line are “positive real numbers” while on the right side are “negative real numbers”. Origin 0 is neutral as it is neither positive nor negative. Real number line A.3.3 Real number properties In order to convert algebraic expressions into equivalent forms, some basic properties of real number system are necessary. These properties will be especially useful when discussing calculus. Here we shall be reviewing four basic properties of a set of real number numbers, these are: Associative property Commutative property Identity property and Inverse property Associative refers to “grouping” or “regrouping” elements (note, this does not mean simplification). Commutative refers to how elements are moved around. An identity is a number which when added to another number equals to the same number. Inverse means opposite or reverse; an inverse is also another number on the real number line when combined on the left or right through operations (+ or *) outputs an identity value. Under each of these properties, we look at addition, multiplication, and distributive (combination of multiplication and addition) operations. As examples, we shall use \\(a, b, \\text{ and } c\\) as arbitrary elements in a set of real numbers \\(\\mathbb{R}\\). Addition Properties Associative When elements are grouped or regrouped in an addition computation, output remains the same. That is, whichever way these elements are grouped, output will remain constant. \\[\\therefore a + (b + c) = (a + b) + c\\] Commutative Commutative property of addition states that order of elements does not matter as it results to same output. \\[\\therefore a + b = b + a\\] Identity Here we are looking for a real number (identity) when added to another number results to that number; this number is zero. \\[\\therefore 0 + a = a + 0 = a\\] Inverse Additive inverse is “subtraction”, so for “a” a real number, it’s inverse is “-a”. \\[\\therefore a + (-a) = (-a) + a = 0\\] Multiplicative Properties Associative Just like additive associations, grouping or regrouping elements of a multiplicative operation results in the same output. \\[\\therefore a(bc) = (ab)c\\] Commutative Like commutative property of addition, order of a multiplication operation on elements of a real number line results to same output. \\[\\therefore ab = ba\\] Identity Identity for multiplication is 1 \\[\\therefore (1)a = a(1) = a\\] Where \\(a\\) is any real \\(\\mathbb{R}\\) number Inverse Multiplicative inverse (or reciprocal) is “division”, so for \\(a\\) a real number, it’s multiplicative inverse is \\(1/a\\). Note, “a” cannot be 0 as zero is not defined: 0 cannot be a divisor. \\[\\therefore a(1 \\div a) = (1 \\div a)a = 1\\] Distributive Properties Used when an operation involves both addition and multiplication. This property can also be referred to as “distributive property of multiplication over addition”. This property means that a term multiplied by other terms in parenthesis, simplification should be performed by “distributing” multiplication over terms in parenthesis. \\[\\therefore a(b + c) = ab + ac\\] also \\[(a + b)c = ac + bc\\] It is worth noting that, relative to addition, commutativity and associativity are used to change order of addition as well as insert or remove parenthesis as need be. However, the same cannot be done for subtraction and division. A.3.4 Additional Properties Using preceding operations (addition and multiplication), their subtraction and division can be expressed as: Subtraction For any real number a and b; \\[a - b = a + (-b)\\] Division For any real number \\(a\\) and \\(b\\) and where \\(b \\neq 0\\); \\[\\frac{a}{b} = a(\\frac{1}{b})\\] Zero Properties For all real numbers \\(a\\) and \\(b\\): \\(a * 0 = 0\\) \\(ab = 0 \\text{ if and only if } a = 0 \\text{ or } b = 0\\) A.3.5 Fraction properties Division in the form \\(a \\div b\\) and where \\(b \\neq 0\\) can be written as \\(\\frac{a}{b}\\). Top part of this division (element \\(a\\)) is called numerator and bottom part is called denominator. \\[\\frac{\\text{numerator}}{\\text{denominator}}\\] A.4 Operations on Polynomials In this section we refresh on one of the most frequently used mathematical concepts, and that is Polynomials. We shall discuss how to work with polynomials which form a core basis in most statistical models. But before that we re-look at exponents and specifically natural number exponents which we will use in our polynomials. Here are the core concepts we will be reviewing: Natural Number Exponents Polynomials Shape of Polynomials Combining like terms Addition and subtraction Multiplication Combined operations A.4.1 Natural Number Exponents Repeated multiplication of natural numbers (counting numbers or positive integers) \\(\\mathbb{N}\\) are often simplified by exponents. Exponents are real numbers \\(\\mathbb{R}\\) of multiplication repetitions or multiplication factor. Exponents are also called powers. For any natural number \\(a\\) multiplied by itself \\(x\\) times can be expressed as: \\[a^x\\] Where: \\(a\\) is called a base (natural number being multiplied) and \\(x\\) is called an exponent (multiplication factor) \\(a^x\\) is read, “\\(a\\) raised to the exponent of \\(x\\)”. Two often used exponents are two and three which is base multiplied by itself twice or thrice. Example: Exponential form Expanded form Output \\(2^2\\) 2 x 2 4 \\(2^3\\) 2 x 2 x 2 8 \\(2^4\\) 2 x 2 x 2 x 2 16 A.4.1.1 Exponents They tell us how many times a natural number should be multiplied A negative exponent means divide (inverse of multiplication) A fraction of an exponents like 1/n means taking nth root e.g \\(7^{\\frac{1}{2}}\\) = \\(\\sqrt{7}\\) and \\(21^{\\frac{1}{3}}\\) = \\(\\sqrt[3]{21}\\) Natural Sequence of Exponents Rule Example \\(a^1 = a\\) \\(3^1\\) = 3 \\(a^0 = 1\\) \\(3^0\\) = 1 \\(a^{-1} = \\frac{1}{a}\\) \\(3^{-1}\\) = 0.3333333 First property of Exponents This is also known as product of exponents properties. It is used to simplify multiplication of two natural number exponents with similar base. When two exponents with the same base are multiplied, their expanded form is the same as addition of exponents. For example, \\(2^3\\) x \\(2^3\\) can be expanded to (2 x 2 x 2) * (2 x 2 x 2) = 2 x 2 x 2 x 2 x 2 x 2; a total of 6 2’s. This can be simplified to \\(2^6\\) giving us 64. \\[\\therefore a^x * a^y = a^{x+y}\\] If there are constants with the same base, multiply them and then add their exponents. For example: \\[10x^2 * 5x^3 = (10*5)x^{2+3} = 50x^5\\] This leads us to our first (and most important) property of exponents; it states that, for any natural number \\(m\\) and \\(n\\), and any real number \\(b\\): \\[b^mb^n = b^{m+n}\\] The following properties can be reasoned in the same way as above: \\(x^m/x^n = x^{m-n}\\) \\((x^m)^n = x^{mn}\\) \\((xy)^n = x^ny^n\\) \\((x/y)^n = x^n/y^n\\) \\(x^{-n} = 1/x^n\\) A.4.2 Polynomials Algebraic expressions are numbers (constants/coefficients), symbols (variables like \\(x\\) and \\(y\\)) and operators (addition, subtraction, multiplication, division) grouped together to denote a value. Terms are individual objects of an expressions, that is, individual numbers or variables (symbols) or numbers and variables. A mathematical expression Polynomials are special algebraic expressions consisting of several terms. They are formed by constants, variables and non-negative integers exponents combined with addition, subtraction, and multiplication, but not division. Examples of polynomials Examples of non-polynomials \\(2x^4 + 3x^7 + 20\\) \\(\\frac{1}{x}\\) \\(2xy^2 + 5xy^3 + 2\\) \\(2x^{-2} + 5x^2\\) \\(2x + 3x + 1\\) \\(\\frac{a + b}{a^2 - b}\\) \\(5 or 0\\) Polynomials are constructed with one, two, three or more terms, for example a polynomial with: one variable is expressed by adding or subtracting constants and terms of the form \\(ax^n\\) two variables is expressed by adding or subtracting constants and terms of the form \\(ax^my^n\\) three variables is expressed by adding or subtracting constants and terms of the form \\(ax^my^nz^o\\) for more than three variables we use similar pattern as above A.4.2.1 Classifications of Polynomials by their degree Degree refers to highest exponent in a polynomial. Highest exponent for a one variable polynomial is simply its highest exponent, but for two or more variables, degree is the largest exponent after totaling exponents of each term. For example, \\(6xy^2 + 3xy^4 +2\\) is a 5 degree polynomial because first term has an exponent of \\(1 + 2 = 3\\), second term has an exponent of \\(1 + 4 = 5\\) and final term which is a constant has an exponent of 0. Degree can be written as deg(\\(6xy^2 + 3xy^4 + 2\\)) = 5 Below is a table with names of degrees for equations with one variable. Names of degrees Degree Name Example 0 Constant 5 1 Linear \\(2x + 10\\) 2 Quadratic \\(2x^2 + 5\\) 3 Cubic \\(2x^3 + x + 3\\) 4 Quartic \\(2x^4 + 2x^2 + 6\\) 5 Quintic \\(2x^5 + 3x + 2x^2 + 3\\) Note: higher order equations (those with high degree; &gt; 2) are harder to solve Polynomials are often written with highest degree first, this is called standard form polynomials of one variable are easy to plot as they have smooth and continuous lines A single term polynomial is called monomial, a two-term polynomial binomial and three-term polynomial trinomial A.4.3 Shape of polynomials Shape of a polynomial’s graph is connected to its degree; for odd-degree polynomials (\\(f(x)\\) = \\(x\\) or \\(x^3\\) or \\(x^5\\)), with a positive coefficient, graph starts from the negative and ends on the positive and across the x-axis at least once. For even polynomials (\\(f(x)\\) = \\(x^2\\) or \\(x^4\\) or \\(x^6\\)) with a positive coefficient, graph starts positive and ends in the positive. Even polynomials can cross x-axis once, twice or not all. Graphs of polynomial functions are continuous meaning they do not have holes or breaks. In addition, these graphs do not have sharp corners as one would expect from a graph of an absolute function. Each graph of a polynomial with a certain degree has an expected minimum number of vertices. Vertices for this continuous graphs are points separating an increasing portion and a decreasing portion or vice versa. In general, graph of a polynomial function of a positive degree \\(n\\) can have at most \\((n-1)\\) vertices which can cross the x-axis at most \\(n\\) times. op &lt;- par(&quot;mfrow&quot;) par(mfrow = c(2, 3)) # First-degree polynomial x &lt;- seq(-5, 5, 0.01) plot(c(-5, 5), c(-5, 5), type = &quot;n&quot;, ylab = FALSE) lines(x, 0.5*x, col = 4) title(&quot;First-degree polynomial&quot;, line = 1) title(xlab = &quot;x&quot;, ylab = &quot;h(x)&quot;, line = 2) title(sub = &quot;n = 1, therefore 0 vertices&quot;, line = 3) text(4.2, -1.4, labels = expression(paste(&quot;f(x) = &quot;, 0.5*x)), srt = 90) # Third-degree polynomial third &lt;- expression(x^3 - 2*x) third_vertices &lt;- c(-sqrt(2/3), sqrt(2/3)) x &lt;- sort(c(third_vertices, seq(-2, 2, 0.01))) plot(c(-5, 5), c(-5, 5), type = &quot;n&quot;, ann = FALSE) lines(x, eval(third), col = 4) title(&quot;Third-degree polynomial&quot;, line = 1) title(xlab = &quot;x&quot;, ylab = &quot;j(x)&quot;, line = 2) title(sub = &quot;n = 3, therefore 2 vertices&quot;, line = 3) points(third_vertices, y = eval(third)[which(x %in% third_vertices)], pch = 21, bg = 4) text(4.2, 0, labels = expression(paste(&quot;j(x) = &quot;, x^3 - 2*x)), cex = 0.9, srt = 90) # Fifth-degree polynomial fifth_vertices &lt;- c(-1.64443286, -0.5439123, 1.64443286, 0.5439123) x &lt;- sort(c(fifth_vertices, seq(-2, 2, 0.01))) fifth &lt;- expression(x^5 - 5*x^3 + 4*x + 1) plot(c(5, -5), c(5, -5), type = &quot;n&quot;, ann = FALSE) title(&quot;Fifth-degree polynomial&quot;, line = 1) title(xlab = &quot;x&quot;, ylab = &quot;f(x)&quot;, line = 2) title(sub = &quot;n = 5, therefore 4 vertices&quot;, line = 3) lines(x, eval(fifth), col = 4) x &lt;- fifth_vertices points(fifth_vertices, eval(fifth), pch = 21, bg = 4) text(4.2, 0, labels = expression(paste(&quot;f(x) = &quot;, x^5 - 5*x^3 + 4*x + 1)), cex = 0.8, srt = 90) # Second-degree polynomial x &lt;- seq(-2, 2, 0.01) second &lt;- expression(x^2 - 2) plot(c(-4, 4), c(-4, 4), type = &quot;n&quot;, ann = FALSE) title(&quot;Second-degree polynomial&quot;, line = 1) title(xlab = &quot;x&quot;, ylab = &quot;H(x)&quot;, line = 2) title(sub = &quot;n = 2, therefore 1 vertex&quot;, line = 3) lines(x, eval(second), col = 4) points(0, -2, pch = 21, bg = 4) text(3.7, 0, labels = expression(paste(&quot;H(x) = &quot;, x^2 - 2)), cex = 0.9, srt = 90) # Fourth-degree polynomial fourth &lt;- expression(2*x^4 - 4*x^2 + x - 1) fourth_prime &lt;- expression(8*x^3 - 8*x + 1) fourth_vertex &lt;- c(-1.0574538, 0.1270510, 0.9304029) x &lt;- sort(c(fourth_vertex, seq(-1.7, 1.6, 0.01))) plot(c(-5, 5), c(-5, 5), type = &quot;n&quot;, ann = FALSE) title(&quot;Fourth-degree polynomial&quot;, line = 1) title(xlab = &quot;x&quot;, ylab = &quot;J(x)&quot;, line = 2) title(sub = &quot;n = 4, therefore 3 vertices&quot;, line = 3) lines(x, eval(fourth), col = 4) x &lt;- fourth_vertex points(x, eval(fourth), pch = 21, bg = 4) text(4.2, 0, labels = expression(paste(&quot;J(x) = &quot;, 2*x^4 - 4*x^2 + x - 1)), cex = 0.8, srt = 90) # Sixth-degree polynomial sixth &lt;- expression(x^6 - 7*x^4 + 14*x^2 - x - 5) sixth_prime &lt;- expression(6*x^5 - 28*x^3 + 28*x - 1) sixth_vertices &lt;- c(-1.777750, 0.035760, 1.807227, -1.237497, 1.172260) x &lt;- sort(c(seq(-2.3, 2.3, 0.01), sixth_vertices)) plot(c(-5, 5), c(-5, 5), type = &quot;n&quot;, xlab = &quot;x&quot;, ylab = &quot;&quot;) lines(x, eval(sixth), col = 4) x &lt;- sixth_vertices points(sixth_vertices, eval(sixth), pch = 21, bg = 4) title(&quot;sixth-degree polynomial&quot;, line = 1) title(xlab = &quot;x&quot;, ylab = &quot;F(x)&quot;, line = 2) title(sub = &quot;n = 6, therefore 5 vertices&quot;, line = 3) text(4.2, 0, labels = expression(paste(&quot;F(x) = &quot;, x^6 - 7*x^4 + 14*x^2 - x - 5)), cex = 0.7, srt = 90) par(mfrow = op) A.4.4 Combining like terms Like terms are terms with similar variables and exponents but they could have different coefficients (constant preceding a term). For example \\(10x\\) and \\(6x\\) are like terms. Note, if a term has no constant before a variable, then coefficient is understood to be 1. If no constant appears but a negative (-) sign appears in front, then it is understood to be -1. Example: \\(5t^3 - t^3 + 6\\) has coefficients, 5, -1, and 6 There are some distributive properties which are necessary for the process of combining like terms, these are: a(b + c) = (b + c)a = ab + ac a(b - c) = (b - c)a = ab - ac a(b + c + … + f) = ab + ac + … + af Now let’s do one example of combining like terms: \\[10xy^2 + 2xy^2 + xy^2 + xy + 3\\] Like terms in this example are our first three terms: \\(10xy^2\\), \\(2xy^2\\), and \\(xy^2\\). \\(xy\\) is not a like term as \\(y\\) does not have exponent 2. \\[\\therefore 10xy^2 + 2xy^2 + xy^2 + xy + 3 = (10xy^2+2xy^2+xy^2) + xy + 3 = 13xy^2 + xy + 3\\] Note: Where parenthesis are present, we begin by clearing expressions in parenthesis using distributive properties then combine like terms. For example \\(9(x^2 + y^2) - 3(2x^2 - 3y^2)\\) can be simplified to \\(3x^2 + 18y^2\\) Always work with signs, it can either be positive or negative (except for 0 which is sign-less) A.4.5 Addition and subtraction Additions and subtractions of polynomials involves removing parentheses and combining like terms. Let’s add the following three polynomials as our example: \\[5x^2 - 2x + 6 \\\\ 2x^3 +x + 3 \\\\ -x^3 - 2\\] Additional arrangement \\[(5x^2 - 2x + 6) + (2x^3 + x + 3) + (-x^3 - 2)\\] Remove parentheses (factoring in signs) \\[5x^2 -2x + 6 + 2x^3 + x + 3 - x^3 - 2\\] Putting like terms together (from highest exponent) \\[2x^3 - x^3 + 5x^2 - 2x + x + 6 + 3 - 2\\] Simplify like terms \\[x^3 + 5x^2 - x + 7\\] Subtraction of polynomials follows similar procedures. A.4.6 Multiplication Multiplication of algebraic expressions like polynomials, requires extensive use of distributive properties for real numbers as well as other real number properties. For this, we shall use the following two polynomials: \\[(3x^3 - 2x^2)(9x^3 + x^2 + 5)\\] We multiply first term with all terms in the second polynomial then second term with all second polynomial’s terms. This should result in: \\[27x^6 + 3x^5 + 15x^3 -18x^5 - 2x^4 - 10x^2\\] Putting like terms together simplifies it to: \\[27x^6 - 15x^5 - 2x^4 + 15x^3 - 10x^2\\] Note: Products of binomials (two-term polynomials) factors occur frequently thus some handy formulas for their products have been given Special Products \\((a - b)(a + b) = a^2 - b^2\\) \\((a + b)^2 = a^2 + 2ab + b^2\\) \\((a - b)^2 = a^2 - 2ab + b^2\\) A.4.7 Combined operations For combined operations, polynomials will often have several grouping using different symbols like parentheses “()”, brackets “[]” and curly braces “{}”. To simplify these polynomials, it is best to remove these grouping symbols from inside, that is, from “()” to “[]” and finally “{}”. In terms of operations precedence, multiplication and division precede addition and subtraction while taking exponents precedes multiplication and division. As an example, let’s simplify this polynomial: \\[2 + \\{4x^2 - [4x^3 - 2x^2(x + 3)]\\}\\] Begin by removing inner “()” \\[2 + \\{4x^2 - [4x^3 - 2x^3 - 6x^2]\\}\\] Remove [] (multipling by -1) \\[2 + \\{4x^2 - 4x^3 + 2x^3 + 6x^2\\}\\] Remove “{}” \\[2 + 4x^2 - 4x^3 + 2x^3 + 6x^2\\] Now we simplify \\[-2x^3 + 10x^2 + 2\\] A.5 Factoring Polynomials In this section we look at concepts of factoring polynomials which can be quite handy in simplification and graphing. We will specifically look at: Common Factors Factoring by grouping Factoring second-degree polynomial Special Factoring Formulas Factoring Polynomials with rational zero’s theorem A.5.1 Common Factors This is an initial process of factoring and it involves factoring out factors common in all terms. Example Given \\[6z^2w^3 + 3z^4w^2 - 9z^2w^2\\] we can factor out a common factor which is \\(3z^2w^2\\) giving us: \\[3z^2w^2(2w + z^2 - 3)\\] A.5.2 Factoring by grouping Other than factoring out common factors, terms can be grouped in such a way that it make it efficient to complete factoring process for polynomials. There is no rule of the thumb here, but it is important to take into account sign of each term. Example Given this function: \\[6z^2 + 3z - 4z - 2\\] we can group it’s terms as: \\[3z(2z + 1) - 2(2z + 1)\\] Which become: \\[(3z - 2)(2z + 1)\\] If we multiplied these groups we should get our original function \\(6z^2 + 3z - 4z - 2\\). A.5.3 Factoring Second-Degree Polynomial Second-degree polynomials widely used in statistical models. Some of these polynomials can be simplified to first-degree polynomials with integer coefficients which makes it handy for a number of issues including determining points where \\(y = f(x) = 0\\). Since not all second-degree polynomials can be transformed to two first degree polynomials, then it is good to start off by checking if it is possible to transform them. We do this using a factorability evaluation called ac Evaluation. For a polynomial \\(ax^2 + bx + c\\) or \\(ax^2 + bxy + cy^2\\), we can determine if it has first-degree factors with integer coefficients by: taking a product of \\(a\\) and \\(c\\), that is \\(ac\\) and look for two factors of \\(ac\\) which sum up to \\(b\\) (coefficient of the second term) If these two factors exist, then polynomial has first-degree factors with integer coefficients and we can label these two factors as \\(p\\) and \\(q\\). Basically this, \\[pq = ac \\qquad{} \\text{ and } \\qquad{} p + q = b\\] must be satisfied. Therefore, once we know \\(p\\) and \\(q\\) exist, then we can use our “factoring by grouping” knowledge to formulate these two first degree polynomials. Example Given \\(9z^2 + 80z - 9\\), we begin by checking if we have \\(p\\) and \\(q\\) such that \\(pq\\) equals \\(ac\\). In this example, \\(a = 9\\) and \\(c = -9\\), thus \\(ac = -81\\). Two factors which sum to \\(80\\) are \\(-1\\) and \\(81\\), we therefore have \\(p\\) and \\(q\\) and as such we can factor it out using integer coefficients. We do this by substituting \\(b\\) with \\(p\\) and \\(q\\), grouping them and then factoring out common factors. \\[9z^2 - z + 81z - 9\\] \\[(9z^2 - z) + (81z - 9)\\] \\[z(9z - 1) + 9(9z - 1)\\] \\[(z + 9)(9z - 1)\\] Again if we multiplied \\((z + 9)\\) with \\((9z - 1)\\) we should get our original polynomial \\(9z^2 + 80z - 9\\). A.5.4 Special Factoring Formulas There are special factoring formulas generated to ease process of factoring certain polynomials which appear frequently these are: Perfect square: \\(u^2 + 2uv + v^2 = (u + v)^2\\) Perfect square: \\(u^2 - 2uv + v^2 = (u - v)^2\\) Difference of squares: \\(u^2 - v^2 = (u - v)(u + v)\\) Difference of cubes: \\(u^3 - v^3 = (u - v)(u^2 + uv + v^2)\\) Sum of cubes: \\(u^3 + v^3 = (u + v)(u^2 - uv + v^2)\\) Notice pattern being formed by differences, we are multiplying one first degree difference with it’s \\(n - 1\\) degree expanded expression. Therefore we can write difference of a fifth exponent as: \\[u^5 - v^5 = (u - v)(u^4 + u^3v + u^2v^2 + uv^3 + v^4)\\] Examples \\(9z^2 - 4y^2\\) is the same as \\((3z)^2 - (2y)^2\\) which can be factored out with difference of squares \\((3z - 2y)(3z + 2y)\\) \\(6(z - 2)^2 - 4y^2\\) can be factored to \\([3(z - 2) - 2y][3(z - 2) + 2y]\\) A.5.5 Factoring polynomials with rational zero’s theorem Factoring polynomials of higher degree (&gt; 3) can become quite challenging especially when using techniques already discussed. For that reason it might be good to use other methods. One method is Rational Zeros Theorem. This theory basically locates all \\(x\\) values which equate given function to zero. These \\(x\\) values are are called roots of a function. This theory uses coefficients of highest term and last term (constant). Its reasoning is that, roots of a function will often be a ratio of a factor of it’s constant and it’s leading coefficient. Symbolically we can express these possible roots of a function as \\(\\frac{p}{q}\\) where \\(p\\) is a factor of it’s constant and \\(q\\) is a factor of it’s leading coefficient. Take note not all \\(\\frac{p}{q}\\) will lead to a root, therefore we need to determine which among them yields a root. To do this we need to do four things: Arrange polynomial in a decreasing order, that means having highest degree term first and constant last. It also means that all degree terms must be given; for those that are not there a zero term can be added like \\(0x^3\\). Determine all factors of constant and leading coefficient, these includes their negative values. Compute all combinations of \\(\\frac{p}{q}\\) and eliminate any duplicates 1. Use division method to determine \\(\\frac{p}{q}\\)’s that are roots. Let us go over one example to grasp this concept. Example Given \\[j(x) = -5x^4 - 4x^3 + 42x^2 + 12x - 45\\] we want to find all its roots. From our basic algebra we know these must total to 4 since it is a fourth-degree polynomial. Our initial activity is to order our polynomial and include 0 terms where needed. Since our polynomial is in good order, then we can proceed to our next activity. From our equation \\(p\\) is -45 and \\(q\\) is -5, factors of \\(p\\) are 1, 3, 5, 9, 15, 45 and their negatives. Factors of \\(q\\) are 1, 5 and their negatives. Now we need to get all unique combinations of \\(\\frac{p}{q}\\). These are: p &lt;- c(1, 3, 5, 9, 15, 45) q &lt;- c(1, 5) possible_vals1 &lt;- expand.grid(p, q) possible_vals2 &lt;- expand.grid(p, -1 * q) possible_vals &lt;- rbind(possible_vals1, possible_vals2) p_over_q &lt;- possible_vals[,1]/possible_vals[,2] p_over_q &lt;- unique(p_over_q) p_over_q ## [1] 1.0 3.0 5.0 9.0 15.0 45.0 0.2 0.6 1.8 -1.0 -3.0 ## [12] -5.0 -9.0 -15.0 -45.0 -0.2 -0.6 -1.8 Our final step is to determine which among these \\(\\frac{p}{q}\\) are roots. We shall do this by dividing our function by a one degree polynomial formed by each of these \\(p/q\\)’s. Therefore we will have our equation as our divided and each of these one degree polynomials will be our divisor. Idea here is to determine which outputs a zero remainder. We should also note that dividing by a one degree polynomial leads to divided being a polynomial of a lesser degree. By reducing these polynomials we are left with polynomials which we can solve for \\(x\\) using previously discussed methods. For our initial division, we will have our dividend as \\[-5x^4 - 4x^3 + 42x^2 + 12x - 45\\] and our divisor for \\(\\frac{p}{q} = 1\\) as \\(x - 1\\) We can now determine its quotient and remainder as we do with any other division. \\[x - 1 )\\overline{-5x^4 - 4x^3 + 42x^2 + 12x - 45}\\] Core idea about this division is to determine terms which output zero when subtracted from a divided’s term but divisible by leading term of divisor. Therefore we begin by dividing first term of our divided with first term of our divisor, output should be able to equate first term to zero when it is subtracted. In this case \\(-5x^4\\) divided by \\(x\\) is \\(-5x^3\\) which we place above our division bar. \\[-5x^3\\\\ x - 1) \\overline{-5x^4 - 4x^3 + 42x^2 + 12x - 45}\\] We proceed by multiplying \\(-5x^3\\) by our divisor \\(x + 1\\) to get \\(-5x^4 + 5x^3\\) and place it right below our divided first two terms. \\[-5x^3\\\\ x + 1) \\overline{-5x^4 - 4x^3 + 42x^2 + 12x - 45}\\\\ -5x^4 + 5x^3\\qquad{} \\qquad{} \\qquad{} \\quad{}\\] We follow this by subtracting \\(-5x^3 + 5x^3\\) from \\(-5x^4 - 4x^3\\) and place output below line under \\(-5x^4 + 5x^3\\). \\[-5x^3\\qquad{}\\qquad{}\\qquad{}\\qquad{}\\qquad{}\\\\ x + 1) \\overline{-5x^4 - 4x^3 + 42x^2 + 12x - 45}\\\\ -5x^4 - 5x^3\\qquad{} \\qquad{} \\qquad{} \\quad{}\\\\ \\overline{\\qquad{}\\qquad{} -9x^3}\\qquad{}\\qquad{}\\qquad{}\\qquad{}\\] If we continues with this pattern then we should obtain a quotient of \\(-5x^3 - 9x^2 + 33x\\) and a 0 remainder. This means 1 is a zero root of \\(j\\). As you must have noticed, doing this division is rather involving but if we take a closer look we will see a pattern to simplify this process. Two things to take note in this pattern, leading term of our divisor is only used to clear terms in our divided (equating them to zero). The other thing to note is that our variables do not matter in our division as long as they are complete and in a decreasing order. What is of concern to us are coefficients of our divided and second term of our divisor. Given these facts, we should see that leading term of our quotient (\\(-5x^3\\)) has same coefficient asleading term of our divided (\\(-5x^4\\)) but with one degree less. Coefficient of our second quotient (\\(-9\\)) is a difference of second term of our divided (\\(-4\\)) and product of leading coefficient and second term of our divisor (\\(-5 * -1 = 5\\)). Coefficient of third term in our quotient (\\(33\\)) is a difference of coefficient of third term in our divided and product of second difference (\\(-9\\)) and second term of our divisor (\\(-9 * -1 = 9\\)). Fourth term of our quotient (45) is a difference of coefficient of fourth term of our divided (\\(12x\\)) and product of third difference (33) and second term of our divisor (\\(33 * -1 = -33\\)). To make this computation simple to work with, we will do additions rather than difference for our \\(p/q\\). For clarity, we will form a line with our \\(p/q\\) on our left and coefficients of our divided on on our right. We can then take totals after taking note that our leading coefficient will always be coefficient of leading term in our divided. In essence we should have something like this \\[1\\rfloor \\qquad{}\\qquad{} -5\\quad{}-4\\quad{}42\\quad{}12\\quad{}-45\\quad{}\\\\ \\qquad{}\\qquad{}\\qquad{} -5\\quad{}-9\\quad{}33\\quad{}45\\\\ \\text{_________________________________}\\\\ \\qquad{}\\qquad{} -5\\quad{}-9\\quad{}33\\quad{}45\\quad{}0\\] Since we have a remainder zero which is what we got with our division, then this reasoning is correct and we can do this with other \\(p/q\\). But before running through all other \\(p/q\\)’s , let us appreciate a few facts from this output. A remainder zero means we have reduced our fourth degree by 1 thus becoming a third-degree polynomial. \\[h(x) = -5x^3 - 9x^2 + 33x + 45\\] Something else to note is that we can take our new function \\(-5x^3-9x^3+33x+45\\) and use other methods to locate zeros. But with efficiency of computer programs, we can easily run through all our \\(p/q\\)’s using our simplified method and establish -3 is also a root \\(j\\). We are therefore left with two other roots to determine. coeffs &lt;- c(-5, -4, 42, 12, -45) n &lt;- length(p_over_q) remainder &lt;- sapply(1:n, function (i) ((((coeffs[1]*p_over_q[i]) + coeffs[2]) * p_over_q[i] + coeffs[3]) * p_over_q[i] + coeffs[4]) * p_over_q[i] + coeffs[5]) remainder ## [1] 0.000 -144.000 -2560.000 -32256.000 -257040.000 ## [6] -20782080.000 -40.960 -24.192 36.864 -16.000 ## [11] 0.000 -1680.000 -26640.000 -230400.000 -20054160.000 ## [16] -45.696 -36.864 40.320 zeros1 &lt;- p_over_q[which(remainder == 0)] To get these last two roots we need to use our new function \\(h\\). This function has different coefficients (-5, -9, 33, and 45) but has same \\(p/q\\) since constant and leading coefficents are similar. Therefore running through all our \\(p/q\\)’s again we get -3 as a root of \\(h\\). Since we did not get our two last roots, we can use reduced function from running -3. This new function is a second-degree polynomial \\[-5x^2 + 6x + 15\\] This is now much simpler function to work with as there are quite a number of methods for determining roots of a second degree polynomial. One method is a quadratic (second-degree polynomial) formula which we will discuss later, but for purposes of solving our second-degree polynomial we will mention how it is used. For a general quadratic equation \\[ax^2 + bx + c = 0 \\qquad{} a \\ne 0\\] \\(x\\) can be solve with this quadratic formula \\[x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\\] Therefore, from our quadratic equation \\(-5x^2 + 6x + 15\\), \\(a = -5, \\text{ }b = 6 \\text{ and } c = 15\\). We substitute this values in our formula \\[x = \\frac{-6 \\pm \\sqrt{-6^2 - 4(-5)(15)}}{2(-5)}\\] x1 &lt;- (-6 + sqrt((-6)^2 - 4*-5*15))/(2*-5) x2 &lt;- (-6 - sqrt((-6)^2 - 4*-5*15))/(2*-5) hx_at_zero &lt;- c(-3, -1.23303, 1, 2.43303) Output from this formula are our last two roots -1.2330303 and 2.4330303. In conclusion, \\(j(x) = 0\\) or roots of \\(j\\) occurs when \\(x = -3, -1.2330, 1 \\text{ and } 2.43303\\) A.6 Scientific Notation Large and small numbers are often expressed in exponential form for ease of writing and manipulation. This exponential form is said to be in “Scientific notation”. Numbers expressed in scientific notation are expressed as: \\[a * 10^x \\qquad{} 1 \\leqslant a &lt; 10\\] where \\(a\\) a decimal value and \\(x\\) is an integer This means a finite decimal value can be expressed as a product of a number between 1 and 10 and an integer exponent of base 10. Positive exponents means number is greater than or equal to 10, negative exponent means number is greater than 0 but less than 1, while zero exponents means number is greater than or equal to 1 but less than 10. A simple way to think of this is to count how many decimal places to add a zero (due to base 10). If exponent is positive, then we move decimal place to the right, if exponent is negative we move decimal place to the left. Examples: Decimal Notation Scientific Notation 100 \\(1 x 10^2\\) 1,000 \\(1 x 10^3\\) 9,600,000,000 \\(9.6 x 10^9\\) 0.2 \\(2 x 10^{-1}\\) 0.0000036 \\(3.6 x 10^{-6}\\) Most calculators and statistical programs can calculate in either decimal or scientific notation, but they often output scientific notation when value is large or small. Their scientific notation is often in form of a decimal value followed by letter \\(e/E\\) then exponent and it’s sign, for example 3.6e-06 for 0.0000036. A.7 Rational Exponents and Radicals In this section we discuss: Fractional exponents nth Root of Real Numbers Rational Exponents and Radicals Properties of Radicals A.7.1 Fractional exponents When a number is raised to a fraction or when an exponent is a fraction, it is called “fractional exponent”. For example: \\(9^{1/2}\\), \\(81^{1/3}\\) \\(10000^{1/4}\\). Below we see how fractional exponents are actually nth roots of it’s base. A.7.2 Nth Root of Real Numbers While an exponent is number of times a value is multiplied by itself, \\(nth\\) root is number of times it takes to get to its original value. For example, for \\(9^2 = 81\\), 2 is an exponent while 81 is it’s output, to get original value 9 from this output, we take square root of 81. Similarly, in \\(10^4 = 10000\\), taking fourth root of \\(10000\\) gets us back to \\(10\\). Nth is a generalization of root value (number of time we need to get original value) like 2nd and forth in our examples above. There are two frequently used roots, these are “square root” for 2nd root and “cube root” for third root. Incidentally, word “root” is used to mean origin just like a tree which originates from it’s roots. Positive numbers have two real nth roots if \\(n\\) is even like 2nd, fourth, and sixth. For example, 16 has -4 and 4 as it’s square roots and 10000 has -10 and 10 as it’s fourth root. Negative numbers have no real nth root if n is even, this means it is an error to take nth root of a negative number like \\(\\sqrt{-2}\\) or \\(\\sqrt[4]{-10000}\\). However, there is a complex number system used specifically for taking nth root of negative values. We shall not discuss this system here as it is way beyond the scope of this introductory or refresher session. Reason why an error occurs is because no real number raised to an even exponent can be negative. For odd n, there is only one real nth root for example \\(\\sqrt[3]{-100} \\approx -4.6\\) or \\(\\sqrt[5]{-960} \\approx -3.9\\) A.7.3 Rational Exponents and Radicals There two ways to represent nth root, with a square root symbol \\(\\sqrt{}\\) or as a fractional exponent. Initial representation using square root symbol is called nth root radical. \\(\\sqrt{\\quad{}}\\) symbol is called a radical, \\(n\\) is referred to as an index of radical and \\(a\\) is a radicand: \\[\\sqrt[n]{a}\\] Fractional exponent are also nth root because of first property of exponents, that is, \\(b^mb^n = b^{m+n}\\). Logic here is that, if you raise a number by a fraction and multiply this number and fraction as many times as n or denominator, then we get it’s original value since multiplying values with simila base means addition of their exponents. Take for example \\(27^{1/3}\\), if we multiply this value three time, first property of exponents tell us we need to add it’s exponential, that is \\(\\frac{1}{3} + \\frac{1}{3} + \\frac{1}{3} = 1\\) and any number raised to 1 is that number. Based on this fact, and that nth root gets us original value; this fractional exponent is thus an nth root. What we mean is that \\(27^{1/3}\\) is equivalent to \\(\\sqrt[3]{27}\\) which output 3. For any real non-negative number raised by a fractional natural numbers without similar prime factors and where n is even, rational exponents can be defined as: \\[a^{x/n} = \\begin{cases} (a^{1/n})^x &amp; = (\\sqrt[n]{a})^x\\\\ (a^x)^{1/n} &amp; = (\\sqrt[n]{a^x}) \\end{cases}\\] and \\[a^{-m/n} = \\frac{1}{a^{m/n}} \\quad{} a \\neq 0\\] Examples: \\(27^{2/3} \\quad{} = (27^{1/3})^2 \\quad{} = (\\sqrt[3]{27})^2 \\quad{} = 9\\) \\(27^{2/3} \\quad{} = (27^2)^{1/3} \\quad{} = \\sqrt[3]{27^2} \\quad{}= 9\\) \\(27^{-2/3} \\quad{} = \\frac{1}{27^{2/3}} \\quad{} = \\frac{1}{9}\\) Note: When index is 2, it often omitted and therefore that expression would have a radical and a radicad, for example \\(\\sqrt{16}\\) If there are two nth root, then positive root will often be outputted, this is called principal nth root. A.7.4 Properties of Radicals Following on properties of exponents, properties of radicals aid in changing or simplifying radicals. If \\(n\\) and \\(m\\) are natural numbers greater than or equal to 2, and if \\(x\\) and \\(y\\) are positive real numbers, then: \\(\\sqrt[n]{x^n} \\quad{} = x\\) \\(\\sqrt[n]{xy} \\quad{} = \\sqrt[n]{x} \\sqrt[n]{y}\\) \\(\\sqrt[n]{\\frac{x}{y}} \\quad{} = \\frac{\\sqrt[n]{x}}{\\sqrt[n]{y}}\\) Examples of simplification using properties of radicals: Using initial property: \\(\\sqrt{(4x^{3}2y)^2} \\quad{} = 4x^{3}2y\\) Using second property: \\(\\sqrt[3]{64} \\sqrt[3]{8} \\quad{} = \\sqrt[3]{64 * 8} \\quad{} = \\sqrt[3]{512} \\quad{} = \\sqrt[3]{8^3} \\quad{} = 8\\) Using third property: \\(\\sqrt{\\frac{xy}{100}} \\quad{} = \\frac{\\sqrt{xy}}{\\sqrt{100}} \\quad{} = \\frac{\\sqrt{xy}}{10}\\) A.8 Linear Equations and Inequalities in One Variable Linear equations are written as: \\[y = mx+b\\] or as an inequality \\[y \\geqslant mx+b\\] Both equations are first-degree polynomials. Inequality equations have symbols \\(&lt;\\), \\(&gt;\\), \\(\\leq\\), \\(\\geq\\) instead of \\(=\\). A value substituted for a variable in an equation to make it true is called a solution. For instance, 2 is a solution in equation below: \\[10 = 3x + 4 \\qquad{} \\to x = 2\\] Set of all solutions is called a solution set, hence solving an equation/inequality means finding it’s solution set. Two equations are said to be equivalent if they have similar solution set. Idea of equivalent equations is used to transform equations such that their solutions are simpler to get. For example, these two equations are similar \\[4x + 6 = 6x + 2\\] That is because they have a similar solution 14 which we obtain when we solve for \\(x\\). \\[\\text{Putting like terms together: }\\qquad{} 4x - 6x = 2 - 6\\] \\[ -2x = -4 \\qquad{} \\therefore x = 2\\] We can confirm 2 is a solution for both equations by substituting it for \\(x\\). \\[4(2) + 6 = 6(2) + 2 \\qquad{}\\text{which evaluates to } \\quad{}14=14\\] To solve linear equations, the following equality properties are used: Addition and Subtraction property: Same quantity is added or subtracted from each side of a given equation Multiplication and division properties: Same non-zero quantity is multiplied or divided on each side of a given equation Examples of solving linear equations: \\[5x - 2(3x + 4) = 2x - 2(3x - 3.5)\\] Opening brackets we get: \\[5x - 6x - 8 = 2x - 6x + 7\\] Putting like terms together we get: \\[-x - 8 = -4x + 7\\] Using addition and subtraction properties we get: \\[4x - x = 8 + 7\\] Using division property we get: \\[x = \\frac{15}{3} \\qquad{} \\therefore x = 5\\] We can confirm this is a solution for these equations by substituting 5 for \\(x\\) \\[5(5) - 2(3*5 + 4) = 2*5 - 2(3*5 - 3.5) \\qquad{} \\text{which evaluates to }\\quad{} -13 = -13\\] A.9 Quadratic Equation When there is one variable, quadratic equation can be expressed as: \\[ax^2 + bx + c = 0 \\qquad{} a \\neq 0\\] Where: \\(x\\) is a variable and \\(a\\), \\(b\\), and \\(c\\) are constants This equation is referred to as a standard form (note terms are in decreasing order). Core methods of solving quadratic equations are: Square rooting Factoring and Using a quadratic formula A.9.1 Solving quadratic equations by square root This method is used when a quadratic equation has no 1st degree, that is, \\(bx\\) is not there. This equation is expressed as: \\[ax^2 + c = 0 \\qquad{} a \\neq 0\\] Solving for \\(x\\) is done by square rooting both sides, that is: \\[\\text{if} \\quad{} a^2 = b, \\quad{} \\text{then} \\quad{} a = \\pm \\sqrt{b}\\] For example, for this equations: \\[3x^2 - 81 = 0\\] we solve for \\(x\\) by ading both sides with 81 and dividing 3 before square rooting both sides. \\[\\sqrt{x^2} = \\pm \\sqrt{\\frac{81}{3}}\\] This leads us to: \\[x = \\pm \\sqrt{27}\\] To confirm this, we can substitute value obtained for \\(x\\), \\(3\\sqrt{27^2} - 81=\\) 0. A.9.2 Solving quadratic equations by factoring To factor a number means to get pairs of numbers whose product outputs original number. These pairs are referred to as factors, and process of determing factors is referred to as factoring. As an example, number 10 has two pairs (four in total) whose product is 10, these pairs are 2 and 5 (25 = 10) and 1 and 10 (1 10 = 10). Here is a table with all factors for numbers 1 through 10. Number Factors 1 1 2 1, 2 3 1, 3 4 1, 2, 4 5 1, 5 6 1, 2, 3, 6 7 1, 7 8 1, 2, 4, 8 9 1, 3, 9 10 1, 2, 5, 10 For quadratic equations of the form \\[x^2 + bx + c = 0\\] Where \\(a = 1\\) solving for \\(x\\) involves finding factors of constant term (\\(c\\)) which add up to the middle term (\\(b\\)). These factors are used to form two equations whose product outputs original quadratic equation. These equations are in form of \\[(x + m)(x + n)\\] where \\(m\\) and \\(n\\) are our two factors. Let us look at an example of solving for \\(x\\) using factoring methods. Example 1 Given \\[x^2 + 6x + 9 = 0\\] we can begin solving for \\(x\\) by factoring it as \\[(x + 3)(x + 3) = 0\\] Note, product of these equations should output original equation. With that we can now solve for \\(x\\) by dividing both sides by \\((x + 3)\\) and then aadding -3: \\[\\frac{(x + 3)(x + 3)}{(x + 3)} = \\frac{0}{(x + 3)}\\] \\[x + 3 = 0\\] \\[x + 3 - 3 = 0 - 3\\] \\[\\therefore x = -3\\] Implication of sign of \\(b\\) and constant term \\(c\\) Sign of \\(b\\) and \\(c\\) have an implication on factors to be used. If \\(c\\) is positive both factor can be negative or positive; specifically if \\(b\\) is: positive, then factors should be positive negative, then factors should be negative Overall these factors must add up to \\(b\\) If \\(c\\) is negative then factors have alternating signs, that is, if \\(b\\) is: positive, then larger factor is positive negative, then larger factor is negative Overall, these factors should be \\(b\\) units apart. Example 2 Given \\[x^2 + 2x - 8 = 0\\] then our two equations are \\[(x + 4)(x - 2)\\] Since \\(c\\) is negative and \\(b\\) is positive. Solving for \\(x\\) we get a solution set with $x =-4 $ or \\(x = 2\\). When quadratic equation has a leading coefficient \\(a\\), and all coefficients (\\(a\\), \\(b\\) and \\(c\\)) have a shared factor, then factoring will begin by reducing these coefficients with their greatest shared factor. Example 3 Given \\[4x^2 - 24x + 36 = 0\\] We can solve for \\(x\\) by factoring 4 out as it is our greatest shared factor: \\[4(x^2 - 6x + 9) = 0\\] Factoring brackets: \\[4(x - 3)(x - 3) = 0\\] Multiplying both sides by \\(4(x-3)\\) \\[x - 3 = \\frac{0}{4(x-3)}\\] This thus lead to \\(x = 3\\). A.9.3 Solving using a quadratic formula When a quadratic equation cannot be solved by either square root or factoring methods, then a quadratic formula is used. To reason out this formula we need to begin by grasping an important concept known as completing squares. A.9.3.1 Completing square This converts a quadratic equation into a perfect square such that a standard form or general quadratic equation like \\[ax^2 + bx + c = 0\\] becomes \\[(x + A)^2 = B\\] Where \\(A\\) and \\(B\\) are constants. Core concept of completing squares is to make left side of a quadratic equation a perfect square which enables us to use square root method to solve for \\(x\\). Left side of a quadratic equation is transformed to a square by moving constant term across equals sign and then getting a third term which makes left side a square of a binomial (polynomial with two terms). For us to make left side a square of a binomial, we need to take note and check for possible pattern from an existing and well known square of a binomial, that is: \\[(a + b)^2\\] It’s exapanded form is a trimodal (polynomial with three terms) \\[(a+b)^2 = (a + b)(a + b) = a^2 + 2ba + b^2\\] Since our left side would be remaining with two terms given that we would have taken our constant across our equals sign, then we would need a third term. From our trimodal above we can see third term is a square of half coefficient of \\(a\\), that is, \\((\\frac{1}{2}.2b)^2 = b^2\\). As an example, given \\[x^2 - 4x -2 = 0\\] We can make our left side a square of a binomial by taking 2 across equals sign and adding left and right sides of our equation with square of half coefficinet of second term \\(-4x\\). \\[x^2 - 4x = 2\\] Square of half of 4 is four, therefore \\[x^2 - 4x + 4 = 2 + 4\\] Since \\(x^ - 4x + 4\\) is \\((x-2)^2\\), then \\[(x-2)^2 = 6\\] \\(x\\) is therefore \\(\\sqrt{8}\\) When a quadratic expression cannot be expressed as a perfect square (coefficient of second term in trinomial is not twice the second term in binomial), then it can be expresses as a sum of a square and a constant. For example, quadratic equation: \\[x^2 - 12x + 40\\] can be expressed with \\((x - 6)^2\\) but only if we added a constant 4. Adding this constant to make a perfect square is why this method is referred to as completing the square. A.9.3.2 Formulating the Quadratic Formula From foregoing discussion, we know that our third term in an expanded equation of as square of a binomial is a square of half it’s second term’s coefficient. Quadratic equation is formulated using this knowldge to solve for x. Therefore, from our standard quadratic equation \\[ax^2 + bx + c = 0 \\qquad{} a \\neq 0\\] We begin by eliminating coefficient of \\(x^2\\), that is \\(a\\). \\[\\frac{a}{a}x^2 + \\frac{b}{a}x + \\frac{c}{a} = \\frac{0}{a}\\] We then move our constant \\(\\frac{c}{a}\\) to our right side. \\[x^2 + \\frac{b}{a}x = -\\frac{c}{a}\\] Now we make our left side a perfect square by adding square of half coefficient of \\(x\\) in our second term, which is \\[(\\frac{1}{2} * \\frac{b}{a})^2 = \\frac{b^2}{4a^2}\\] \\[x^2 + \\frac{b}{a}x + \\frac{b^2}{4a^2} = \\frac{b^2}{4a^2} - \\frac{c}{a}\\] We can now convert out left side to a perfect square. \\[x^2 + \\frac{b}{a}x + \\frac{b^2}{4a^2} = (x + \\frac{b}{2a})^2\\] We also can simplify our right hand side. \\[ \\frac{b^2}{4a^2} - \\frac{c}{a}= \\frac{b^2 - 4ac}{4a^2}\\] Our equation now looks like this; \\[(x + \\frac{b}{2a})^2 = \\frac{b^2 - 4ac}{4a^2}\\] Square rooting both sides we get: \\[x + \\frac{b}{2a} = \\pm \\sqrt{\\frac{b^2 - 4ac}{4a^2}}\\] We can simplify our right hand side as: \\[\\frac{\\pm \\sqrt{b^2 - 4ac}}{\\pm \\sqrt{4a^2}} = \\frac{\\pm \\sqrt{b^2 - 4ac}}{2a}\\] Finally we can solve for \\(x\\) by subtracting both sides with \\(\\frac{b}{2a}\\). \\[x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\\] This is what is referred to as a quadratic formula. It can be used to solve for \\(x\\) in any quadratic equation when we cannot use square root or factoring methods. It is useful to note, \\(b^2 - 4ac\\) under our radical (\\(\\sqrt{\\quad{}}\\)) is called a discriminant. It gives information about expected solution, that is, a positive discriminant has two real solutions, a zero discriminant has one real solution and a negative discriminant has no real solution* * In the latter case, no real number can be obtained because we cannot take square root or (any even root like 4th or 6th) of a negative number. A.10 Other Pre-Calculus Topics In this sub-section we want to revisit these four mathematical concepts: Sequences, series, and summation and Arithmetric and Geometric Sequences nth-term formulas Factorial and Binomial Theorem A.10.1 Sequences, Series and Summation A.10.1.1 Sequences Sequences are often referred to as a successive list of numbers numbers, however, a sequences is a function which outputs this successive list of numbers or elements. For example a sequence of even numbers can be given by \\[a(n) = 2n + 2\\] where \\(a(n)\\) is used instead of \\(j(x)\\) Output of this sequence are called terms of a sequence, for example \\(a(1) = 4\\) is our first term, \\(a(2) = 6\\) is our second term, and \\(a(3) = 8\\) is our third term. For this sequence, ordered list of elements is 4, 6, 8, and so on. This ordered list of element is what is wrongly referred to as a sequence. Note, for our sequence \\(a(n) = 2n + 2\\), we can also refer to it in an abbreviated for such as {2n + 2}. Also note, since this sequence has no upper limit (it is not finite), then we refer to it as an infinite sequence. However, if a sequence has an upper limit, then we would refer to it as a finite sequence, like if we defined limit of \\(a(n)\\) as 100 (\\(2 &lt;= n &lt;= 100\\)). A.10.1.2 Series and Summation Notation A series is an operation consisting of addition of ordered finite or infinite sequence of terms like \\(a_1 + a_2 + a_3 + a_4\\) for a finite series and \\(a_1 + a_2 + a_3 + ...\\) for an infinite series. Here series is an operation of adding \\(a_i\\), one after the other. Series are often conviniently written using a summation notation (\\(\\sum\\)), with summation index written below and above summation notation like \\[\\sum_{i = 1}^{10}\\] where \\(i\\) indicates summing index which begins from 1 and end at 10. This summing index need not be represented by letter \\(i\\), it can also be other letter’s like \\(k\\) or \\(j\\). Here is an example of a finite series using letter \\(a\\) as a summing index: \\[\\sum_{a=2}^5 a^2 = 2^2 + 3^2 + 4^2 + 5^2 + 6^2\\] \\[ = 4 + 9 + 16 + 25 + 36\\] Summation notation for alternating series When we have an alternating sequence (negative and positive), then we need to show an alternating series. For example, for this sequence \\[a_k = (-1)^{k-1} * (a_{k-1})^2 \\qquad{} with \\quad{} a_1 = 2\\] we can write this series \\[a_k = \\sum_{k=2}^{4} (-1)^{k-1}(a_{k-1})^2 = 2, -4, 16, -256\\] A good example where summation of a series is used is in denoting computation of arithmetric mean, that is: \\[\\bar{x} = \\frac{1}{n} \\sum_{k=1}^{n} x_k\\] Where: \\(x\\) is range of values \\(n\\) is number of terms A.10.2 Arithmetric and Geometric Sequences A.10.2.1 Arithmetric sequences Arithmetic sequences have a constant difference “d” between each term. That means, for any term \\[a_1, a_2, a_3, ..., a_n, ...\\] there is a constant difference such that, \\[a_n - a_{n-1} = d\\] or \\[a_n = a_{n-1} + d \\qquad{} \\text{for every } n \\geqslant 1\\] Good example of arithmetic sequences are odd and even numbers, they both have a constant difference of two. Therefore, odd series is given by: \\[\\sum_{k=0}^n 2k + 1\\] and even series is given by: \\[\\sum_{k=0}^n k + 2\\] A.10.2.2 Geometric sequences Geometric sequences have a constant ratio “r” between terms. This means, for any term: \\[a_1, a_2, a_3, ..., a_n, ...\\] there is a constant ratio “r” such that: \\[\\frac{a_n}{a_{n-1}} = r\\] Example of terms in a geometric sequence are 2, 4, 8, 16, 32 which has a constant ration of 2, that is 4/2, 8/4, 16/8 and 32/16 equals to 2. A.10.2.3 Nth-Term Formulars Given that {\\(a_k\\)} is an arithmetic sequence with a common difference \\(d\\) between it’s terms, then we can build a pattern for a an nth-term. That is, we know: \\[a_2 = a_1 + d\\] \\[a_3 = a_2 + d = a_2 + 2d\\] \\[a_4 = a_3 + d = a_2 + 3d\\] then, \\[a_n = a_1 + (n-1)d \\qquad{} for \\space all \\space n &gt; 1\\] Similarly, we can formulate an nth-term for a geometric sequence by taking note of it’s sequence, that is: \\[a_2 = a_1r\\] \\[a_3 = a_2r = a_1r^2\\] \\[a_4 = a_3r = a_2r^3\\] \\[\\therefore a_n = a_1r^{n-1} \\qquad{} for\\space all\\space n &gt; 1\\] A.10.3 Factorials and Binomial Theorem A.10.3.1 Factorial This is a product of a positive integer and all positive integers below it, it is denoted by \\(n!\\), a notation introduced by “Christian Kramp”. More compactly, factorial function is defined as a product, that is: \\[n! = \\prod_{k=1}^nk\\] Where: \\(n\\) is the initial integer \\(k\\) sequence terms starting from 1 up to \\(n\\) \\(\\prod\\) means product This means: \\[1 * 2 * ... * (n-2) * (n-1) * (n)\\] But it is easier to think computationally as: \\[n * (n-1) * (n-2) * ... * 2 * 1\\] For example, \\(6!\\) means a product of 6 and all values below it, that is: \\[6! = \\prod_{k=1}^6 = 6 * 5 * 4 * 3 * 2 * 1\\] this should output 720. \\(1!\\) and \\(0!\\) equals to 1, latter case is due to convention that product of no numbers at all is 1. Factorials are applicable in many mathematical and statistical concepts. For example in algebra we use it as coefficient of the binomial formula, in combinatorics (which we will cover in our probability chapter), it used to determine different ways of arranging n-things and to facilitate expression manipulation. It might be of interest to note that: \\[n! = n * (n-1)! \\qquad{} n \\] This can make some computations efficient like: \\[\\frac{4!}{3!} = \\frac{4 * 3!}{3!} = 4\\] A.10.3.2 Binomial Coefficient One important formula we will use in our combinatorics section under probability chapter is the binomial coefficient. Binomial coefficient is used to count number of ways \\(n\\) objects can be arranged in \\(k\\) ways, for example how many ways can a class of 10 students select two representatives; order here does not matter (choosing Hellen and Janice or Janice and Hellen is similar). So in a way we are looking at how many ways elements can be grouped without considering it’s order. This is often described as n choose k, though some use different notations like \\(r\\) instead of \\(k\\). There are a number of ways to denote binomial coefficient, these include: \\[C_{(n,k)} = C_{n,k} = {_n}C_k = {_n}C^k = C_k^n = \\binom{n}{k}\\] Out of this, standard notation is \\(\\binom{n}{k}\\) although \\(C_n^k\\) is often used for it’s typing/writing convenience. To understand this formula for binomial coefficient, let us revisit class of 10 students example from which we want to select two “reps”. Expectation is that these two posts will have different people such that no student can occupy both posts. Question now is, how many ways can we select two different students or a unique pair of students for these posts without considering their order. Let’s reason it out, we have 10 students and two posts, for the first post all 10 students have an equal chance of filling it. Now suppose one student is selected, s/he is removed from possible selection in next post hence post two now has 9 possible candidates. So we can say there are 90 (10*9) possible way of filling these two posts. But let’s take note that these possibilities are counting same pair with different order to be two different pairs hence it would have “Hellen - Janice” and “Janice - Hellen” to be two different pairs yet they are similar. Therefore now we need to figure out how we can count only unique pairs. Consider a hat with 3 letters; \\(a,b, \\text{ and } c\\) and we want to select two letter. If we use earlier reasoning, we get that we have 3 * 2 or 6 possible ways we can select these letter. These are: matrix(c(&quot;a&quot;,&quot;a&quot;,&quot;b&quot;,&quot;b&quot;,&quot;c&quot;,&quot;c&quot;,&quot;b&quot;,&quot;c&quot;,&quot;a&quot;,&quot;c&quot;,&quot;a&quot;,&quot;b&quot;), ncol = 6, byrow = TRUE) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] &quot;a&quot; &quot;a&quot; &quot;b&quot; &quot;b&quot; &quot;c&quot; &quot;c&quot; ## [2,] &quot;b&quot; &quot;c&quot; &quot;a&quot; &quot;c&quot; &quot;a&quot; &quot;b&quot; Notice pairs in column 1 and 3, 1 and 5, 4 and 6 are similar, they just have a different order. To get unique pairs we need to find out how many ways we can arrange two letters differently and divide with our earlier output. For example, given letters \\(a\\) and say \\(b\\), how many ways can we arrange them? Here we have two objects and two spots, first spot has 2 possibilities and second has 1 hence we have 2*1 or 2 possibilities (\\(a\\) &amp; \\(b\\) or \\(b\\) &amp; \\(c\\)). To grasp this, suppose we had 4 letters and 4 spots, how many ways can we arrange them? It would be \\(4 * 3 * 2 * 1\\) or \\(4!\\). Going back to our three letters, we now know there are 6 unique pairs, and there are 2 ways of arranging two objects, hence we can get all possible pairs without considering their order by dividing 6 by 2. Therefore there are 3 unique pairs or there are three ways of selecting two letters from a total of 3 letters. combn(letters[1:3], 2) ## [,1] [,2] [,3] ## [1,] &quot;a&quot; &quot;a&quot; &quot;b&quot; ## [2,] &quot;b&quot; &quot;c&quot; &quot;c&quot; To our example on students, we know we have 90 possibilities, to get unique pairs we need to multiply by 2 * 1 or simply 2!. Therefore we get there are 45 possible ways of filling two post out of 10 objects. We can now generalize this reasoning to a formula. Given this would be quotient of total events by number of ways these events can be arranged, , then our numerator will be: \\[n * (n-1) * (n-2) * ... * (n - (k - 1))\\] where \\(n\\) is total number of objects and \\(k\\) is number of objects to be selected and our denominator will simply be \\(k!\\), hence our formula will be: \\[\\frac{n*(n-1)*(n-2)*...*(n-(k-1))}{k!}\\] We can improve on our numerator so it does not look so long or uncomprehesible. Recall \\(4!\\) can be written as \\(4 * 3!\\), then let’s use this concept to turn our numerator to a factorial. For our class example we had $10 * 9 $ as our numerator, if we made it a factorial \\(10!\\) then we would have \\(10*9*8*7*6*5*4*3*2*1\\) or simply \\(10*9*8!\\). We need to eliminate this \\(7!\\) and mathematically we can do this by dividing with the same number, that is \\(7!\\). Since \\(7!\\) is really \\((n - k)!\\), then we can revise our formula to something simpler as: \\[\\frac{n!}{k!(n-k)!}\\] This is what is referred to as a binomial coefficient and we will use it to formulate binomial theorem. A.10.3.3 Binomial Theorem In our previous section on polynomials, we saw how to expand a binomial \\((a + b)^2\\) to \\(a^2 + 2ab + b^2\\). Here we look at how to expand a binomial for any exponent. To do this we begin by taking note of pattern formed with exponents 1 through 5. \\[(a+b)^1 = a + b\\] \\[(a+b)^2 = a^2 + 2ab + b^2\\] \\[(a+b)^3 = a^3 + 3a^2b + 3ab^2 + b^3\\] \\[(a+b)^4 = a^4 + 4a^3b + 6a^2b^2 + 4ab^3 + b^4\\] \\[(a+b)^5 = a^5 + 5a^4b + 10a^3b^2 + 10a^2b^3 + 5ab^4 + b^5\\] From above outputs, we can observe this pattern: Number of terms in each expression is \\(n+1\\) like initial has 2 terms, second has three terms, third has four terms and so on. Exponent of initial term increases from left to right while that of the second term increases from right to left. For example, in our third expression (\\(a^3 + 3a^2b + 3ab^2 + b^3\\)) we see that our first term (\\(a^3\\)) has \\(a\\) raised to three while \\(b\\) is raised 0, second term (\\(3a^2b\\)) has \\(a\\) raised to 2 while \\(b\\) is raised to 1, third term (\\(3ab^2\\)) has \\(a\\) raised to 1 while \\(b\\) is raised to 2, final term (\\(b^3\\)) has \\(a\\) raised to 0 while \\(b\\) is raised to 3. Basically \\(a\\) is raised 3, 2, 1, 0 while \\(b\\) is raised by 0, 1, 2, 3; a factorial sequential terms. In each term, exponents sum up to \\(n\\). As an example, looking at \\(a^3 + 3a^2b + 3ab^2 + b^3\\), we see that initial term has exponents 3 and 0 totaling to 3, second term has 2 and 1 totaling to 3, third term has 1 and 2 totaling to 3, and fourth term has 0 and 3 totaling to 3. First and last terms have a coefficient of 1, second coefficient will have coefficient of \\(n\\), for subsequent terms we multiply coefficient of preceding term with exponent of \\(a\\) and then divide by index/position of that (previous) term. For example, to get coefficient of the third term in our fifth expression, we multiply previous term’s coefficient 5 with exponent of \\(a\\) 4 to get 20 and divide it by 2 which is the index (position) of that term hence we get 10. To get coefficient of the fourth term we multiply 10 by 3 to get 30 and divide by 3 to get 10. Based on this observed pattern, let’s try and expand \\((a+b)^6\\) First we are expecting our expression to have 7 terms, that is \\(6 + 1\\) \\[\\frac{}{1} \\frac{}{2} \\frac{}{3} \\frac{}{4} \\frac{}{5} \\frac{}{6} \\frac{}{7}\\] Second, we know exponents of \\(a\\) are decreasing from 6 to 0 while that of \\(b\\) are increasing from 0 to 6. We also know for each term these coefficients sum to \\(n\\). For now we will use \\(\\mathbb{N}\\) as a placeholder for our coefficients. \\[u^6 + \\mathbb{N}u^5v + \\mathbb{N}u^4v^2 + \\mathbb{N}u^3v^3 + \\mathbb{N}u^2v^4 + \\mathbb{N}uv^5 + v^6\\] For coefficients, we know initial coefficient is 1 and second coefficient is 6 (same as \\(n\\)). Subsequent coefficient will be a product of previous coefficient and exponent of \\(a\\) divided index of previous term, therefore we should get, \\((6*5)/2=\\) 15, \\((15*4)/3=\\) 20, \\((20*3)/4=\\) 15, \\((15*2)/5=\\) 6 and \\((6*1)/6=\\) 1. \\[\\therefore (u+v)^6 = u^6 + 6u^5v + 15u^4v^2 + 20u^3v^3 + 15u^2v^4 + 6uv^5 + v^6\\] Now we can formulate a formula given our observed pattern. This time let us start with our coefficients, we want to generalize a simple computation expression. For reference and reasoning purposes, let’s use coefficients of \\((a+b)^6\\) expanded, that is \\(6, 15, 20, 15 and 6\\) with \\(n\\) equal to 6. We obtained these coefficients by multiplying \\(n\\) by exponent of \\(a\\) and then dividing by index/position of the term. That is: For the second term we had a coefficient of 6 which is a product of first term’s coefficient 1 and \\(a\\)’s exponent 6. We divided this by index/position of previous term 1 thus giving us 6. 6 is actually our \\(n\\) hence we can generalize second term as \\[\\frac{n}{1}\\] Our third term had a coefficient of 15 which was obtained by multiplying 6 our \\(n\\) by 5 which is \\(n-1\\) divided by 2 which is position of previous term. For now let us take third term as follows but we will amend it slightly later. \\[\\frac{n(n-1)}{2}\\] For our fourth term, we had a coefficient of 20 which we obtained by multiplying previous coefficient 15 by 4 (exponent of “a”) and dividing by 3 (position of previous term). Basing our computation on \\(n\\) and thereby having computation linkied to start we get. \\[\\frac{\\frac{6 *5}{2} * 4}{3} = 20\\] notice if we took 2 below we essentially get \\[\\frac{6 * 5 * 4}{1 * 2 * 3}\\] Symbolically we can represent this as \\[\\frac{n(n-1)(n-2)}{1 * 2 * 3}\\] Given these, we should be able to see an interesting pattern. Numerator is a product of \\(n\\) and terms of a sequence of integers decreasing from \\(n\\) which correspond to number of terms. For example second term’s numerator is \\(6(6-1)\\) which is \\(6 * 5\\), third term’s numerator is \\(6(n-1)(n-2)\\) which is \\(6 * 5 * 4\\) and based on this pattern, fourth term’s numerator will be \\(6 * 5 * 4 * 3\\). With this sequence, that is \\(n, (n-1), (n-2), ..., (n-k)\\), where \\(k\\) is index /position of previous term, we can simplify it by converting it to a factorial. We do this by dividing \\(n!\\) by \\((n-k)!\\) (recall how we formulated factoria ). That will be our numerator. Denominator is product of terms of a sequence, that is for the second term it would be 1, for third term would be product of index of first term 1 and index of second term 2, third would product of index of first, second and third. Basically factorial of index of previous term (\\(k\\)), hence second term would be \\(1!\\), third term would be \\(2!\\), fourth term would be \\(3!\\) and so on. What we have now is literally a binomial coefficient, that is: \\[\\frac{n!}{k!(n-k)!}\\] This means coefficient of any term in an expanded binomial can be obtained by this binomial coefficient. For our variables, we know first variable like \\(a\\) would have it’s coefficients decreasing from n while that of \\(b\\) will be increasing from 0. To get exponent of any “a” we minus index of previous terms \\(k\\) from \\(n\\), that is \\(a^{n-k}\\). For \\(b\\) exponent will simply be index of previous term \\(k\\). We can now combine all we have noted to form this expression: \\[(a+b)^n = C_{n,0}a^n + C_{n,1}a^{n-1}b + C_{n,2}a^{n-2}b^2 + ... + C_{n,n}b^n\\] We can reduce it by using summation notations \\[(a + b)^n = \\sum_{k=1}^n \\binom{n}{k}a^{n-k}{k}\\] This is what is refered to as a Binomial Theorem and we can use it to expand a complete binomial or find a specific term in an expanded binomial expression. For example, given \\((y - 1)^20\\) we can get it’s tenth term as For this example, \\(n\\) is 20 and \\(k\\) is equals to 9 (index of previous term). \\[\\binom{20}{9}y^{20-9}(-1)^9 = \\frac{11!}{9!(11)!}y^{11}-1\\] where \\(n = 20\\) and \\(k = 9\\) (index of previous term) This should output \\(-167960y^{11}\\). A.11 Elementary Functions In this section we refresh on one of mathematics core concept, that is “functions”. As a build up to this concept we look at a Cartesian coordinate system and point-by-point graphing. A.11.1 Cartesian Coordinate System A Cartesian (rectangular) coordinate system is made up of two real number lines; horizontal and vertical. Together these number lines are called coordinate axes and individually as horizontal and vertical axis. They cross each other through their origin which is 0. Horizontal axis is referred to as x axis while vertical axis is referred to as y axis. Coordinate axes divide plane into four parts called quandrants. Points on a plane like “P” are plotted at a point where their vertical and horizontal lines intersect. For point “P” the vertical and horizontal lines intersect at point 5, 5. These two numbers written as ordered pair are coordinates of point “P”. For any point, it’s initial coordinate is referred to as abscissa and it’s second coordinate is referred to as an ordinate. Sometimes coordinates of a point are referred to in terms of axis labels, that is x coordinate and y coodinate. Origin is a point with coordinates (0, 0). An important point to note is that there is a one-to-one correspondence between points in a plane and elements in a set of all ordered pairs of real numbers. This note is what is known as fundamental theorem of analytic geometry. A.11.2 Point-by-Point Graphing Point-by-point plotting is a process of sketching a graph of an equation. To sketch a graph of an equation we need to plot enough points on a coordinate system like a Cartesian plane and connect these points with a smooth curve until graph’s apparent shape is visible. Points are ordered pairs or solution set. For example, for this equation \\[x = 2y + 3\\] Given integers 0 through 23 as \\(y\\), we can compute \\(x\\) as $x = 2*(0:10) + 3 = $ 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23 and plot \\(x\\) against \\(y\\): y &lt;- 0:23 x &lt;- 2*y +3 plot(y, x, type = &quot;l&quot;, col = 4, panel.first = grid(20)) title(&quot;Plot of x = 2y + 3&quot;) A.11.3 Introduction to Functions A.11.3.1 Definition In mathematics, a function is a rule, process or a method which links one set of elements to another set of element. This link is what is called correspondence. In mathematical functions, for each element in the first set, there exists one and only one correspondence (link/relationship) in second set. Elements in initial set are referred to as domain and range for second group of terms. Using these two terms (domain and range), we can define a function as one to which each element in our domain there corresponds one and only one element in our range. As an example, each cat has a heart rate measured by beats per minute (bpm). For this example domain would be a specific cat and it’s range would be that cat’s beats per minute (bpm). This example can be expressed as a mathematical function. However, if we were to look at cats (domain) and it’s owners (range), then this might not be defined as a mathematical function. That is because there is a possibility of a cat having more than one owner. Another example, if domain are natural numbers \\(\\mathbb{N}\\) and their range is their square, then this can be considered a mathematical function, however, if range of these natural numbers \\(\\mathbb{N}\\) were their square root instead of squares, then they would not be considered mathematical functions. This is because squares output only one value while square roots output two (negative and positive) values. A.11.3.2 Functions and Equations Equations are statements of equality containing one or more variables. Equality regards contents on left and right hand side of an equals sign \\(=\\). We have been using these equations like: \\[y = x^3 - 2(x) \\qquad{} x &gt; 1\\] In this equation, \\(x\\) is our input and \\(y\\) is our output, therefore, given a certain value of \\(x &gt; 1\\), we expect to get an output \\(y\\). We threfore note \\(y\\) is dependent on \\(x\\) and \\(x\\) is independent since any value can be used as long as it is greater than 1. This is why \\(x\\) is called an independent variable and \\(y\\) is called a dependent variable. Question now is, when do equations specify a function. Recall definition of a function, each domain can only have one and only one range. If we take domain to be inputs of an equation and range to be their outputs, then an equation can be defined as a function when an input has one and only one output. Our earlier example of \\(y = x^3 - 2(x) \\qquad{} x &gt; 1\\) outputs only one value hence it specify’s a functions. But \\(y = \\sqrt{x}\\) is not a function as square roots outputs two values. Graphically, if a vertical line intersects a graph at exactly one point, then to each \\(x\\) value there corresponds exactly one \\(y\\) value which means that equation is a function. Example, plot for equation \\(y = x^3 +2*x\\) If there are more than points on a graph where it intersects vertical line, then there are \\(x\\) values with more than one \\(y\\) value and therefore equation is not a function. y &lt;- sqrt(x) y2 &lt;- -sqrt(x) plot(x, y, ylim = range(c(y, y2))+0.2, type = &quot;l&quot;, col = 4, panel.first = grid(15)) title(expression(paste(&quot;y = &quot;, sqrt(x)))) abline(v = 0, col = &quot;grey60&quot;) abline(h = 0, col = &quot;grey60&quot;) lines(x, y2, col = 4) A.11.3.3 Function Notation Functions are named with alphabetical letters like \\(f\\) and \\(g\\). For example: \\[f: y = x^3 + 2x\\] \\[g: y = \\sqrt{x}\\] To specify domain value \\(x\\) for function \\(f\\) we often use symbol \\(f(x)\\) read as “\\(f\\) of \\(x\\)” or “\\(f\\) at \\(x\\)” thus replacing output or range \\(y\\) (\\(y = f(x)\\)). This is what we refer to as function notation. Writting our two functions in function notation we get \\[f(x) = 2x + 1\\] \\[g(x) = \\sqrt{x}\\] Now we can get a specific range or output of \\(f\\) or \\(g\\), for example, for \\(x = 4\\) \\(f(4) = 9\\) and \\(g(2) = 2\\). We need to take note that, if \\(x\\) is not in domain of a \\(f(x)\\), then \\(f(x)\\) would not exist, we commonly refer to this as \\(f\\) is not defined at x. \\(f(x)\\) can also be used to evaluate functions with expression which involve one and more than one variable. For example, given this function with two expression on it’s numerator \\[\\frac{f(x + y) - f(x)}{y}\\] And given this \\[f(x) = x^3 + 3x + 2\\] we can evaluate our function at \\(x = a\\) \\[f(a) = \\frac{((a^3 + y) + 3(a + y) + 2) - (a^3 + 3a + 2)}{y}\\] A.12 Graphs and Transformations In this section we shall go through some elementary functions often encountered in mathematics, then go over vertical and horizontal shifts, as well as reflections, expansions and contractions and conclude with piece-wise-defined functions. A.12.1 Some core functions There are six basic functions often encountered in mathematics, these are Identity function, Absolute value function, Square functions, Cube function, Square root function, and cube root function. Below are graphs of these functions, it is important to know their definition, domain, range and shape. We begin with identity function, whose range is equal to it’s domain; \\(x\\) values are similar to \\(y\\) values and they real numbers \\(\\mathbb{N}\\). Second plot is for an absolute function. An absolute value is a value’s distance from zero and denoted as \\(|x|\\) for any value \\(x\\). It can be defined as: \\[f(x) = |x| = \\begin{cases} x &amp;\\text{if } x \\geq 0 \\\\ 0 &amp; \\text{if } x = 0 \\\\ -x &amp; \\text{if } x &lt; 0 \\end{cases}\\] An absolute function is like \\(g(x) = |x|\\) where domain \\(x\\) are real numbers \\(\\mathbb{R}\\) and range is an interval from 0 to infinity \\([0, \\infty)\\) Our third plot is for a square function, that is \\(h(x) = x^2\\). Domain for this function are real numbers \\(\\mathbb{R}\\) and range is an interval from 0 to infinity \\([0, \\infty)\\) Fourth plot is for a cube function \\(m(x) = x^3\\). Both it’s domain and range are real numbers \\(\\mathbb{E}\\) m &lt;- x^3 plot(x, y, type = &quot;n&quot;, panel.first = grid(10), ann = FALSE) abline(v = 0, col = &quot;grey60&quot;) abline(h = 0, col = &quot;grey60&quot;) lines(spline(x, m), col = 4) title(expression(paste(&quot;m(x) =&quot;, x^3))) title(xlab = &quot;x&quot;, ylab = &quot;m&quot;, line = 2) Fifth plot is on square root function \\(n(x) = \\sqrt{x}\\) with domain and range being an interval between 0 and infinity (\\([0, \\infty\\)). n &lt;- sqrt(x[x &gt;= 0]) plot(x, y, type = &quot;n&quot;, panel.first = grid(10), ann = FALSE) abline(v = 0, col = &quot;grey60&quot;) abline(h = 0, col = &quot;grey60&quot;) lines(spline(x[x &gt;= 0], n), col = 4) title(expression(paste(&quot;n(x) = &quot;, sqrt(x)))) title(xlab = &quot;x&quot;, ylab = &quot;n&quot;, line = 2) Final plot is for a cube root function \\(p(x) = \\sqrt[3]{x}\\) with domain and range being set of all real numbers \\(\\mathbb{R}\\) negroots &lt;- -abs(x[x &lt; 0])^(1/3) p &lt;- c(negroots, x[x &gt;= 0]^(1/3)) plot(c(-3, 3), c(-3, 3), type = &quot;n&quot;, panel.first = grid(10), ann = FALSE) abline(v = 0, col = &quot;grey60&quot;) abline(h = 0, col = &quot;grey60&quot;) lines(spline(x, p), col = 4) title(expression(paste(&quot;n(x) = &quot;, sqrt(x, 3)))) title(xlab = &quot;x&quot;, ylab = &quot;n&quot;, line = 2) par(mar = op) A.12.2 Vertical and Horizontal Shifts Transformation occurs when a new function is formed by performing an operation on another function. Transformation involves: moving a graph from one position to another yet maintaining it’s shape re-sizing the graph A.12.2.1 Transformation by maintaining shape of graph When transformation does not involve changing shape of a graph, transformation is done by either rotating (turning), reflecting (flipping) or translating (moving/sliding) a graph. A.12.2.1.1 Rotating Rotation of a graph means turning a graph around a center. Note, distance from this center to any point on the shape remains the same. A.12.2.1.2 Reflecting This means “flipping” a graph across a line called a “mirror/central line”. When the graph is reflected, reflected image has the same shape as original and every point has similar distance from the central line or mirror line. Mirror line can be in any direction, that is diagonal,horizontal, or vertical. Here are two examples of triangles reflected across a diagonal and horizontal central line. A.12.2.1.3 Translation Translation simply means to move. In this case it’s a graph being moved and this movement does not involve rotation, re-sizing or anything else which would change shape of the graph. Translation of graphs is also referred to as shifts. Translation can either be horizontal or vertical, and referred to as Horizontal and Vertical translation. Here’s is a vertical translation of function \\(f(x) = x^3\\). Here’s a horizontal translation for a cube function \\(f(x) = x^3\\). A.12.2.2 Transformation by Resizing Re-sizing involves dilation, contraction, compression, and enlargement/expansion while maintaining shape of a graph. In order to maintain shape of a graph, re-sizing begins by drawing a line from a central point to each point on that graph. A resized graph is them produced by increasing or decreasing a simiar distance from each point along this lines. Here’s an example of re-sizing, it can be an expansion or contraction depending on original shape. If original shape was inner box, then re-sizing was an expansion, if original shape was outer shape, then re-sizing was done by contraction. A.12.3 Piecewise-defined functions Piece-wise-defined functions are functions which behave differently depending on input (x) value. Two examples of these kind of functions are: Different average score obtained by a student depending on number of hours spent revising and practicing Absolute function Suppose average score obtained by a student who has been studying and practicing for: zero and just about five hours is 40%, five and just about ten hours is 60%, ten hour upto twelve point five hours is 80% twelve point five hours and above is 100% (maximum score) We could represent this in a piece-wise function defined as follows: \\[S(t) = \\begin{cases} 8t &amp; \\text{if } 0 \\leqslant t &lt; 5\\\\ 6t &amp; \\text{if } 5 \\leqslant t &lt; 10\\\\ 8t &amp; \\text{if } 10 \\leqslant t \\leqslant 12.5\\\\ 100 &amp; \\text{if } t &gt; 12.5 \\end{cases}\\] Where: \\(S(t)\\) is score function (score is dependent on time spent revising and practicing) \\(t\\) is time in hours t &lt;- seq(0, 12.5, 0.001) t1 &lt;- t[t &gt;= 0 &amp; t &lt; 5] t2 &lt;- t[t &gt;= 5 &amp; t &lt; 10] t3 &lt;- t[t &gt;= 10 &amp; t &lt;= 12.5] t4 &lt;- c(12.501, 14, 15, 16) plot(c(0, 17), c(0, 101), type = &quot;n&quot;, xlab = &quot;Time (hours)&quot;, ylab = &quot;Estimated Score&quot;) lines(t1, t1*8, col = 4) lines(t2, t2*6, col = 4) lines(t3, t3*8, col = 4) lines(t4, c(100, 100, 100, 100), col = 4) points(c(0, 5, 10, 12.5), c(0*8, 5*6, c(10, 12.5)*8), pch = 21, bg = 4) points(c(5, 10), c(5*8, 10*6), pch = 21) title(&quot;Piecewise-defined functions&quot;) Note, coloured dot means dot is part of graph while transparent dot means dot is not part of graph. A.12.4 Linear Functions A.12.4.1 Intercepts Intercepts are point at which graph of a function crosses it’s axis. Point at which a graph crosses \\(x\\) axis is referred to as x intercept and where it crosses it’s \\(y\\) axis is referred to as y intercept. For example, if graph of a function crosses x axis with x vlaue 2, then 2 is that graph’s x-intercept. If a graph of a function crosses y axis with y value 3, then 3 is that graph’s y-intercept.Note if y-intercept exists, then 0 is in domain of that function and y-intercept can referred by \\(h(0)\\) for function \\(h\\). A.12.4.2 Linear functions, equations and Inequalities Linear functions produce graphs with a straight line. These functions are expressed as: \\[f(x) = mx + b \\qquad{} m \\neq 0\\] Where: \\(m\\) and \\(b\\) are \\(\\mathbb{R}\\) Domain (\\(x\\)) is a set of all real numbers \\(\\mathbb{R}\\) Range (\\(f(x)\\)) is also a set of all real numbers \\(\\mathbb{R}\\) A constant function is one where \\(m = 0\\), hence it’s function is given by: \\[f(x) = b\\] Where: Domain is a set of all real numbers \\(\\mathbb{R}\\) Range is a constant \\(b\\) Both linear and constant functions are first degree polynomials and also known as first-degree functions. Note: Since a function cannot have a domain (\\(x\\)) with two ranges (\\(y\\)), then graphs of functions cannot produce a vertical line. A constant function produces produces a horizontal straight line This means a linear function is a straight line that is neither horizontal nor vertical. A linear function can have more than one variable. A linear function with two variable is written as: \\[\\text{Standard Form } Ax + By = C \\] Where \\(A\\), \\(B\\) and \\(C\\) are real constants \\(A \\text{ and } B\\) are not both 0 Standard form can be rewritten as a linear function: \\[y = -\\frac{A}{B}x + \\frac{C}{B}\\] A.12.4.3 Slope of a line Slope is a measure of steepness of a line relative to \\(x\\), more specifically it is a ratio of change in \\(y\\) and change in \\(x\\). That is, given two points \\(P_1(x_1, y_1)\\) and \\(P_2(x_2, y_2)\\), slope is: \\[m = \\frac{y_2 - y1}{x_1 - x_2} \\qquad{} x_1 \\neq x_2\\] Where \\(m\\) denotes slope Change in \\(x\\) or horizontal change is also called Run while change in \\(y\\) or vertical change is also called Rise. Slope can thus be defined as: \\[m = \\frac{\\text{Vertical } \\Delta { (Rise)}}{\\text{Horizontal } \\Delta { (Run)}}\\] Where \\(\\Delta\\) means change Slope can be: Positive if line is diagonal and \\(y\\) is increasing as \\(x\\) increases Negative if line is diagonal and \\(y\\) is decreasing as \\(x\\) increases Zero (0) if line is horizontal Undefined if line is vertical It can be shown that, given a linear equation \\(y = mx + b\\), slope is equal to \\(m\\) while \\(b\\) is it’s \\(y\\) intercept, this is referred to as a slope-intercept form. If we know coordinates of two point of a line or if we know a line’s slope and coordinate of a point, we can find it’s equation using what is called a point-slope form. Point-slope form for a line with slope \\(m\\) that passes through (\\(x_1, x_2\\)) is: \\[\\frac{y - y_1}{x - x_1}=m \\qquad{} \\therefore y - y_1 = m(x - x_1)\\] A.12.5 Quadratic Functions Quadratic functions are second degree functions, they can be defined as: \\[f(x) = ax^2 + bx + c \\qquad{} a \\neq 0\\] Where \\(\\text{a, b and c}\\) are real numbers and domain is a set of all \\(\\mathbb{R}\\) numbers. Quadratic function produce a “u or inverted u” shaped-like graphs called parabola. As an example, let us plot \\(f(x) = x^2 + 5x - 3\\) and determine it’s intercepts. x &lt;- seq(-5, 9, 0.001) fx &lt;- function(x) -x^2 + 5*x + 3 plot(c(-5, 10), c(-10, 10), type = &quot;n&quot;, xlab = &quot;x&quot;, ylab = &quot;f(x)&quot;) lines(x, fx(x), col = 4) title(expression(paste(&quot;f(x) = &quot;, -x^2 + 5*x + 3))) abline(h = 0, v = 0, lty = 2) points(c(-0.54, 0, 5.54), fx(c(-0.54, 0, 5.54)), pch = 21, bg = c(4, 5, 4)) legend(&quot;topright&quot;, legend = c(&quot;x-axis&quot;, &quot;y-axis&quot;), pch = 21, pt.bg = 4:5) Y intercept is value of \\(y\\) as graph crosses y-axis, at this point \\(x = 0\\), therefore for this graph, y-intercept is \\(f(0) = 0^2 + 5(0) + 3 = 3\\). X intercept is where \\(f(x) = 0\\) and we can solve for \\(x\\) using Quadratic formula. Our input values for this formula are: \\(a\\) = -1 \\(b\\) = 5 \\(c\\) = 3 \\[\\therefore x = \\frac{-(5) \\pm \\sqrt{5^2 - 4(-1)(3)}}{2(-1)}\\] Computing all this we get: \\[x \\approx -0.54 \\text{ | } x \\approx 5.54\\] We conclude by saying, this parabola (graph of our quadratic equation) has two x-intercepts at about \\((-0.54, 0)\\) and about \\((5.54, 0)\\) and one y-intercept at \\((0, 3)\\). A.12.5.1 Properties of quadratic functions and their graphs From a quadratic function \\[f(x) = ax^2 + bx + c \\qquad{} a \\neq 0\\] transforming it to \\[f(x) = a(x - h)^2 + k\\] can present a number of useful properties. We discuss these properties below, but first we see how to transform this quadratic function. Transforming Quadratic Functions Let us transform our earlier example \\[f(x) = 2x^2 + 8x + 16\\] to \\(f(x) = a(x - h)^2 + k\\). We begin by partitioning terms with an \\(x\\) using parenthesis, this is what we want to convert to a perfect square. \\[(2x^2 + 8x) + 16\\] Next we want to factor out coefficient of \\(x^2\\) which is 2. \\[2(x^2 + 4x) + 16\\] we want to make what is in parenthesis a perfect square hence we need to use concept of completing squares to get last value \\(c\\). Whatever value we get, we need to subtract it outside our brackets for algebraic balance. \\[2(x^2 + 4x + \\_) + 16 - \\_\\] We know to complete this square we need to take half of our second term and square it, thus \\((\\frac{4}{2})^2 = 4\\). Note, this value 4 is multiplied by 2 (what we factored out of \\(a\\)) making it 8. So we subtract this 8 from 16 to get: \\[2(x^2 + 4x + 4) + 8\\] Now we can transform what is within parenthesis to be a perfect square; \\[2(x + 2)^2 + 8\\] To make it similar to \\(f(x) = a(x - h)^2 + k\\) we minus a negative \\(2\\). \\[2(x - (-2))^2 + 8\\] 1. Determining direction and breadth of a parabola We know a quadratic function has a curve or a parabola graph which faces upwards or downwards. We can determine it’s direction by looking at it’s initial coefficient \\(a\\). If \\(a\\) is positive, then it is facing upwards, if it negative then it is facing downwards. For example, from our earlier quadratic equation \\(f(x) = 2x^2 + 8x + 16\\), since \\(a = 2\\), then would know it is facing upwards even without plotting it. \\(a\\) can also tell us if our parabola is narrow or wide. If |\\(a\\)| is greater than 1, then parabola is narrower as it is increasing rapidly like 2 times if \\(a\\) is 2 or -2. If |\\(a\\)| is less than 1 like -0.5 or 0.5, then parabola would be wider as it is decreasing or increasing at a slower rate. 2. Determining presence and number of x-intercepts From a quadratic function’s discriminant, we can tell if it’s graph has an “x-intercept” or not, we can also determine if there is one or two. For \\(b^2 - 4ac &lt; 0\\) graph has no “x-intercept”, for \\(b^2 - 4ac = 0,\\) there is exactly one “x-intercept”, and for \\(b^2 - 4ac &gt; 0\\), graph has two “x-intercepts”. Locating parabola’s vertex Each parabola has a lowest point if it is facing upward and highest point if it is facing downward, this point is called a Vertex. If we can intuitively reason from our second form of a quadratic function \\(f(x) = a(x - h)^2 + k\\), we can either add or subtract from \\(k\\), that is, if initial term \\(a(x - h)^2\\) is positive, then we would be adding to \\(k\\), if it is negative then we would be reducing from \\(k\\) and if it is 0, then \\(f(x) = k\\). With that in mind, given a range of values (domain/x) we can either be adding or subtracting from \\(k\\) meaning from \\(k\\) we can either be increasing (if positive) or decreasing (if negative). \\(k\\) is therefore our maximum point if it is a downward facing parabola or minimum point if it is an upward facing parabola. To make \\(f(x) = k\\), we need to equate our first term (\\(a(x - h)^2\\)) to 0. We do this by making \\(x = h\\). That is: \\[f(h) = a(h - h)^2 + k = k\\] At this point \\(f(h) = k\\) parabola is at it’s minimum if it’s upward facing and \\(a &gt; 0\\) or maximum if it’s downward facing and \\(a &lt; 0\\). Therefore vertex point is given by (\\(h, k\\)). For our example with equation \\(2(x - (-2))^2 + 8\\), vertex point is (\\(-2, 8\\)). m &lt;- -15:12 jan &lt;- function(m) 2*(m - (-2))^2 + 8 plot(m, jan(m), type = &quot;l&quot;, col = 4) points(-2, 8, pch = 21, bg = 4) text(-2, 50, &quot;Vertex&quot;) Line of symmetry On one side of a vertex, parabola would increasing and on other side it would be decreasing. This means picking any two points on opposite sides with same distance from a vertex would have similar \\(f(x)\\). For example moving 8 points above and 8 points below gives us; \\[f(-10) = 2(-10 - (-2))^2 + 8 = 136\\] \\[f(6) = 2(6 - (-2))^2 + 8 = 136\\] If we drew a line passing through a vertex, it would split a parabola into two, this line of symmetry is referred to as axis. plot(m, jan(m), type = &quot;l&quot;) pts &lt;- c(-10, -2, 6) points(c(-10, -2, 6), jan(pts), pch = 21, bg = c(5, 4, 6)) lines(c(-2, -2), c(0, 370), lty = &quot;dashed&quot;) text(-2, 390, &quot;Axis&quot;) mtext(&quot;vertex&quot;, 1, at = -2) A.12.6 Exponential functions Following up on natural exponents and basic functions, we know that \\[f(x) = 2^x\\] and \\[g(x) = x^2\\] are not similar. \\(f(x)\\) is an exponential function and \\(g(x)\\) is a square function. Generally we can write an exponential function as: \\[f(x) = b^x \\qquad{} b &gt; 0, \\space{} b \\neq 1\\] Domain for this function is a set of all real \\(\\mathbb{R}\\) numbers and range is set of all positive real numbers. In this function, we are excluding base 1 as it outputs a constant. We are also excluding zero as anything less than 0 (negative values) will not output a real number. x &lt;- seq(-5, 5, 0.001) y &lt;- 2^(-x) plot(x, y, type = &quot;l&quot;, col = 4, xlab = &quot;x&quot;, ylab = &quot;y&quot;) If we plot \\(f(x) = 2^x \\qquad{} b &gt; 1\\) and it’s inverse \\(f(x) = b^{-x}\\), their graphs will be a reflection of each other across y-axis. y1 &lt;- 2^x y2 &lt;- 2^(-x) plot(c(-6, 6), c(0, 33), type = &quot;n&quot;, xlab = &quot;x&quot;, ylab = &quot;y&quot;) abline(v = 0) for(i in seq(5, 30, 5)){ segments(-0.1, i, 0.1, i) } lines(x, y1, col = 4) lines(x, y2, col = 5) From this graphs it is clear to see that they both pass through point(0, 1). Reason for this is because minimum positive exponent is 0 and as a exponent to any base would output 1, that is \\(b^0 = 1\\). We generate other useful properties for graphs of base greater than 1. These include: All graphs of exponent functions pass through point (0, 1) These graphs are continuous curves with no holes or jumps As curve of this graph approaches x-axis heading towards infinity it thins out but does not reach x-axis (horizontally aligned to x-axis), this line is referred to as an asymptote. A.12.6.1 Base e Exponential Function Any real number can be used as a base in exponential functions, however, there is one particular base that is extensibily used in a number of mathematical expressions and formulas modelling real-worlds phenomena, this is base \\(e\\). Base \\(e\\) is an irrational number named after Leonhard Euler (1707-1783). As with other irrational numbers base \\(e\\) cannot be represented with any finite decimal fraction but it can be approximated. Base \\(e\\) can be computed with expression: \\[1 + (\\frac{1}{x})^x\\] Using this expression we can compute some values of \\(e\\) \\(x\\) \\((1 + \\frac{1}{x})^2\\) 1 2 100 2.7048138 10,000 2.7181459 100,000 2.7182682 1,000,000 2.7182805 Continuing with this computation and as \\(x\\) increases, output approaches an irrational number 2.7183 but does not really reach there, instead it’s decimal notation continues on. This number is what is called \\(e\\), \\(e\\) to 15 decimal places is Let’s plot exponential function with base \\(e\\) and it’s reciprical \\(1/e\\) and see how they reflect each other across y-axis. x &lt;- seq(-4, 4, 0.001) y1 &lt;- exp(x) y2 &lt;- exp(-x) plot(c(-5, 7), c(0, 56), type = &quot;n&quot;, xlab = &quot;x&quot;, ylab = &quot;y&quot;) abline(v = 0) for(i in axTicks(2)) { segments(-0.1, i, 0.1, i) } lines(x, y1, col = 4) lines(x, y2, col = 5) legend(&quot;topright&quot;, legend = c(expression(e^x), expression(e^-x)), lty = 1, col = 4:5) Other than base \\(e\\), base 10 and base 2 are often used. Base 10 is often used as it corresponds to our base-10 number system. A.12.7 Logarithimic functions Logarithms are widely used concept in many disciplines, in statistics, it is often used to scale down (reduce) a wide range of quantities to smaller scopes. Domain of logarithms cannot be negative as there is no base which can be raise to any number to produce a negative value log(-100) ## [1] NaN Before discussing logarithmic functions, we need to understand two concepts, these are one-to-one functions and inverse of a function. A one-to-one function is a function whose range correspond to exactly one domain. That means no two domains (\\(x\\)) correspond to same range (\\(f(x)\\)). So for a continuous function that is either increasing or decreasing for all it’s domain values, then it can be called a one-to-one function. As an example, let us look at these two functions: \\[f(x) = 2^x\\] and \\[g(x) = x^2\\] \\(f\\) is a one-to-one function while \\(g\\) is not. This is because for any integer range for function \\(f\\), there can only be one domain, while for function \\(g\\) there can be two domains. Like if we had a range of 16, domain in function \\(f\\) would be 4, that is \\(f(4) = 2^4 = 16\\). In function \\(g\\) for a range of 16, we would have two possible domains, 4 and -4 that is, \\(g(4) = 4^2 = 16\\) as well as \\(g(-4) = -4^2 = 16\\). An inverse of a function is a function formed when right side of a one-to-one function is interchanged with that of the left hand side. That means, if point (x, y) is on a one-to-one function, then it’s inverse would be (y, x). Inverse only apply’s if function is a one-to-one function. Logarithmic functions are inverse of exponential functions. Therefore, given: \\[y = 2^x\\] it’s inverse is: \\[x = 2^y \\text{ or } y = log_2x\\] This equation finds how many times 2 is raised (multiplied by it’s self) to get \\(y\\). This is a logarithmic equation with base 2. It is read as “logarithm of \\(y\\) to base \\(b\\)”. This is only when \\(x = 2^y\\) and this is what we graph. For example, inverse of \\(y = 3^x\\) is \\(x = 3^y\\). We can plot these two equations on the same coordinate plane given a domain set of {-3,-2,-1,0,1,2,3} x1 &lt;- seq(-5, 5, 0.001) y1 &lt;- 3^x1 plot(-5:10, -5:10, type = &quot;l&quot;, lty = &quot;dashed&quot;, xlab = &quot;x&quot;, ylab = &quot;y&quot;) abline(h = 0, v = 0, col = 8) lines(x1, y1) lines(y1, x1, col = 4) Let’s look at some example of conversion from Logarithmic to Exponential Converting logarithmic to exponential Logarithm Exponent 1. \\(log_{10}100 = 2\\) \\(10^2 = 100\\) 1. \\(log_{2}8 = 4\\) \\(2^4 = 8\\) 1. \\(ln 7.389056\\) \\(e^2 = 7.389056\\) A.12.7.1 Properties of Logarithimic Functions There are a number of handy properties for logarithmic functions these are: \\(log_b 1 = 0\\) \\(log_b b = 1\\) \\(log_b b^x = x\\) \\(log^{log_bx} = x, \\quad{} x&gt; 0\\) \\(log_b MN = log_bM + log_bN\\) \\(log_b \\frac{M}{N} = log_bM - log_b N\\) \\(log_bM^p = \\text{p } log_bM\\) \\(log_bM = log_bN \\text{ if and only is } M = N\\) Just like exponential functions, base \\(e\\) and 10 are the most frequently used logarithmic bases. Base 10 logarithm is called Common logarithms while base \\(e\\) is called Natural logarithms. In most calculators, often used logarithm is base 10 and it is labeled “log” while base \\(e\\) or natural logarithm is labeled “ln” or “LN”. Hence notation form is: Common logarithms: \\(log \\text{ x} = log_{10} x\\) Natural logarithm: \\(ln\\text{ x} = log_e x\\) In R, function log() computes natural logarithm by default but it can also be used for other bases. Function log10() and log2() are specific for base 10 and base 2. A.13 Introduction to Calculus In this section we aim to discuss an important mathematical topic referred to as Calculus. Calculus will become handy in plotting graphs of different models and computing area under a curve. Generally, calculus involves change; how much something has changed. It has two core topic, these are derivatives and integrations. While derivative divides an object into smaller pieces to find change, integration joins these small pieces to find total change. Derivatives and integration are inverse of one another. A.13.1 Derivative As noted, derivatives involves diving an object into smaller piece so as to find it’s change. Question is when is this applicable and how do we go about it. Think of anything with a set of continuous values, for example a jogger could have an average jogging speed of say 6 kph (Kilometer per Hour) but actual speed would vary across jogging track, hence if asked speed at any particular point, it might vary from this average speed. With this in mind, to establish speed at any particular point (distance) in time, we need to make some point estimates or computation. To get speed at a particular point in time might not be possible and we can reason this out. Suppose we want to know speed at which our jogger has been jogging between her second kilometer with a time of 1/4hr and third kilometer with a time of 1/2hr. Given speed is computed as change in distance over change in speed, we can easily establish her speed between 2nd and 3rd kilometer as: \\[Speed = \\frac{3\\text{km} - 2\\text{km}}{\\frac{1}{2}\\text{hr} - \\frac{1}{4}\\text{hr}} = 4\\text{ kph}\\] Now if we wanted to know her speed at exactly 2 kilometers with same time of 1/4hr what do we expect, let us compute it we see. \\[Speed = \\frac{2\\text{km} - 2\\text{km}}{\\frac{1}{4}\\text{hr} - \\frac{1}{4}\\text{hr}} = \\frac{0}{0} = \\text{undefined}\\] We have something that is undefined, therefore our next possible solution is to get small portions of distance and time covered just around our interest of 2 km at 1/4hr. That is, go backwards a small distance from 2 kilometers and slightly ahead of 2 kilometers. Difference in this change should be a good approximation of distance covered at exactly two kilometers. Core issue here is to be as close as possible to 2 kilometers on either sides without really being at exactly 2 kilometers. Jogging Track Establishing this speed (at distance close to 2km but not being at exactly 2km) is what differentiation calculus is all about. Our particular aim in this subsection is to compute points on curve or non linear functions. Core concepts we will be going through are: Rate of change Slope Limits Derivative Derivative of constants, exponential Form, and Sums Derivatives of Products and Quotients and Chain Rule: Exponential Form A.13.1.1 Rate of change In our preceding discussion, we saw how to measure change at a particular exact point. Based on this let us look at an example of a change in a quadratic function and formulate a general function for measuring change. Suppose reproduction of a certain micro-organism can be modeled with this function (hypothetical example): \\[R(t) = t^2 \\qquad{} x &gt;0\\] Where: \\(R\\) means reproduction \\(t\\) is time in minutes We are given a graph of this function which depicts reproduction of microorganism for a time between 0 and 5 minutes. Now we seek to determine number of micro-organisms reproduced between 2 and 4 minutes. Here are our two points: x &lt;- seq(0, 5, 0.001) y &lt;- x^2 plot(x, y, type = &quot;l&quot;, xlab = &quot;Time (minutes)&quot;, ylab = &quot;# of Microorganisms&quot;, col = 4) title(&quot;Reproduction of microorganisms&quot;) points(c(2, 4), c(2^2, 4^2), pch = 21, bg = 4) segments(c(2, 4, 2), c(2^2, 2^2, 2^2, 2^2), c(4, 4, 4), c(2^2, 4^2, 4^2), lty = 2, lwd = 2) text(2-0.7, 2^2+0.1, labels = &quot;(2, 4)&quot;) text(4-0.7, 4^2+0.1, labels = &quot;(4, 16)&quot;) text(3, 5.1, labels = 2, cex = 0.8) text(3.7, 9, labels = 12, srt = 90, cex = 0.8) Number of microorganisms reproduced between 2 minutes and 4 minutes is computed as change in number of microorganisms divided by time in minutes, that is: \\[R(16) - R(4) = \\frac{16 - 4}{4 - 2} = \\frac{12}{2} = 6\\] We can use this example to formulate a general function for measuring change or more specifically what we refer to as average rate of change. Given function \\(y = f(x)\\), when one domain \\(x\\) changes from \\(a\\) to \\(a + h\\) (\\(h\\) being our change), then \\(y\\) will change from \\(f(a)\\) to \\(f(a + h)\\). It’s average rate of change in \\(y\\) to the changes in \\(x\\) is therefore given by: \\[\\frac{f(a + h) - f(a)}{(a + h) - a} = \\frac{f(a+h)-f(a)}{h} \\quad{} h \\ne 0\\] This mathematical expression is referred to as a difference quotient. Now recall our jogger example, let us use this difference quotient to establish speed at a particular point like 6 kilometers. We are told our jogger’s average speed can be modeled by: \\[Speed = x^3 -2x^2\\] Based on our earlier discussion, to establish speed at exactly 6km, we need a small change (\\(h\\)) before and after 6km. This change should be small enough to get as close to 6km without being at exactly 6km. We do this using small values of \\(h\\) (change); 0.1, 0.01 and 0.001 to compute speed before and after 6km. ## 5.9 5.99 5.999 6 6.001 6.01 6.1 ## h -0.10 -0.0100 -0.001 0 0.001 0.0100 0.10 ## Speed 82.41 83.8401 83.984 NaN 84.016 84.1601 85.61 We can see as \\(h\\) approaches 0 on both sides, speed is approaching 84 kph. We can therefore approximate speed at exactly 6km to be about 84 kph. ## 5.9 5.99 5.999 6 6.001 6.01 6.1 ## h -0.10 -0.0100 -0.001 0 0.001 0.0100 0.10 ## Speed 82.41 83.8401 83.984 84 84.016 84.1601 85.61 Mathematically we can describe this by saying: \\[84 \\text{ kph is a limit for average speed as } h \\text{ approaches } 0\\] Note word limit which we can describe as a value a function or sequence approaches. Symbolically we can express it as: \\[\\frac{f(6 + h) - f(6)}{h} \\to{84} \\quad{} \\text{as} \\quad{} h \\to{0}\\] Alternatively we can also express it as: \\[\\lim_{h \\to 0} \\frac{f(6 + h) - f(6)}{h} = 84\\] This value 84 is also referred to as instantaneous rate of change or in physics instanteneous velocity. Symbolically if limit exists, we can express this instantaneous rate of change at \\(x = a\\) as: \\[\\lim_{h \\to 0} \\frac{f(a + h) - f(a)}{h}\\] In most texts, instantaneous rate of change is usually shortened to just rate of change which should be distinguished from average rate of change. A.13.1.2 Slope Slope is also referred to as a Gradient and it’s a measure of steepness and direction of a line or part of a line connecting two points. Part of a line includes a secant which is a line passing through two points on a graph of a function. There are a number of ways to compute slope, here we will look at two methods. One method computes slope by using difference quotient, another method computes difference between distance in \\(y\\) direction over that of \\(x\\) direction. Let us begin by looking at slope of secant line using difference quotient. \\[\\text{Slope of secant line} = \\frac{f(a + h) - f(a)}{h}\\] In the following graph, slope is the steepness of secant (blue) line passing through points \\((a, f(a))\\) and \\((a+h, f(a + h))\\). As a specific example, we can graph function \\(f(x) = x^3\\), with a segment passing through point (\\(6, f(6)\\)) and (\\(6+2, f(6+2)\\)), when \\(a = 6\\) and \\(h = 2\\). and compute slope of this secant line as: \\[= \\frac{f(6+2) - f(6)}{2}= \\frac{8^3 - 6^3}{2}= \\frac{512 - 216}{2} = 148\\] In general slope of a graph of a function \\(y = f(x)\\) at point (a, f(a)) is given by formula for instantaneous rate of change that is: \\[\\lim_{h \\to 0} \\frac{f(a + h) - f(a)}{h}\\] Note: This is only possible if a limit exists (we will discuss in subsequent section instances where a limit would not exist). When we have one point, we can determine it’s slope by computing very near values below and above 2 without really being at 2. Slope would be value we would be approaching on left and right side of 2. Using our function and graph, let’s determine a limit of \\(a = 7\\) as \\(h\\) decreases. y &lt;- ((7 + h)^3 - 7^3)/h y[4] &lt;- ceiling(y[3]) matrix(y, nrow = 1, dimnames = list(&quot;fx&quot;, 2+h)) ## 1.9 1.99 1.999 2 2.001 2.01 2.1 ## fx 144.91 146.7901 146.979 147 147.021 147.2101 149.11 With that we can say 147 is our limit and an approximation of slope at this point (7, \\(7^3\\)). A line passing though this point (with this gradient) is called a tangent line. Basically a tangent is a line passing though exactly one point on a graph. We can now plot our tangent with a gradient of 147 and passing through (7, \\(7^3\\)). With our gradient, we know each point above or below would be in steps equal to gradient, hence point below 7, that is 6, would be \\(7^3 - 147\\) and point above would be \\(7^3 + 147\\). x &lt;- 1:10 y &lt;- x^3 plot(x, y, type = &quot;l&quot;) title(expression(paste(&quot;f(x) =&quot;, x^3))) points(7, 7^3, pch = 21, bg = 4) lines(6:8, c(7^3 - y[4], 7^3, 7^3 + y[4])) Picking any two points on this line should output a gradient of 147. The other method for computing slope as mentioned, looks at division of distance in \\(y\\) direction over that of \\(x\\) direction. That is, given two point (\\(x_1, y_1\\)) and (\\(x_2, y_2\\)) we can computer it’s slope as: \\[\\text{Slope} = \\frac{Rise}{Run}= \\frac{y_2 - y_1}{x_2 - x_1}\\] Note, slope or gradient is denoted by letter \\(m\\). Using this method, we can compute our two point (6, \\(6^3\\)) and (8, \\(8^3\\)) as: \\[m = \\frac{8^3 - 6^3}{8 - 6} = \\frac{512 - 216}{2} = 148\\] A.13.1.3 Limits Limit is a value we are approaching from left and right of a domain as we make change (\\(h\\)) smaller and smaller. We can express a limit as: \\[\\lim_{x \\to c}f(x) = L\\] Or \\[f(x) \\to L \\quad{} \\text{as } x \\to c\\] We have discussed an existing limit as a value being approached as \\(h\\) greatly decreases, now let us look at an example where a limit does not exist. We are given a function \\[f(x) = \\frac{|x^3|}{x^3}\\] and we want to establish if \\[\\lim_{h \\to 0} \\frac{|x^3|}{x^3}\\] exists or not. Here we want to look at what values we get for \\(f(x)\\) when \\(h\\) approaches 0 but not exactly at 0 itself. Using these values as our \\(h\\) ## -2 -1 -0.1 -0.01 -0.001 0 0.001 0.01 0.1 1 2 We can present our computed \\(f(x)\\) in this table: ## -2 -1 -0.1 -0.01 -0.001 0 0.001 0.01 0.1 1 2 ## f(x) -1 -1 -1 -1 -1 NaN 1 1 1 1 1 As expected, there is no value of \\(f(x) = 0\\), but what is of interest are our two values on left and right of 0. These values approach two different numbers, left side is -1 and right side is +1. Clearly there is a disjoint here. We refer to -1 as limit from the left and +1 as limit from the right, these two form concept of one-sided limits. For our example, since one-sided limits are not near any one specific number, then we conclude that there exists no limit at \\(x = 0\\). But we acknowledge that there exists a left and right limits. That is: \\[\\lim_{x \\to 0} \\frac{|x^3|}{x^3} \\quad{} \\text{does not exist}\\] We can graphically represent this as: plot(c(-2.2, 2.2), c(-1.5, 1.5), type = &quot;n&quot;, xlab = &quot;x approaching 0&quot;, ylab = &quot;&quot;) abline(v = 0, h = 0, col = 8, lty = &quot;dashed&quot;) lines(h[1:5]-0.003, fx[1:5], col = 4) points(c(-0.001, 0.001), c(-1, 1), pch = 21, cex = 0.7, col = 4) lines(h[7:length(h)]+0.005, fx[7:length(fx)], col = 4) title(&quot;Limit does not exist&quot;) From this graph we not only see a disjoint but we also see \\(f(x)\\) at 0 does not exist (blank point). Recall a transparent point indicates point is not given graph. Generally, for a limit to exist, left and right limits must exist and they must be approaching similar value. Symbolically, we can represent left-hand limit as: \\[\\lim_{x \\to c^{-}} f(x) = K\\] Where \\(K\\) is left-hand limit \\(x \\to c^{-}\\) means \\(x\\) approaches \\(c\\) from left side and \\(x &lt; c\\) For example, \\[\\lim_{x \\to 0^{-}} \\frac{|x^3|}{x^3} = -1\\] We can express right-hand limit as: \\[\\lim_{x \\to c^+} f(x) = L\\] Where \\(L\\) is right-hand limit \\(x \\to c^{+}\\) means \\(x\\) approaches \\(c\\) from right side \\(x &gt; c\\) For example \\[\\lim_{x \\to 0^{+}} \\frac{|x^3|}{x^3} = 1\\] A.13.1.3.1 Properties of limits Given two functions \\(f\\) and \\(g\\) with limits \\(L\\) and \\(M\\) existing and are real numbers \\(\\mathbb{R}\\): \\[\\lim_{x \\to c} f(x) = L \\qquad{} \\qquad{} \\lim_{x \\to c} g(x) = M\\] then Limit of an identity is equal to identity: \\(\\lim_{x \\to c}x = c\\) Limit of a sum is equal to sum of the limits \\(\\lim_{x \\to c} [f(x) + g(x)] = \\lim_{x \\to c} = L + M\\) Limit of a difference is equal to difference of the limits \\(\\lim_{x \\to c} [f(x) - g(x)] = \\lim_{x \\to c}f(x) - \\lim_{x \\to c}g(x) = L - M\\) Limit of a function with a coefficient is equal to product of coefficient and limit: \\(\\lim_{x \\to c} kf(x) = k \\lim_{x \\to c} f(x) = kL\\) for any constant \\(k\\) Limit of a product is equal to products of all limits: \\(\\lim_{x \\to c}[f(x).g(x)] = [\\lim_{x \\to c}f(x)][\\lim_{x \\to c}g(x)] = LM\\) Limit of a quotient is equal to division of the limits as long as denominator is not 0: \\(\\lim_{x \\to c}\\frac{f(x)}{g(x)} = \\frac{\\lim_{x \\to c}f(x)}{\\lim_{x \\to c}g(x)} = \\frac{L}{M} \\quad{} \\text{if } M \\neq 0\\) Limit of an nth-root is nth-root of that limit as long as limit is greater than 0 for even roots: \\(\\lim_{x \\to c} \\sqrt[n]{f(x)} = \\sqrt[n]{\\lim_{x \\to c}f(x)} = \\sqrt[n]{L} \\quad{} L &gt; 0 \\text{ for n even}\\) Using these properties, let us compute some limit: \\[\\lim_{x \\to 2}(2x^2 - x)\\] From first property we know \\[\\lim_{x \\to 2} x = 2\\] We also know when a term has a coefficient we compute limit then multiply with coefficient, thus \\[\\lim_{x \\to 2}2x^2 = 2 . 2^2 = 2 . 4 = 8\\] \\[\\therefore \\lim_{x \\to 2}(2x^2 - x) = 8 - 2 = 6\\] From these examples, we can build a more generalized limit of a function \\(f(x)\\). That is, for any real value \\(\\mathbb{R}\\) \\(c\\), we can write this limit \\[f(x) = \\lim_{x \\to c}(x^3 + 2x)\\] as \\[\\lim_{x \\to c}(c^3 + 2.c)\\] which function \\(f\\) at c; \\(f(c)\\) \\[\\therefore f(x) = \\lim_{x \\to c}(x^3 + 2x) = \\lim_{x \\to c}(c^3+2.c) = f(c)\\] We can use this knowledge to evaluate any limit. For example, for a polynomial function of the form: \\[f(x) = a_nx^n + a_{n-1}x^{n-1} + ... + a_0\\] limit of \\(f(x)\\) is given by \\[\\lim_{x \\to c}f(x) = \\lim_{x \\to c}(a_nx^n + a_{n-1}x^{n-1} + ... + a_0)\\] \\[= a_nc^n + a_{n-1}c^{n-1} + ... + a_0 = f(c)\\] We can therefore conclude by saying, limit of a polynomial function where \\(c\\) is any real number is expressed as \\[\\lim_{x \\to c} f(x) = f(c)\\] A.13.1.3.2 Limits of Difference Quotients Given a function, we can determine it’s limit using difference quotient. For example given: \\[f(x) = 3x + 2\\] and difference quotient of \\(x = 2\\) \\[\\lim_{h \\to 0} \\frac{f(2 + h) - f(2)}{h}\\] We can determine limit of this function as \\[\\lim_{h \\to 0} f(x) = \\frac{[3(2+h)+2] -[3(2)+2]}{h} = \\frac{(6+3h+2)-(6+2)}{h} = \\frac{3h}{h}\\] \\[\\therefore \\lim_{h \\to 0} f(x) = 3\\] Slope of a graph Now recall how we determined slope of a tangent line, we estimated it’s limit by getting smaller values of \\(h\\). Our \\(h\\) or change was approaching 0 without it being at exactly 0. Given what we have learnt with limits, we can now compute slope of any point without going through process of reducing \\(h\\) by using difference quotient. For example given function \\(y = f(x) = x^3 - 5x\\) and point (3, 12), we can determine slope of it’s tangent using difference quotient as follows: \\[\\lim_{h \\to 0} \\frac{f(3+h) - f(3)}{h}\\] Substituting with function to solve for \\(y\\) we get: \\[\\lim_{h \\to 0} \\frac{[(3 + h)^3 - 5(3+h)] - [3^3 - 5(3)]}{h}\\] From our binomial expansion section, we saw how to expand \\((3+h)^3\\) to \\(27 + 27h + 9h^2 + h^3\\), if we substitute it in our equation and make necessary computations we get: \\[\\lim_{h \\to 0} \\frac{27 + 27h + 9h^2 + h^3 - 15 - 5h - 27 + 15}{h}\\] We can now simplify this to: \\[\\lim_{h \\to 0} \\frac{22h + 9h^2 + h^3}{h}\\] Factoring our \\(h\\) in our numerator we get: \\[\\lim_{h \\to 0} \\frac{h(22 + 9h + h^2)}{h}\\] \\(h\\) and \\(h\\) cancel each other outputting \\(\\lim_{h \\to 0} (22 + 9h + h^2)\\), which means slope is 22. We can confirm this by our earlier method of reducing \\(h\\). yfx &lt;- (((3+h)^3 - 5*(3+h)) - (3^3 - 5*3))/h yfx[4] &lt;- ceiling(yfx[3]) matrix(yfx, nrow = 1, dimnames = list(&quot;fx&quot;, 3+h)) ## 1 2 2.9 2.99 2.999 3 3.001 3.01 3.1 4 5 ## fx 8 14 21.11 22 21.991 NaN 22.009 22.0901 22.91 32 44 cat(&quot;\\n&quot;, yfx[4], &quot;is our limit as h approaches 0 and f(x) approaches 3 \\n&quot;) ## ## 22 is our limit as h approaches 0 and f(x) approaches 3 A.13.1.4 Derivative Given our preceding discussion, we can now locate slope of any secant or tangent line. For tangent line, we need to establish it’s limit which we agreed would suffice as an approximation of it’s slope. So for each point we have to compute a limit to get it’s slope. Now let’s consider a more efficient way to determine limit of any point given a function. We want to generalize one limit which we can use to determine slope of any point on graph. By doing this we are not only making this process efficient but we are also establishing a relationship between a function and slope of a tangent line at any point on the graph of that function. For example, if we had a graph \\(y = f(x) = x^2\\), we can establish limit for any point \\(a\\) using a two step process. Initial step involves getting slope of a secant line using difference quotient and step two involves getting slope of a tangent. Step 1 \\[\\frac{f(a + h)-f(a)}{h} = \\frac{(a+h)^2-a^2}{h} = \\frac{a^2+2ah+h^2-a^2}{h}\\] \\[\\therefore \\frac{f(a+h) -f(a)}{h} = 2a + h \\qquad{} h \\ne 0\\] Step 2: Limit of difference quotient Slope of any tangent line is given by: \\[= \\lim_{h \\to 0} \\frac{f(a+h) - f(a)}{h} = \\lim_{h \\to 0}(2a+h) = 2a\\] Therefore for any graph of \\(y = f(x) = x^2\\), slope of any tangent line on this graph would be given by \\(2a\\). Let us try to get slope of any tangent line with graph \\(y = g(x) = x^3\\). Step 1 \\[\\frac{f(a+h)-f(a)}{h}= \\frac{(a+h)^3-a^3}{h}= \\frac{a^3 +3a^2h+3ah^2+h^3-a^3}{h}\\] \\[\\therefore \\frac{f(a+h)-f(a)}{h} = 3a^2+3ah+h^2 \\qquad{} h \\ne 0\\] Step 2 Slope of a tangent line of this graph is given by: \\[\\lim_{h \\to 0}\\frac{f(a+h)-f(a)}{h}= \\lim_{h \\to 0}(3a^2 + 3ah+h^2)= 3a^2\\] What we can be note in both \\(f(x) = x^2\\) and \\(g(x) = x^3\\) is that slope of any tangent line on their graph is a function of \\(a\\). \\(a\\) being point of tangency (point on on a graph/curve). We know how to generalize this process of determining slopes of tangent lines along a graph, now let us establish relationship between this generalized slope with function of it’s graph. Relationship between slopes of tangent lines along a graph and function of it’s graph Given this plot Let’s determine slope of points with x value -4, -3, -2, -1, 0, 1, 2, 3 and -4. Since this is a square function, then we expect slope at any point on this graph to be \\(2a\\) or in this case \\(2x\\). That means graph’s steepness at each point is twice it’s domain at that point. For our graph, although we know it is a square function, we cannot be certain by mere observation that steepness at each point on this graph is twice that of it’s domain at that point, for example, look at these three graphs, they are all square functions but they all have different slopes. x &lt;- seq(-4, 4, 0.001) y1 &lt;- function(x) 2*x^2 y2 &lt;- function(x) x^2 y3 &lt;- function(x) 0.5*x^2 plot(c(-5, 5), c(0, 40), type = &quot;n&quot;, xlab = &quot;x&quot;, ylab = &quot;y&quot;) lines(x, y1(x), col = 4) lines(x, y2(x), col = 5) lines(x, y3(x), col = 6) legend(&quot;topright&quot;, legend = c(expression(2*x^2), expression(x^2), expression(0.5*x^2)), title = &quot;f(x) =&quot;, lty = 1, col = c(1, 4, 5), cex = 0.8) Slope in this case is determined by coefficient of \\(x^2\\) which we referred to as \\(a\\). When this coefficient is greater than one like \\(2x^2\\), then graph is much steeper and when it is less than 1 like \\(0.5x^2\\), then it is less steeper. So we need to establish a function for any slope of a tangent lines along our graph. To do this we use one vital clue from our graph, our vertex or minimum point (0, -2). We can use this point to determine general slope of tangent lines along our graph using our square form of a quadratic equation, that is \\(a(x-h)^2 + k\\). From our discussion on vertices, we know \\(h = k\\) and \\(k\\) is distance from x-axis to minimum point, that is \\(y\\) which is -2. Given this fact and x being 0, then our equation is now: \\[a(0+(-2))^2 - 2 = 4a - 2\\] Taking 2 to the other side we get: \\[4a = 2\\] Dividing both sides with 4 we get: \\[a = \\frac{2}{4} = 0.5\\] Therefore general slope for our graph is not standard \\(2a\\) but rather a fraction of it, that is \\(0.5a\\). We can now compute slope for given \\(x\\) values: x Slope of Tangent: line at (x, f(x)) -4 -2 -3 -1.5 -2 -1 -1 -0.5 0 0 1 0.5 2 1 3 1.5 4 2 Given general slope of tangent lines along a graph like \\(0.5a\\) we want to determine function of a graph. In other words, we what to establish relationship of a slope’s function to that of a graph’s function. A general slope’s function like \\(0.5a\\) tells us how steep our graph will be, so given that standard square function \\(f(x) = x^2\\) has a general slope of \\(2a\\), then any other general slope of a square function will be a fraction of this (less steeper) or multiple of this (more steeper). In that regard our graph’s steepness will be a fraction of 2, that is 0.5/2 which makes it 0.25. We can now proceed to formulate our graph’s function in the form: \\[f(x) = a(x + h)^2 + k\\] As discussed, \\(a\\) tells us how steep our graph is and we have determined it to be 0.25. \\(h\\) and \\(k\\) as mentioned during our discussion on vertex are horizontal and vertical transformation of a vertex, that is distance from zero for x and y axis. Looking at our graph, we have no horizontal transformation as \\(x = 0\\), but we have a vertical transformation as \\(y = -2\\). We can therefore compile our function as: \\[f(x) = 0.25x^2 - 2\\] If we want to confirm this function has a general slope of \\(0.5a\\), we compute slope of a tangent line at \\(x\\) by evaluating \\[\\lim_{x \\to 0} \\frac{f(x+h) - f(x)}{h}\\] We begin by substituting with our function: \\[\\lim_{x \\to 0} \\frac{[0.25(x+h)^2 - 2] - [0.25x^2 - 2]}{h}\\] \\[= \\frac{[0.25x^2 + 0.25(2xh) + 0.25h^2 - 2] - [0.25x^2 - 2]}{h}\\] \\[= \\frac{h(0.5x + 0.25h)}{h} = 0.5x + 0.25h\\] Therefore tangent line at any point x along this graph would have a slope of \\(0.5x\\). Byt that we have established a relationship between general slope of tangent line along a graph and it’s graph’s function. A.13.1.4.1 Definition of a derivative function In basic terms derivative means getting limit of a point along a graph. In notation form we define derivatives for \\(y = f(x)\\) at \\(x\\) as: \\[f&#39;(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}\\] This is if a limit exists. \\(f`(x)\\) is used to denote a derivative function. \\(f`\\) is a new function with it’s domain being a subset of domain of \\(f\\). This function (\\(f&#39;\\)) is used to estimate slope of tangent lines and instantaneous rate of change. \\(h \\to 0^{-}\\) and \\(h \\to 0_{+}\\) are used to indicate left and right differentiation. As an example, let’s determine derivative of \\(f(x) = 3x^2 + 5\\) at \\(x\\). To do this we begin by getting slope of secant with point (x, f(x)) and (x+h, f(x+h)) then slope of tangency point. Initial step thus involves using difference quotient and simplifying it while second step involves getting a limit for difference quotient. \\[\\frac{f(x+h) - f(x)}{h} = \\frac{[3(x+h)^2 + 5] - [3x^2 + 5]}{h}\\] Since \\((x+h)^2 = x^2 + 2xh + h^2\\) then \\[\\frac{[3(x^2 + 2xh + h^2) + 5] - 3x^2 - 5}{h} = \\frac{3x^2+6xh+2h^2+5-3x^2-5}{h}\\] This leads us to \\[\\frac{6xh+2h^2}{h} = \\frac{h(6x+2h)}{h} = 6x + 2h\\] which becomes slope of our secant line. Now we can determine slope of tangent line with tangency point on graph. \\[f&#39;(x) = \\lim_{h \\to 0} \\frac{f(x+h)-f(x)}{h} = \\lim_{h \\to 0}(6x + 2h) = 6x\\] With that we note slope of a tangent line along graph of \\(f\\) or at any point \\((x, f(x))\\) is: \\[m = f&#39;(x) = 6x\\] Therefore, given \\(x = 2\\), we can compute slope of a tangent line at that point as: \\[m = f&#39;(2) = 6(2) = 12\\] A.13.1.4.2 Non-existance of a derivative Non-differentiability means limit does not exist at given point, like derivative might not exist at \\(x = a\\) if \\(x = a\\) does not have a limit. This can be expressed by derivative: \\[f&#39;(a) = \\lim_{h \\to 0} \\frac{f(a+h) - f(a)}{h}\\] A.13.1.5 Denoting derivatives There are two other ways of denoting derivatives of \\(f\\) at \\(x\\) other than \\(f&#39;(x)\\), these are: \\(y&#39;\\) and \\(\\frac{dy}{dx}\\) We will use these symbols at some point given situation but overall they do mean the same thing. A.13.1.6 Derivatives of Constants, Exponential Forms and Sums In this section we go over a few ways of getting derivatives of most often used functions. A.13.1.6.1 Derivative of a constant function A constant function as mentioned earlier is expressed as: \\[f(x) = C\\] Where \\(C\\) is a constant. Graph of this function is a horizontal line with slope = 0. Given slope = 0, then \\(f&#39;(x) = 0\\). We can show this using our two step process we have been using to determine derivative. \\[f(x) = \\frac{f(x+h) - f(x)}{h} = \\frac{C - C}{h} = \\frac{0}{h} = 0 \\quad{} h \\neq 0\\] \\[\\lim_{h \\to 0} 0 = 0\\] \\[\\therefore f&#39;(x) = 0\\] We can also express derivative of any \\(y = f(x) = C\\) as: \\[y&#39; = 0 \\quad{} \\text{ and } \\quad{} dy/dx = 0\\] Sometimes we can note \\(y\\) as \\(C\\) like: \\(C&#39; = 0\\) or \\(\\frac{d}{dx}C = 0\\) which all mean \\(y&#39; = frac{dy}{dx}=0\\) Here are some examples of derivatives of constants showing how different derivative notations are used. In each of the following functions, determine their derivatives Function Derivative 1. \\(f(x) = 9\\) \\(f&#39;(x) = 0\\) 2. \\(y = e\\) \\(y&#39; = 0\\) 3. \\(y = -2\\) \\(dy/dx = 0\\) 4. \\(\\frac{d}{dx}12 = 0\\) This what is referred to as derivative of a constant function rule. Derivative rules are basically functions for computing derivatives of functions. A.13.1.6.2 Exponential Rule Exponential functions are functions expressed as \\(f(x) = x^2\\) with \\(k\\) being a real number \\(\\mathbb{R}\\). Examples of Exponential rule include: \\[f(x) = x \\qquad{} h(x) = x^2 \\qquad{} m(x) = x^3\\] Root functions like \\(\\sqrt{x}\\) and \\(sqrt[3]{x}\\) are also Exponential function because nth-root is basically \\(1/n\\) for example square root of 16 is similar to 16 raised by one half. \\[\\sqrt{16} = 16^{1/2} = 4\\] Domain of a Exponential function depends on exponent as it (domain) will be a multiple of this exponent. In our derivative section we used a two step process to generalize a slope function which we used to determine derivative of two functions; \\(f(x) = x^2\\) and \\(g(x) = x^3\\), this gave us \\(2a\\) and \\(3a^2\\). If we went ahead and generalized slopes of next two exponents then: for \\(f(x) = x^4\\) derivative would be \\(f&#39;(x) = 4x^3\\) and \\(f(x) = x^5\\) derivative would be \\(f&#39;(x) = 5x^4\\) With that we have an interesting pattern as coefficient of \\(a\\) is similar to exponent of it’s function and exponent of this generalized slope is one less the exponent of it’s function. That is: If \\(f(x) = x^n\\), then \\(f&#39;(x) = nx^{n-1}\\) We can replicate this with our other derivative notation. \\(y&#39; = nx^{n-1}\\) and \\(dy/dx = nx^{n-1}\\) This is what we call a Exponential Rule. Therefore a Exponential rule is a formula for computing derivatives of Exponential functions. Here are some interesting examples on using our Exponential rule formula. We want to get derivative of \\(f(x) = x\\). \\[f&#39;(x) = 1x^{1-1} = x^0 = 1\\] We can confirm this with algebraically as: \\[\\frac{(x+h) - x}{h} = \\frac{x+h-x}{h}=\\frac{h}{h}=1\\] Let us get derivative of \\(y = x^{^-9}\\) \\[y&#39; = -9x^{-9-1} = -9x^{-8} \\quad{} \\text{or} \\quad{} -\\frac{9}{x^{8}}\\] Now let us get derivative of \\(y = x^{3/2}\\) \\[\\frac{dy}{dx} = \\frac{3}{2}x^{\\frac{3}{2} - \\frac{2}{2}} = \\frac{3}{2}x^{1/2}\\] A.13.1.6.3 Derivative of a constant times a function Given function \\(f(x) = kg(x)\\) where \\(k\\) is a constant and \\(g(x)\\) is a function differentiable at \\(x\\). Then we can find it’s limit as: \\[\\frac{f(x+h) - f(x)}{h} = \\frac{kg(x+h)-kg(x)}{h} = \\frac{k[g(x+h)-g(x)]}{h}\\] Using constant multiple rule or limit of a constant times a function which evaluates to the constant times the function, then: \\[k\\lim_{h \\to 0} \\frac{g(x+h)-g(x)}{h} = k.g&#39;(x) = kg&#39;(x)\\] We can therefore note, derivative of a constant times a differentiable function is that constant times derivative of that function. In general we can express this rule constant times a function as: \\(f&#39;(x) = kg&#39;(x)\\) or \\(y&#39; = kg&#39;\\) or \\(\\frac{dy}{dx} = k\\frac{dg}{dx}\\) That’s if \\(y = f(x) = kg(x)\\) A.13.1.6.4 Derivatives of sums and differences Given \\(f(x) = g(x) + m(x)\\) where \\(g\\) and \\(m\\) are differentiable at \\(x\\), then we can differentiate \\(f\\) at \\(x\\) as follows: \\[\\frac{f(x+h) - f(x)}{h} = \\frac{[g(x+h) + m(x+h)] - [g(x) + m(x)]}{h}\\] \\[= \\frac{g(x+h)+m(x+h)-g(x)-m(x)}{h} = \\frac{g(x+h)-g(x)}{h}+ \\frac{m(x+h)-m(x)}{h}\\] \\[\\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}= \\lim_{h \\to 0}[\\frac{g(x+h)-g(x)}{h}+\\frac{m(x+h)-m(x)}{h}]\\] \\[\\lim_{h \\to 0} \\frac{g(x+h)-g(x)}{h} + \\lim_{h \\to 0}\\frac{m(x+h) -m(x)}{h} = g&#39;(x) + m&#39;(x)\\] Basically this means derivative of sum of two differentiable functions is sum of their derivatives. This also holds for difference of two differentiable functions. We can express this sum and difference rule as: \\[f&#39;(x) = g&#39;(x) \\pm m&#39;(x)\\] if \\(y = f(x) = g(x) + m(x)\\) A.13.1.7 Derivatives of Products and Quotients Derivatives of a constant, exponents, sums and differences were rather simple and intuitive. Derivatives of products and quotients are not quite so and for that reason we discuss them separately. A.13.1.7.1 Derivatives of Products We want to begin by identifying a pattern which we can use to establish derivatives of product. Therefore, if we are given two functions, \\(F(x) = x\\) and \\(G(x) = x^2\\) and told a third function \\(f(x)\\) is a product of these two functions, that is \\(f(x) = F(x)G(x) = x*x^2\\), we want to establish derivative of \\(f(x)\\). Let us work it out using our difference quotient and then get it’s limit. \\[\\frac{f(x+h)-f(x)}{h}=\\frac{[(x+h)*(x+h)^2] - [x*x^2]}{h} = \\frac{(x+h)*(x^2+2xh+h^2)-x^3}{h}\\] \\[= \\frac{x^3+2x^2h+xh^2+x^2h+x^2h+2xh^2+h^3-x^3}{h} = \\frac{h(2x^2+xh+x^2+2xh+h^2)}{h}\\] \\[2x^2 + xh + x^2 + 2xh + h = 2x^2 + x^2 = 3x^2\\] From this, derivative of \\(f(x) = F(x)G(x) = x*x^2\\) is equal to \\(3x^2\\). This derivative is actually a product of our initial function and derivative of the second function plus derivative of our initial function times second function. We can express this product rule as follows: If \\(y = f(x) = F(x)G(x)\\) where \\(F&#39;(x)\\) and \\(G&#39;(x)\\) exist, then \\(f&#39;(x) = F(x)G&#39;(x) + F&#39;(x)G(x)\\) A.13.1.7.2 Derivatives of quotients Just like product rule, derivatives of quotients have a method formulated by getting derivative of it’s quotients. This formula simply states that derivative of a quotient of two functions is similar to denominator times derivative of its numerator minus numerator times derivative of denominator, all over denominator squared. This quotient rule can be expressed as: \\[f(x) = \\frac{F(x)}{G(x)} \\qquad{} f&#39;(x) = \\frac{G(x)F&#39;(x) - F(x)G&#39;(x)}{[G(x)]^2}\\] if \\(y = f(x) = \\frac{F(x)}{G(x)}\\) A.13.1.8 Chain Rule: Exponential Form In it’s basic form, chain rule enables differentiation of functions expressed as \\(f[g(x)]\\) if \\(f(x)\\) and \\(g(x)\\) are differentiable. A.13.1.8.0.1 General exponent rule We now know how to differentiate an exponential function using: \\[f&#39;(x) = nx^{n-1}\\] For function \\(y = f(x) = x^n\\) with \\(n\\) being a \\(\\mathbb{R}\\) number. However if we wanted to get derivative of a Exponential function of a exponential function, something like \\(f(x) = [g(x)]^3\\) where \\(g(x) = x^2\\), we need to reformulate another formula. Like before, to generalize a formula for \\(f&#39;(x) = [g(x)]^n\\) where \\(g(x)\\) is any differentiable exponential function, we need to establish a pattern. Let us consider \\(F(x) = [f(x)]^2\\), which is similar to \\(F(x) = f(x)f(x)\\). Using product rule we can differentiate \\(F\\) as: \\[F&#39;(x) = f(x)f&#39;(x) + f(x)f&#39;(x)\\] \\[\\therefore F&#39;(x) = 2f(x)f&#39;(x)\\] Now let’s consider \\(y = [f(x)]^3\\) which is the same as \\(y = [f(x)]^2f(x)\\). Using product rule we can thus differentiate \\(F\\) as: \\[\\frac{d}{dx}[f(x)]^3 = [f(x)]^2\\frac{d}{dx}f(x) + f(x)\\frac{d}{dx}[f(x)]^2\\] substituting \\(\\frac{d}{dx}[f(x)]^2\\) with what we got earlier that is \\(2f(x)f&#39;(x)\\) we get \\[\\frac{d}{dx}[f(x)]^3 = [f(x)]^2f&#39;(x) + f(x)[2f(x)f(x)]\\] \\[\\therefore \\frac{d}{dx}[f(x)]^3 = 3[f(x)]^2f&#39;(x)\\] With that we have a pattern which can hold true for any exponent, that is: \\[\\frac{d}{dx}[f(x)]^n = n[f(x)]^{n-1}f&#39;(x)\\] Where \\(n\\) is any positive integer This what we call general exponent rule part of a differential rule called chain rule. This general rule can also be efficiently written as: \\[y&#39; = fx^{n-1}\\] or \\[\\frac{d}{dx}f^n = nf^{n-1}\\frac{dx}{dx}\\] Where \\(f = f(x)\\) Example Given: \\[f(x) = \\frac{1}{(3x + 2)^2}\\] let us get it’s derivative \\(f&#39;(x)\\) To begin with, we know \\(f\\) can be written as: \\[f(x) = (3x + 2)^{-2}\\] then \\[f&#39;(x) = -2(3x + 2)^{-3}*(3x + 2)&#39;\\] \\[(3x + 2)^{&#39;} = 3\\] \\[\\therefore f&#39;(x) = -2(3x + 2)^{-3} * 3 = -6(3x + 2)^{-3}\\] A.13.2 Graphing and Optimization In this section we want to get a bit more acquainted with shape of polynomials as core functions in statistics. We are also going to determine minimum and maximum points of these graphs. To do this we will rely on concept of derivative to determine slope of a graph at a particular point. As we go through this section, it would be good to recall that graph of a polynomial function with a positive degree \\(n\\) can have at most \\((n-1)\\) turning points and can cross x-axis at most \\(n\\) times. With this in mind, core issues we will discuss on in this section are: Continuity First derivative Second derivative Optimization A.13.2.1 Continuity A continuous graph is one without holes or breaks; it can be drawn from start to end without breakage. This concept of continuity is of importance when plotting and analyzing graphs. Below are two graphs, \\(f\\) and \\(g\\); function \\(f\\) is continuous over an interval while function \\(g\\) is discontinuous after a certain point (2). op &lt;- par(&quot;mfrow&quot;) par(mfrow = c(1, 2)) x &lt;- -5:5 plot(x, type = &quot;l&quot;, xlab = &quot;x&quot;, ylab = &quot;y&quot;) title(&quot;f(x) = x&quot;) plot(x, x^2/(2*x), type = &quot;l&quot;, col = 4, xlab = &quot;x&quot;, ylab = &quot;y&quot;) points(0, 0, pch = 21, col = 4) segments(-1, -0.5, -0.1, -0.1, col = 4) segments(0.1, 0.1, 1, 0.5, col = 4) title(expression(paste(&quot;g(x) =&quot; , x^2/(2*x)))) par(mfrow = op) We can thus define continuity of a function at point \\(x = a\\) with these conditions: \\(\\lim_{x \\to a} f(x)\\) exists \\(f(a)\\) exists \\(\\lim_{x \\to a} f(x) = f(c)\\) If one or more of this conditions is violated, then function is said to be discontinuous at \\(x = a\\). We can also note a function is continuous on an open interval \\((a, b)\\) if it is continuous at each point on that interval. Example Let us consider our two functions \\(f(x) = x\\) and \\(g(x) = x^2/2x\\) and see how they adhere to these conditions. For function \\(f\\) We see that this function (\\(f(x) = x\\)) is continuous on open interval (\\(-\\infty\\), \\(\\infty\\)), therefore given point \\(x = 0\\): \\(\\lim_{x \\to 0} f(x)\\) exists, it is 0. \\(f(0)\\) also exists as \\(f(0) = 0\\) \\(\\lim_{x \\to 0} f(0) = f(0) = 0\\) All three conditions are met. For function \\(g\\) We note a discontinuity marked by a hollow point on our graph, so for this point \\(x = 0\\): \\(\\lim_{x \\to 0} f(x)\\) exists as one-sided limits are similar, they are approaching 4. \\(f(0)\\) does not exist as 0 \\(\\div\\) 0 is not defined \\(\\lim_{x \\to 0} f(0) \\ne f(0)\\) that is \\(\\lim_{x \\to 0}f(0) = NAN\\) Here only the first condition is satisfied while the other two do not conform. One-sided continuity Like limits, we can also talk of one-sided continuity like continuity on the right hand means at \\(x = a\\), \\(\\lim_{x \\to a^+}f(x) = f(a)\\) and continuity on the left hand \\(\\lim_{x \\to a^{-}}f(x) = f(c)\\) Open, closed and half-closed intervals A graph shaped like a semi-circle can be said to be continuous on a closed interval \\([a, b]\\) where \\(a\\) is a point where it begins and \\(b\\) point it ends. Continuity at closed intervals are continuous over closed interval like \\([a, b]\\) and is continuous on right side from \\(a\\) and continuous on left side at \\(b\\). A function can have a half-closed interval like \\([0, \\infty)\\). A.13.2.1.1 Continuity properties There are some handy properties formulated to determine intervals of continuity for some important classes of functions without looking at their graph as well as having to use conditions of continuity. A general property states that, for any two continuous functions on similar interval, their sum, difference product and quotient are continuous on the same interval except where \\(x = 0\\) making denominator to be 0. Other properties of specific functions are: Constant functions like \\(f(x) = x\\) are continuous for all \\(x\\) For all positive exponential functions of the form \\(f(x) = x^n\\) are continuous for all it’s base \\(x\\). Polynomial functions are continuous for all \\(x\\) like \\(f(x) = x^4 + x^2 + 5\\) Rational functions are continuous except for those values which make denominator to be 0 like \\(x = 2\\) in \\(f(x) = 2x^2/(x - 2)\\) Any positive odd root greater than 1 are continuous wherever it’s radicand is continuous like \\(x^3\\) in \\(f(x) = \\sqrt[5]{x^3}\\) Any positive even root is continuous wherever it’s radicand is continuous and non-negative, that is \\(x\\) in \\(f(x) = x^2\\) is in \\([0, \\infty)\\) A.13.2.1.2 Infinite Limits From our discussion above, we have taken note of one situation where a limit might not exist, this is when values approaching a certain value from left and right are different. Let us consider another situation where a limit does not exist. For functions whose values become extremely large as \\(x\\) approaches \\(a\\), a limit may not exist. This is because these extreme values from left hand side approaching \\(a\\) would be different from those approaching \\(a\\) from right hand side. Therefore their one sided limits would be different. Symbol \\(\\infty\\) and \\(-\\infty\\) are used to describe \\(x\\) near \\(a\\) if extreme values are positive or negative. Example Given function \\[h(x) = \\frac{x}{x-1}\\] for \\(x = 1\\), \\(h(1)\\) is an extreme value heading to infinity hence output is often shown as inf meaning infinity. Exploring values approaching 1 from left we see they are negative and increasing to an extreme value while those approaching from right towards 1 are positive and also increasing to an extreme value. ranges &lt;- c(0.0001, 0.001, 0.01) x &lt;- c(1 - rev(ranges), 1, 1 + ranges) y &lt;- x/(x-1) matrix(c(x, y), nrow = 2, byrow = TRUE, dimnames = list(c(&quot;x&quot;, &quot;f(x)&quot;), rep(&quot;&quot;, length(x)))) ## ## x 0.99 0.999 0.9999 1 1.0001 1.001 1.01 ## f(x) -99.00 -999.000 -9999.0000 Inf 10001.0000 1001.000 101.00 In both sides we actually do not have a limit given that we are approaching different values, we can therefore say \\(\\lim_{x \\to 1}f(x)\\) does not exist. We can also express these one-side limits as: \\[\\lim_{x \\to 1^-} f(x) = -\\infty \\qquad{} \\text{and} \\qquad{} \\lim_{x \\to 1^+} f(x) = \\infty\\] These two statements describe graph of \\(f\\) at \\(x\\) approaching 1 and as we can see below, the line tends to approach 1 but does not get there thus we see a vertical curve tending to approach 1 and heading towards infinity (\\(\\infty\\)). This vertical line to which curve is approaching is referred to as vertical asymptote, in this case it is \\(x = 1\\). x1 &lt;- seq(0.9, 1.1, length.out = 100) y1 &lt;- x1/(x1-1) plot(x1, x1/(x1 - 1), type = &quot;n&quot;, xlab = &quot;x&quot;, ylab = &quot;f(x)&quot;) title(&quot;f(x) = x/(x-1)&quot;) lines(x1[x1 &lt; 1], y1[which(x1 &lt; 1)], col = 4) lines(x1[x1 &gt; 1], y1[which(x1 &gt; 1)], col = 4) abline(v = 1) Let’s also consider a situation where both sides of a function’s values approaching \\(a\\) are positives. For this, let’s consider function \\[g(x) = \\frac{x}{x^2 - 1}\\] at \\(x\\) approaching 1 y &lt;- function(x) x/(x -1)^4 matrix(c(x, y(x)), nrow = 2, byrow = TRUE, dimnames = list(c(&quot;x&quot;, &quot;f(x)&quot;), 1:length(x))) ## 1 2 3 4 5 6 7 ## x 9.9e-01 9.99e-01 9.999e-01 1 1.0001e+00 1.001e+00 1.01e+00 ## f(x) 9.9e+07 9.99e+11 9.999e+15 Inf 1.0001e+16 1.001e+12 1.01e+08 Graph of \\(g\\) should look like this: x2 &lt;- seq(-0.5, 2.5, 0.001) plot(c(-0.6, 2.6), c(0, 20), type = &quot;n&quot;, xlab = &quot;x&quot;, ylab = &quot;g(x)&quot;) title(expression(paste(&quot;g(x) =&quot;, x/(x - 1)^4))) lines(x2, y(x2), col = 4) Notice both graphs are approaching \\(x = 1\\) but not really reaching it. This makes \\(x = 1\\) it’s vertical asymptote. Vertical Asymptote Generally, we can define vertical asymptote as a line \\(x = a\\) for any graph if limit of function \\(f\\) does not exist as \\(x\\) approaches \\(a\\) from left and right because values of \\(f(x)\\) are increasing either negatively or positively to infinity. For large positive or negative values approaching \\(a\\) from left, we express it as: \\[\\lim_{x \\to a^-} f(x) = \\infty \\qquad{} (or -\\infty) \\] For large positive or negative values approaching \\(a\\) from right we express it as: \\[\\lim_{x \\to a^+} f(x) = \\infty \\qquad{} (or - \\infty)\\] If both sides are either positive or negative, then we can express them as: \\[\\lim_{x \\to a} f(x) = \\infty \\qquad{} (or -\\infty)\\] A.13.2.1.3 Limits and Vertical asymptote at points of discontinuity A function can have more than one discontinuity and for each of these discontinuity there could or could not be a vertical asymptote. Here we want to see situations where a discontinuity has a vertical asymptote and where it does not have one. Example Let us consider this function: \\[f(x) = \\frac{x - 3}{x^2 - 4x + 3}\\] From our discussion on properties of continuity, and particularly our fourth property, we know discontinuity for a rational function such as \\(f\\), discontinues when denominator becomes 0. For this function \\(x = 1\\) and \\(x = 3\\) will make it’s denominator to be 0. Now let us look at values approaching 1 and 3 to determine shape of graph at these discontinuities. We begin with \\(x = 1\\) x &lt;- c(1 - rev(ranges), 1, 1 - ranges) y &lt;- function(x) (x-3)/(x^2 - 4*x + 3) matrix(c(x, y(x)), nrow = 2, byrow = TRUE, dimnames = list(c(&quot;x&quot;, &quot;f(x)&quot;), rep(&quot;&quot;, length(x)))) ## ## x 0.99 0.999 9.999e-01 1 9.999e-01 0.999 0.99 ## f(x) -100.00 -1000.000 -1.000e+04 -Inf -1.000e+04 -1000.000 -100.00 We can see values of \\(f(x)\\) approaching 1 are large negative values which we can express as: \\[\\lim_{x \\to _1} \\frac{x - 3}{x^2 - 4x + 3} = -\\infty\\] This means \\(x = 1\\) is a vertical asymptote for graph of \\(y = f(x)\\) For \\(x = 3\\) x &lt;- c(3 - rev(ranges), 3, 3 + ranges) matrix(c(x, y(x)), nrow = 2, byrow = TRUE, dimnames = list(c(&quot;x&quot;, &quot;f(x)&quot;), rep(&quot;&quot;, length(x)))) ## ## x 2.9900000 2.9990000 2.999900 3 3.000100 3.0010000 3.0100000 ## f(x) 0.5025126 0.5002501 0.500025 NaN 0.499975 0.4997501 0.4975124 There is no value at \\(x = 3\\) but left and right side of three are approaching 0.5, hence \\(\\lim_{x \\to 3}\\) exists and therefore there is no vertical asymptote at \\(x = 3\\). We can represent this graphically as shown below: x &lt;- seq(-5, 5, 0.001) plot(c(-6, 6), c(-5, 5), type = &quot;n&quot;, xlab = &quot;x&quot;, ylab = &quot;y&quot;) abline(v = 1, lty = &quot;dashed&quot;) lines(x, y(x), col = 4) title(expression(paste(&quot;f(x) = &quot;, (x-3)/(x^2 - 4*x + 3)))) A.13.2.1.4 Solving inequalities using continuity properties Inequalities are functions whose left and right expressions are not equal to each other, they are either \\(&lt;\\), \\(&gt;\\), \\(\\geqslant\\), \\(\\leqslant\\) or \\(\\neq\\): \\(&lt;\\) and \\(&gt;\\) are specifically called strict inequalities. For such functions, we can solve for \\(x\\) in \\(f(x)\\) that satisfies given inequality. For example, given: \\[\\frac{x^2 - 1}{x - 3} &gt; 0\\] we can determine which \\(x\\) values equates left hand expression to positive values. To do this we use a special line graph called a sign chart. Sign charts are basically charts which partition \\(x\\) values into intervals indicating expected sign of \\(f(x)\\) values. To construct these sign charts we will rely on two concepts; one is our continuity properties and two is a sign properties. We want to use continuity properties to determine points of continuity in a function which will form intervals and then determine sign of \\(f(x)\\) in those intervals. Sign of these intervals is determined by the sign properties on a interval (a, b). This properties states that, “if f is continuous on \\((a, b)\\), and \\(f(x) \\ne 0\\) for all \\(x\\) in \\((a, b)\\), then either \\(f(x) &gt; 0\\) (positive) for all x in \\((a, b)\\) or \\(f(x) &lt; 0\\) (negative) for all \\(x\\) in \\((a, b)\\)”. Let’s look at an example to understand this sign properties. Example Given an interval \\((0, -10)\\) for a continuous function \\(h\\) and \\(h(x) \\ne 0\\) for any \\(x\\) value in that interval, if \\(f(-5) = -5\\), we want to determine if it is possible to get a positive \\(h(x)\\) for any \\(x\\) value in given interval. Let us try and connect (-5, -5) with a positive value like (5, 5). plot(c(-6, -3), c(-6, 6), type = &quot;n&quot;, xlab = &quot;x&quot;, ylab = &quot;f(x)&quot;) abline(h = 0, lty = 2) text(-3.3, 0.35, labels = &quot;h(x) = 0&quot;) lines(c(-5, -5), c(-5, 5), col = 4) points(c(-5, -5), c(-5, 5), pch = 21, bg = 4) We can see to connect these two points with a continuous line we had to pass through \\(h(x) = 0\\) (crossing x-axis) thus violating given condition of \\(h(x) \\ne 0\\). We therefore have to have a negative \\(f(x)\\) for this function to be continuous and still meet set condition. This reasoning still holds if we changed our sign, this is what sign properties is about. With that, we can note, given one \\(h(x)\\) we can determine sign of all other values in that interval. We can now use this sign properties to solve our earlier inequality. We start by making our left side a function \\(h\\). \\[h(x) = \\frac{x^2 - 1}{x - 1}\\] From this function, we want to identify intervals of continuity by noting points of discontinuity. This function has one discontinuous point, this is a point that equate this rational function’s denominator to 0. Also recall we are not including points where \\(h(x) = 0\\), therefore we need to take note of this point. For this function \\(h(x) = 0\\) when \\(x = -1\\) and when \\(x = 1\\). Altogether we have three values for which we will use to determine continuous intervals of \\(h\\). We refer to these values (x = 3, 1, and -1) as partition numbers which we can describe as values which determine open intervals where \\(h(x)\\) does not change sign. We can visualize these partitions on a real number line as shown below: plot(1:10, type = &quot;n&quot;, axes = FALSE, ann = FALSE) abline(h = 5) text(2:9, rep(5, length(2:9)), labels = &quot;|&quot;) points(c(4, 6, 8), rep(5, 3), pch = 21, bg = c(&quot;blue&quot;, &quot;blue&quot;, &quot;white&quot;), cex = 1.2) text(2:9, rep(4.4, length(2:9)), labels = -3:4) text(9.8, 5.35, labels = &quot;x-axis&quot;) These partition numbers give us four open intervals, these are \\((-\\infty, -1)\\), (\\(-1, 1\\)), (\\(1, 3\\)), and (\\(3, \\infty\\)). At these intervals we know \\(h\\) is continuous and therefore sign is similar for all \\(h(x)\\). What we now need is to find at least one \\(f(x)\\) in these intervals and determine their sign. For our example we can pick x = -2, 0, 2, and 4, which become what we refer to as evaluation numbers. \\(h(x)\\) at these point are -3/5, 1/3, -3, and 15, therefore sign for these partitions are -, +, - and +. Now we can construct a sign chart which is basically a real number line showing each partition, their sign and their evaluation numbers. plot(1:10, type = &quot;n&quot;, axes = FALSE, ann = FALSE) abline(h = 5) text(2:9, rep(5, length(2:9)), labels = &quot;|&quot;) points(c(4, 6, 8), rep(5, 3), pch = 21, bg = c(&quot;blue&quot;, &quot;blue&quot;, &quot;white&quot;), cex = 1.2) text(2:9, rep(4.4, length(2:9)), labels = -3:4) text(c(2.5, 3, 5, 7, 9), rep(6, 5), labels = c(&quot;f(x)&quot;, rep(c(&quot;- - -&quot;, &quot;+ + +&quot;), 2))) text(c(3, 5, 7, 9), rep(7, 4), labels = c(expression(paste(&quot;(-&quot;, infinity, &quot;, -1)&quot;)), &quot;(-1, 1)&quot;, &quot;(1, 3)&quot;, expression(paste(&quot;(3, &quot;, infinity, &quot;)&quot;)))) segments(x0 = c(3, 5, 7, 9), y0 = rep(4, 4), x1 = c(3, 5, 7, 9), y1 = rep(3, 4)) segments(3, 3, 9, 3) text(6, 2.5, labels = &quot;Evaluation numbers&quot;) text(10, 4.7, labels = &quot;x-axis&quot;) segments(x0 = c(4, 6, 8)+.1, y0 = rep(5, 3), x1 = c(4, 6, 8), y1 = rep(8, 3), lty = &quot;dashed&quot;) We can now solve our inequality: \\[\\frac{x^2 - 1}{x - 3} &gt; 0\\] as \\(-1 &lt; x &lt; 1\\) or \\(x &gt; 3\\) A.13.2.2 First derivative In this section we want to use derivatives which are basically slopes of a graph at a particular point, to inform us about shape of a given function in addition to locating minimum and maximum points. We shall go over four sub-topics, these are: Increasing and decreasing functions Local Extrema First-Derivative Evaluation Analyzing Graphs A.13.2.2.1 Increasing and Decreasing functions One important aspect of graphing a function is to determine points of increase and points of decrease. We can comfortably do this using a sign chart of it’s derivative function, that is, make a sign chart of slopes of tangent lines along a graph. By doing so we are able to tell where graph is rising and where it is falling based on their sign. We can also be able to tell it’s vertices (minimum and maximum point). Example Let us look at this function \\[h(x) = -x^5 - x^4 + 14x^3 + 6x^2 - 45x -3\\] it’s derivative is: \\[h&#39;(x) = -5x^4 - 4x^3 +42x^2 + 12x - 45\\] This should look familiar as we came across it when discussing factoring polynomials with rational zero’s theorem. If you recall, we were able to locate all points where \\(h(x) = 0\\) these were \\(x = -3.00,\\text{ }-1.23,\\text{ }1.00\\text{ and } 2.43\\). Since in this case these are points on our derivative function, then they represent points where slope is equal to zero. From our previous discussion we know slope = 0 is a point on a graph where it is neither increasing nor decreasing. We referred to these points as vertices of a graph. Do recal polynomials are continous graphs so we will not have any discontinous points. Here is a graph of our function \\(h\\) and a sign chart of it derivative \\(h&#39;\\). op &lt;- par(&quot;mar&quot;) par(mar = c(6.5, 2.1, 0.5, 1.1)) x &lt;- sort(unique(c(seq(-4, 4, by = 0.01), hx_at_zero))) hx &lt;- expression((-x)^5 - x^4 + 14*x^3 + 6*x^2 - 45*x - 3) plot(rep(c(-5,5), 2), c(rep(40, 2), rep(-40, 2)), type = &quot;n&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, xaxt = &quot;n&quot;) abline(h = 0, v = 0, lty = &quot;dashed&quot;) legend(&quot;topright&quot;, legend = hx, bty = &quot;n&quot;, box.col = &quot;white&quot;) lines(spline(x, eval(hx)), col = 4) x &lt;- hx_at_zero points(hx_at_zero, eval(hx), pch = 21, bg = 4) text(c(-4, -2, 0, 2, 4), rep(-46, 4), labels = c(-4, -2, 0, 2, 4), xpd = TRUE, cex = 0.9) # Sign chart rect(-4.3, -75, -3, -53, col = &quot;lightblue&quot;, xpd = TRUE, border = NA) rect(-3, -75, -1.23303, -53, col = &quot;chocolate2&quot;, xpd = TRUE, border = NA) rect(-1.23303, -75, 1, -53, col = &quot;lightblue&quot;, xpd = TRUE, border = NA) rect(1, -75, 2.43303, -53, col = &quot;chocolate2&quot;, xpd = TRUE, border = NA) rect(2.43303, -75, 4.1, -53, col = &quot;lightblue&quot;, xpd = TRUE, border = NA) segments(x0 = -4, y0 = -65, x1 = 4, y1 = -65, xpd = TRUE) text(x = hx_at_zero, y = rep(-59, 4), labels = rep(0, length(hx_at_zero)), xpd = TRUE, cex = 0.7) text(x = hx_at_zero, y = rep(-70, 4), labels = round(hx_at_zero, 1), xpd = TRUE) text(-4.7, -70, labels = &quot;h(x)&quot;, font = 2, xpd = TRUE, cex = 0.7) points(hx_at_zero, y = rep(-65, 4), pch = 19, xpd = TRUE) t &lt;- c(-4, -2, 0, 2, 3) t_num &lt;- -5*t^4 - 4*t^3 + 42*t^2 + 12*t - 45 signs &lt;- ifelse(t_num &lt; 0, &quot;----&quot;, &quot;++++&quot;) text(x = c(-3.7, -2, 0, 1.7, 3.4), rep(-59, length(signs)), labels = signs, xpd = TRUE) text(-4.7, -59, labels = &quot;h&#39;(x)&quot;, cex = 0.7, font = 2, xpd = TRUE) intervals &lt;- c(expression(paste(&quot;(-&quot;, infinity, &quot;, -3)&quot;)),&quot;(-3, -1.23)&quot;,&quot;(-1.23, 1)&quot;,&quot;(1, 2.43)&quot;,expression(paste(&quot;(2.43, &quot;, infinity, &quot;)&quot;))) text(x = c(-3.65, -2, 0, 1.7, 3.4), rep(-55, length(signs)), labels = intervals, cex = 0.8, xpd = TRUE) par(mar = op) It’s clear to see when a graph is decreasing, it’s derivative has negative values and when it is increasing it’s derivative has positive values. For points where derivative is 0, graph reaches it’s peak (minimum or maximum). Graph starts from \\(-\\infty\\) and ends at positive \\(\\infty\\), this is because \\(x\\) can assume any number on a real number line and greater \\(x\\) values on both sides increase or decrease very fast towards infinity. A.13.2.2.2 Partition Numbers and Critical values Domain values of a function \\(f\\) are said to be critical values if they are partition numbers of it’s derivative \\(f&#39;(x)\\). Partition numbers of \\(f&#39;\\) are \\(x\\) values where \\(f&#39;(x) = 0\\) and where \\(f&#39;(x)\\) does not exist (points of discontinuous). However, it should be noted that not all partition numbers of \\(f&#39;\\) are critical values of \\(f\\) and therefore they would not be in domain of \\(f\\). On that note, critical values of a function \\(f\\) are always part of \\(f&#39;s\\) domain. Knowing this two terms and their distinction can guide us in knowing shape of a graph especially at certain points, for example a graph can be generally increasing but have one particular point where it has a slope of 0, or a point where rate of increase or decrease is different. This can easily be shown by a sign chart where all evaluation numbers have similar sign but have partitions of either 0 or not-defined. We can best grasp this concept with examples. f’(x) = 0 as a critical values of f A cube function is a good example of a function whose derivative has one partition number that is also a critical value. For example, given: \\[f(x) = -x^3\\] we establish it’s as derivative \\[f&#39;(x) = -3x^2\\] Since this derivative is a square function then we know it is continuous for all values of \\(x\\), which means we do not expect a discontinuity point as one of its partition numbers. We also know there is only one value which can equate this function to zero and that is when \\(x = 0\\). Therefore we can note we have one partition value for \\(f&#39;\\) and since 0 is also a domain of \\(f\\) (\\(f(0) = 0\\), there is no discontinuity) then we know it is also a critical value of \\(f\\). Let’s make a sign chart which will tell us more about shape of \\(f\\) even before plotting it. plot(0:4, 0:4, type = &quot;n&quot;, axes = FALSE, ann = FALSE) rect(0, 0.8, 2, 3, col = &quot;lightblue&quot;, border = NA) rect(2, 0.8, 4.2, 3, col = &quot;lightblue&quot;, border = NA) text(c(1, 3), rep(2.8, 2), labels = c(expression(paste(&quot;(-&quot;, infinity, &quot;, 0)&quot;)), expression(paste(&quot;(0, &quot;, infinity, &quot;)&quot;)))) text(c(0.6, 1:3), rep(2.5, 4), labels = c(&quot;f&#39;(x)&quot;, &quot;----&quot;, 0, &quot;----&quot;), font = c(1, 2, 1, 2)) segments(0, 2, 4, 2) text(4, 2.2, &quot;x&quot;, font = 2) text(c(1, 2, 3), rep(2, 3), labels = &quot;|&quot;) text(c(0.6, 1:3), rep(1.6, 4), labels = c(&quot;f(x)&quot;, -1:1)) text(c(1, 3), rep(1,2), labels = c(&quot;Decreasing&quot;, &quot;Decreasing&quot;)) From our sign chart we can see our graph is decreasing then “flattens out” at point \\(f(x) = 0\\) and then resumes it’s descent. Here is a plot of \\(f\\). x &lt;- seq(-4, 4, 0.0001) y &lt;- function(x) (-x)^3 plot(c(-5.5, 5.5), c(-64, 64), type = &quot;n&quot;, xlab = &quot;x&quot;, ylab = &quot;f(x)&quot;) abline(h = 0, v = 0, lty = &quot;dashed&quot;) lines(x, y(x), col = 4) In this example we have seen partition value of \\(f&#39;\\) is also a critical value of \\(f\\). We have also seen at \\(f&#39;(x) = 0\\) \\(f\\) is neither increasing nor decreasing hence we have a horizontal tangent line at \\(x = 0\\). Non-existant f’(x) as a critical value of f A function’s derivative can be discontinuous at a certain point but that same point would be continuous on it’s original function. A good example of this are cube root functions. For example if we have the following cube root function \\[h(x) = \\sqrt[3]{x+2} + 3\\] which we can express as an exponential function \\[h(x) = (x + 2)^{\\frac{1}{3}} + 3\\] Then we can write it’s derivative as \\[h&#39;(x) = \\frac{1}{3}(x + 2)^{-\\frac{2}{3}}\\] or simply \\[h&#39;(x) = \\frac{1}{3(x+2)^{\\frac{2}{3}}}\\] Since this is a quotient, we expect discontinuity when denominator evaluates to zero, this should be when \\(x = -2\\). We also know there is no point where \\(f&#39;(x) = 0\\) because of it’s numerator 1, that is, no value divided by 1 can become zero. Therefore derivative \\(h&#39;\\) has one partition number which is -2. -2 is not in domain of \\(h&#39;\\) (\\(h&#39;(-2) = \\infty\\)) but it is in the domain of \\(h\\), that is \\(h(-2) = \\frac{1}{3(-2+2)^{\\frac{2}{3}}} = 3\\) which means -2 is a critical value of \\(h\\). Let us look at sign chart of \\(h&#39;\\) to predict shape of \\(h\\)’s graph. plot(0:4, 0:4, type = &quot;n&quot;, axes = FALSE, ann = FALSE) rect(0, 0.8, 2, 3, col = &quot;chocolate2&quot;, border = NA) rect(2, 0.8, 4.2, 3, col = &quot;chocolate2&quot;, border = NA) text(c(1, 3), rep(2.8, 2), labels = c(expression(paste(&quot;(-&quot;, infinity, &quot;, -2)&quot;)), expression(paste(&quot;(-2, &quot;, infinity, &quot;)&quot;)))) t_num &lt;- c(-3, -1) signs &lt;- ifelse(1/(3*(abs(t_num + 2)^(1/3))^2) &lt; 0, &quot;----&quot;, &quot;++++&quot;) text(c(0.6, 1:3), rep(2.5, 4), labels = c(&quot;h&#39;(x)&quot;, signs[1], &quot;ND&quot;, signs[2]), font = c(1, 2, 1, 2)) segments(0, 2, 4, 2) text(4, 2.2, &quot;x&quot;, font = 2) text(c(1, 2, 3), rep(2, 3), labels = &quot;|&quot;) text(c(0.6, 1:3), rep(1.6, 4), labels = c(&quot;h(x)&quot;, -3:-1)) text(c(1, 3), rep(1,2), labels = c(&quot;Increasing&quot;, &quot;Increasing&quot;)) From this sign chart we can see derivative of \\(h\\) is positive then at \\(h&#39;(-2)\\) there is a discontinuity (not defined), this is followed by positive signs. Therefore on graph of \\(h\\) we expect it to be increasing until it reaches \\(x = -2\\) where it stops increasing before it resumes it’s rise. x &lt;- seq(-5, 5, 0.001) y &lt;- c(-(abs(x[x &lt;= -2] + 2))^(1/3) + 3, (x[x &gt; -2] + 2)^(1/3) + 3) plot(x, y, type = &quot;n&quot;, ylab = &quot;h(x)&quot;) abline(v = 0, h = 0, lty = &quot;dashed&quot;) lines(x, y, col = 4) segments(-2, 2.3, -2, 3.7) points(-2, 3, pch = 21, bg = 4) text(x = -3.8, 4.65, labels = expression(paste(&quot;h(x) =&quot;, sqrt((x+2), 3) + 3))) In this example we have noted that a continuous function can be increasing or decreasing even for points where it’s derivative is non existent. That is, a given point \\(x\\) of a derivative can be non-existent but same point would be in domain of it’s derivative function. Function without critical values A discontinuous point on a derivative is considered a partition number. In our preceding example we saw how this point can be a critical value, here we look at an example where it is not a critical value (not a domain of it’s function). A good example is a quotient of first degree polynomial with a range that does not include 0 and is discontinuous at a point where denominator equals zero. Therefore given: \\[j(x) = \\frac{6 + x}{x + 3}\\] we can get it’s derivative as \\[j&#39;(x) = \\frac{-3}{x^2+6x+9}\\] For \\(j&#39;\\), there is a discontinuity at \\(x = -3\\) which means -3 is our only partition number. -3 is not in domain of \\(f\\) since \\(j(-3)\\) is a very large number approaching infinity. Therefore, we expect \\(f\\) not to be a continuous graph with discontinuity happening at -3. We can anticipate shape of \\(f\\)’s graph from this sign chart. plot(0:4, 0:4, type = &quot;n&quot;, axes = FALSE, ann = FALSE) j_prime &lt;- expression((-3)/(t_num^2 + 6*t_num + 9)) rect(0, 0.8, 2, 3, col = ifelse(eval(j_prime) &gt;= 0, &quot;chocolate2&quot;, &quot;lightblue&quot;), border = NA) rect(2, 0.8, 4.2, 3, col = ifelse(eval(j_prime) &lt; 0, &quot;lightblue&quot;, &quot;chocolate2&quot;), border = NA) text(c(1, 3), rep(2.8, 2), labels = c(expression(paste(&quot;(-&quot;, infinity, &quot;, -3)&quot;)), expression(paste(&quot;(-3, &quot;, infinity, &quot;)&quot;)))) t_num &lt;- c(-4, -2) signs &lt;- ifelse(eval(j_prime) &lt; 0, &quot;----&quot;, &quot;++++&quot;) text(c(0.6, 1:3), rep(2.5, 4), labels = c(&quot;j&#39;(x)&quot;, signs[1], &quot;ND&quot;, signs[2]), font = c(1, 2, 1, 2)) segments(0, 2, 4, 2) text(4, 2.2, &quot;x&quot;, font = 2) text(c(1, 3), rep(2, 1), labels = &quot;|&quot;) points(2, 2, pch = 21, bg = &quot;white&quot;) text(c(0.6, 1:3), rep(1.6, 4), labels = c(&quot;j(x)&quot;, -4:-2)) text(c(1, 3), rep(1,2), labels = c(&quot;Decreasing&quot;, &quot;Decreasing&quot;)) From our sign chart we see derivative of \\(j\\) is negative for interval \\((-\\infty, -3)\\) as well as \\((-3, \\infty)\\). Therefore we expect graph of \\(j\\) to generally be decreasing. Looking at this chart and computing values of \\(f(-4)\\) and \\(f(-2)\\), it is not unimaginable to include a conclusion of some increase given that \\(f(-4) = -3\\) and \\(f(-2) = 0.6\\), but this would be incorrect as a sign chart’s is meant to show general pattern of intervals of a graph rather than difference in points between intervals. We can now look at graph of \\(f\\) to see what our sign chart revealed. x &lt;- seq(-4, -1, 0.01) y = (6 + x)/(3 + x) plot(x, y, type = &quot;n&quot;, ylab = &quot;j(x)&quot;) abline(v = -3, h = 0, lty = &quot;dashed&quot;) lines(x, y, col = 4) text(-1.4, 299, labels = expression(paste(&quot;j(x) =&quot;, (6+x)/(x+3)))) Notice how graph discontinues just before -3 as it heads downward towards negative infinity and picking up somewhere above and begins a decline heading towards positive infinity. In this example we see a discontinuous point on a derivative function which is also discontinuous on it’s function thereby not considered a critical value of that function. It is good to note that intervals where a graph is increasing or decreasing should always be expressed with open intervals which are subset of it’s functions domain. A.13.2.2.3 Local Extrema A point on a graph can be called a local extremum if it is either a local minimum or local maximum (local extrema if both). A local minimum is a vertex on a graph of a continuous function where it changes from a declining state to an increasing state. Basically we are looking at a point following an interval of decrease and we refer to it as “local” because interval being considered is nearest to that vertex. A local maximum is also a vertex on a graph of a continuous function where it changes from an increasing state to a declining state. Given a function, we can locate it’s local extrema by looking at it’s critical values which as noted earlier are points where our derivative equals zero or are not defined. Generally, we can say existence of a local extrema occurs if function \\(j\\) is continuous on an interval \\((a, b)\\) and “\\(c\\)” a number within this interval has \\(j(c)\\) as a local extremum if \\(j&#39;(c)\\) is either equal to zero or is undefined. Do note, \\(j&#39;(c) = 0\\) does not necessarily mean it is a local extrema, we need to look at each critical value to determine if it is a local minimum, local maximum or neither. A.13.2.2.4 First-Derivative Evaluation This evaluation uses sign of nearby values on left and right side of a critical value for an existing derivative to establish if it is a local extrema. If sign changes from negative to positive, then it is a “local minimum”, if it changes from positive to negative, then it is a local maximum. If sign does not change and they are both negative or positive, then it is not a local extremum. op &lt;- par(c(&quot;mfrow&quot;, &quot;mar&quot;, &quot;mai&quot;)) par(mfrow = c(2, 2), mar = c(2.1, 2.1, 2.1, 2.1), mai = rep(0.2, 4)) # Local minimum plot(c(0, 4), c(0, 3), type = &quot;n&quot;, axes = FALSE, ann = FALSE) title(&quot;j(c) is a local minimum&quot;) rect(0, 0.3, 2, 2.5, col = &quot;lightblue&quot;, border = NA) rect(2, 0.3, 4, 2.5, col = &quot;chocolate2&quot;, border = NA) text(c(0.3, 1, 3), rep(2, 3), labels = c(&quot;j&#39;(x)&quot;, &quot;----&quot;, &quot;++++&quot;)) segments(0, 1.5, 3.85, 1.5) text(3.93, 1.5, &quot;x&quot;, font = 2) text(c(0.1, 3.8), rep(1.5, 2), labels = c(&quot;(&quot;, &quot;)&quot;)) text(c(0.1, 2, 3.8), rep(1.1, 3), labels = c(&quot;a&quot;, &quot;c&quot;, &quot;b&quot;)) text(c(0.3, 1.3, 3), rep(0.5, 3), labels = c(&quot;j(x)&quot;, &quot;Decreasing&quot;, &quot;Increasing&quot;)) # Local maximum plot(c(0, 4), c(0, 3), type = &quot;n&quot;, axes = FALSE, ann = FALSE) title(&quot;j(c) is a local maximum&quot;) rect(0, 0.3, 2, 2.5, col = &quot;chocolate2&quot;, border = NA) rect(2, 0.3, 4, 2.5, col = &quot;lightblue&quot;, border = NA) text(c(0.3, 1, 3), rep(2, 3), labels = c(&quot;j&#39;(x)&quot;, &quot;++++&quot;, &quot;----&quot;)) segments(0, 1.5, 3.85, 1.5) text(3.93, 1.5, &quot;x&quot;, font = 2) text(c(0.1, 3.8), rep(1.5, 2), labels = c(&quot;(&quot;, &quot;)&quot;)) text(c(0.1, 2, 3.8), rep(1.1, 2), labels = c(&quot;a&quot;, &quot;c&quot;, &quot;b&quot;)) text(c(0.3, 1.3, 3), rep(0.5, 3), labels = c(&quot;j(x)&quot;, &quot;Increasing&quot;, &quot;Decreasing&quot;)) # Neither (all negative) plot(c(0, 4), c(0, 3), type = &quot;n&quot;, axes = FALSE, ann = FALSE) title(&quot;j(c) is not a local extremum&quot;) rect(0, 0.3, 2, 2.5, col = &quot;lightblue&quot;, border = NA) rect(2, 0.3, 4, 2.5, col = &quot;lightblue&quot;, border = NA) text(c(0.3, 1, 3), rep(2, 3), labels = c(&quot;j&#39;(x)&quot;, &quot;----&quot;, &quot;----&quot;)) segments(0, 1.5, 3.85, 1.5) text(3.93, 1.5, &quot;x&quot;, font = 2) text(c(0.1, 3.8), rep(1.5, 2), labels = c(&quot;(&quot;, &quot;)&quot;)) text(c(0.1, 2, 3.8), rep(1.1, 2), labels = c(&quot;a&quot;, &quot;c&quot;, &quot;b&quot;)) text(c(0.3, 1.3, 3), rep(0.5, 3), labels = c(&quot;j(x)&quot;, &quot;Decreasing&quot;, &quot;Decreasing&quot;)) # Neither (all positive) plot(c(0, 4), c(0, 3), type = &quot;n&quot;, axes = FALSE, ann = FALSE) title(&quot;j(c) is not a local extremum&quot;) rect(0, 0.3, 2, 2.5, col = &quot;chocolate2&quot;, border = NA) rect(2, 0.3, 4, 2.5, col = &quot;chocolate2&quot;, border = NA) text(c(0.3, 1, 3), rep(2, 3), labels = c(&quot;j&#39;(x)&quot;, &quot;++++&quot;, &quot;++++&quot;)) segments(0, 1.5, 3.85, 1.5) text(3.93, 1.5, &quot;x&quot;, font =2) text(c(0.1, 3.8), rep(1.5, 2), labels = c(&quot;(&quot;, &quot;)&quot;)) text(c(0.1, 2, 3.8), rep(1.1, 2), labels = c(&quot;a&quot;, &quot;c&quot;, &quot;b&quot;)) text(c(0.3, 1.3, 3), rep(0.5, 3), labels = c(&quot;j(x)&quot;, &quot;Increasing&quot;, &quot;Increasing&quot;)) par(mfrow = op$mfrow, mar = op$mar, mai = op$mai) We can show this graphically, here we see an example of local extrema for a continuous function which occurs at points where derivative equals zero. extr1 &lt;- (2 - sqrt((-2)^2 - 4*3*(-14)))/6 extr2 &lt;- (2 + sqrt((-2)^2 - 4*3*(-14)))/6 x &lt;- sort(c(seq(-4, 4, 0.1), extr1, extr2)) ind &lt;- c(which(x == extr1), which(x == extr2)) jx &lt;- expression(x^3 - x^2 - 14*x + 11) plot(x, eval(jx), type = &quot;n&quot;, ylab = &quot;j(x)&quot;) abline(v = 0, h = 0, lty = &quot;dashed&quot;) lines(x, eval(jx), col = 4) points(c(extr1, extr2), eval(jx)[ind], pch = 21, bg = c(&quot;chocolate2&quot;, &quot;lightblue&quot;)) title(&quot;Local Extrema&quot;) text(c(extr1, extr2, 3.1), c(22, -10, 26), labels = c(&quot;Local Maximum&quot;, &quot;Local Minimum&quot;, jx), cex = c(0.7, 0.7, 0.8)) Below example shows two continuous functions whose local extrema occur at a point where it’s derivative is discontinuous. # Variables x &lt;- seq(-1, 1, 0.1) n &lt;- length(x) hx &lt;- expression(abs(x)^(1/3) + 0.5) # Coordinate plane plot(c(-4, 4), y = c(-3.5, 3.5), type = &quot;n&quot;, xlab = &quot;x&quot;, ylab = &quot;&quot;) abline(h = 0, lty = &quot;dashed&quot;) title(&quot;Local extrema with undefined derivatives&quot;) # j(x) = |x|^(1/3) #------------------- lines(x, eval(hx), col = 4) points(0, 0.5, pch = 21, bg = 4) text(3.2, 3.2, labels = expression(paste(&quot;j(x) =&quot;, sqrt(abs(x), 3), &quot;+ 0.5&quot;)), cex = 0.9) text(0, 1.7, labels = &quot;Local minimum&quot;, cex = 0.7) # Sign chart for j&#39;(x) = 1/(3*(|x|)^(2/3)) j_prime &lt;- expression(1/(3*abs(x)^(2/3))) j_prime &lt;- ifelse(x &lt; 0, -eval(j_prime), eval(j_prime)) signs &lt;- ifelse(c(j_prime[which(x == -1)], j_prime[which(x == 1)]) &lt; 0, &quot;----&quot;, &quot;++++&quot;) rect(-4.3, 3, -3.3, 3.75, col = &quot;lightblue&quot;, border = NA) rect(-3.3, 3, -2.3, 3.75, col = &quot;chocolate2&quot;, border = NA) text(c(-4, -3.3, -2.7, -2), rep(3.35, 4), labels = c(signs[1], &quot;ND&quot;, signs[2], &quot;j&#39;(x)&quot;), cex = c(0.8, 0.7, 0.8, 0.8)) # h(x) = -|x|^(1/3) #-------------------- lines(x, -eval(hx), col = 4) points(0, -0.5, pch = 21, bg = 4) text(3.2, -3.2, labels = expression(paste(&quot;h(x) =&quot;, sqrt(-abs(x), 3), &quot;+ 0.5&quot;)), cex = 0.9) text(0, -1.7, labels = &quot;Local maximum&quot;, cex = 0.7) # Sign chart for h&#39;(x) = -1/(3*(|x|)^(2/3)) h_prime2 &lt;- expression(-(1/(3*abs(x)^(2/3)))) h_prime2 &lt;- ifelse(x &lt; 0, -eval(h_prime2), eval(h_prime2)) signs2 &lt;- ifelse(c(h_prime2[which(x == -1)], h_prime2[which(x == 1)]) &lt; 0, &quot;----&quot;, &quot;++++&quot;) rect(-4.3, -3.8, -3.3, -3, col = &quot;chocolate2&quot;, border = NA) rect(-3.3, -3.8, -2.3, -3, col = &quot;lightblue&quot;, border = NA) text(c(-3.9, -3.3, -2.7, -2), rep(-3.4, 4), labels = c(signs2[1], &quot;ND&quot;, signs2[2], &quot;h&#39;(x)&quot;), cex = c(0.8, 0.7, 0.8, 0.8)) segments(rep(0, 2), c(-3.5, 2), rep(0, 2), c(-2, 3.5), lty = &quot;dashed&quot;) Below are examples of continuous functions which do not have a local extremum even though one has a point \\(h(x) = 0\\) and the other has a discontinuous point on it’s derivative. op &lt;- par(c(&quot;mfrow&quot;, &quot;mar&quot;)) par(mfrow = c(1, 2), mar = rep(2.1, 4)) # h&#39;(x) = 0 but not a local extremum x &lt;- seq(-5, 5, 0.01) plot(c(-5, 5), c(-127, 150), type = &quot;n&quot;, ann = FALSE) title(ylab = &quot;h(x)&quot;, line = 2.2) lines(x, x^3 - 2, col = 4) points(0, 0, pch = 21, bg = 4) text(-3, 148, labels = expression(paste(&quot;h(x) = &quot;, x^3))) mtext(text = &quot;Not Local Extrema&quot;, side = 3, line = 1.2, font = 2, at = c(8, 170), xpd = TRUE) mtext(&quot;h&#39;(c) = 0&quot;, font = 2) mtext(text = &quot;x&quot;, side = 1, line = 1, at = c(7, -160), xpd = TRUE, font = 2) rect(0, par(&quot;usr&quot;)[3], par(&quot;usr&quot;)[2], -110, border = NA, col = &quot;chocolate2&quot;) text(c(-0.8, 1.35, 2.7, 4.05), rep(-124.04, 4), labels = c(&quot;h&#39;(x)&quot;, &quot;++++&quot;, 0, &quot;++++&quot;)) # j&#39;(x) = c not defined and not a local extremum y &lt;- ifelse(x &lt; 0, -abs(x)^(1/3), x^(1/3)) plot(c(-5, 5), c(-2, 3), type = &quot;n&quot;, ann = FALSE) lines(x, y, col = 4) points(0, 0, pch = 21, bg = 4) text(-2.9, 2.9, labels = expression(paste(&quot;j(x) = &quot;, sqrt(x, 3)))) mtext(&quot;j&#39;(c) not defined&quot;, font = 2) rect(-0.7, par(&quot;usr&quot;)[3], par(&quot;usr&quot;)[2], -1.7, border = NA, col = &quot;chocolate2&quot;) text(c(-1.4, 0.5, 2.2, 4.05), rep(-1.95, 4), labels = c(&quot;j&#39;(x)&quot;, &quot;++++&quot;, &quot;ND&quot;, &quot;++++&quot;)) par(mfrow = op$mfrow, mar = op$mar) A.13.2.2.5 Analysing Graphs We now know how to use derivatives to plot graphs and as mentioned, this would become a handy skill in model-formation during our statistical analysis. In addition to plotting graphs, derivatives can also give us useful information about a model even without the model. For example, suppose we are told concentration of a particular drug on a “patient’s” blood stream can be modeled. We however are not given this model but we know its rate of change is given by function below. \\[C&#39;(t) = 3t^2 - 12\\] where: \\(C\\) = concentration \\(t\\) = time in hours Which has this graph: C_prime &lt;- expression(3*(t - 8)^2 - 12) t &lt;- seq(0, 16, 0.01) plot(c(0, 16), c(-20, 200), type = &quot;n&quot;, xlab = &quot;t (hrs)&quot;, ylab = &quot;Concentration&quot;) abline(h = 0, lty = &quot;dashed&quot;) lines(t, eval(C_prime), col = 4) points(c(6, 10), rep(0, 2), pch = 21, bg = 4) text(2, 196, labels = expression(paste(&quot;h&#39;(t) = &quot;, 3*(t - 8)^2 - 12))) title(&quot;Rate of change of drug concentration&quot;) Given this information, what can we say about model of this drug’s blood concentration and can we plot this model. One thing to note here is that a graph of a derivative is like a sign chart, this is because it tells us intervals where a graph is increasing and where it is decreasing. For instance, for this derivative, we can see it is positive on interval (0, 6), 0 at 6, then negative at interval (6, 10), at 10 it is zero and finally positive at interval (10, 16). Based on this, we can say drug concentration is increasing until 6 hours after being administered when it reaches a local maximum. It then starts from from 6 hours until tenth hour when it reaches local minimum. Finally it increases again up to sixteenth hour. This pattern suggests to us a third-degree polynomial as it has two extrema. We can therefore use what we now know (local extrema, starting and ending time) to sketch a graph of what we expect of this model but we definitely will not be fully accurate. con &lt;- expression((t - 8)^3 - 12*t) t &lt;- seq(4, 12, 0.01) plot(x = c(3, 15), y = c(-115, -80), type = &quot;n&quot;, ann = FALSE, yaxt = &quot;n&quot;) title(&quot;Model of drug concentration in blood stream&quot;) title(xlab = &quot;t (hours)&quot;, ylab = &quot;C(x)&quot;, line = 2) lines(t, eval(con), col = 4) extrema &lt;- c((6 - 8)^3 - 12*6, (10 - 8)^3 - 12*10) segments(x0 = c(6, 10), y0 = c(par(&quot;usr&quot;)[3], par(&quot;usr&quot;)[3]), x1 = c(6, 10), y1 = extrema, lty = &quot;dashed&quot;) segments(x0 = c(5, 9), y0 = extrema, x1 = c(7, 11), y1 = extrema) points(c(6, 10), extrema, pch = 21, bg = 4) A.13.2.3 Second derivative From our discussion on derivative and first derivative to be exact, we can now determine intervals where a graph is increasing and where it is decreasing. Now we want to know shape of a graph by looking at slope of it’s derivative (first). We established this by determining rate of change of an interval, that is, for each interval of a derivative, we want to know if it is increasing or decreasing. Intervals where a derivative is increasing tend to have a certain shape which is different from intervals where it is decreasing. As we did with derivative, we use a sign chart to indicate to us intervals where a derivative is increasing and where it is decreasing. We do this by taking a derivative of a function’s derivative, this is what we call a second derivative. There are three basic concepts to grasp as far as second derivative is concerned, these are concave upward, concave downwards and point of inflection. Concave upwards is an interval of a derivative where it is increasing either positively or negatively while concave downward is a interval where a derivative is decreasing either positively or negatively. Both concave upward and concave downward are referred to as concavity. Point where concavity changes from upward to downward or downward to upward is called point of inflection. This is a point where a change of sign occurs on a second derivative and from our preceding discussion we know partition numbers indicate points where change of sign happens. Partition numbers in this case being points where a second derivative equals zero or is non-existent. We should therefore note that point of inflection only occur at partition numbers of a second derivative, however, not all partition numbers of a second derivative are points of inflection. This is because not all partition numbers will have a change of sign on left or right side of it. We should also note that a partition number of a second derivative must be in domain of it function. Let’s look at an example to put these concepts into focus. Example Given \\(j(x) = x^3\\), it’s derivative \\(j&#39;(x) = 3x^2\\) informs us graph of \\(j\\) is increasing on interval (\\(-\\infty, 0\\)), zero at point 0 and then continues to rise on interval (\\(0, \\infty\\)). Its second derivative \\(j&#39;&#39;(x) = 6x\\) tells us this derivative is increasing on interval (\\(-\\infty, 0\\)), 0 at 0 and then decreases on interval (\\(0, \\infty\\)). From graph of \\(j\\) we can see a concave downward shape at intervals where second derivative is negative and concave downward at interval where second derivative is positive. Point of inflection is at 0. x &lt;- seq(-1.5, 1.5, 0.01) plot(c(-2, 2), c(-2, 2), type = &quot;n&quot;, ann = FALSE) abline(h = 0, lty = &quot;dashed&quot;) lines(x, x^3, lwd = 2, col = 4) title(&quot;Concavity of a graph&quot;) title(xlab = expression(paste(&quot;j(x) = &quot;, x^3)), ylab = &quot;j(x)&quot;, line = 2) text(1 - 0.2, 1^3, labels = &quot;Upwards&quot;, srt = 45, cex = 0.8) text(-1 + 0.2, (-1)^3, labels = &quot;Downwards&quot;, srt = 45, cex = 0.8) # Sign chart of j&#39;(x) rect(par(&quot;usr&quot;)[1], 1.5, -1.2, par(&quot;usr&quot;)[4], col = &quot;chocolate2&quot;, border = NA) text(c(-1.9, -1.6, -1.35, -1), rep(1.8, 4), labels = c(&quot;+++&quot;, 0, &quot;+++&quot;, &quot;j&#39;(x)&quot;)) # Sign chart of j&#39;&#39;(x) rect(1.2, par(&quot;usr&quot;)[3], 1.6, -1.5, col = &quot;lightblue&quot;, border = NA) rect(1.6, par(&quot;usr&quot;)[3], par(&quot;usr&quot;)[2], -1.5, col = &quot;chocolate2&quot;, border = NA) text(c(1, 1.35, 1.6, 1.9), rep(-1.8, 4), labels = c(&quot;j&#39;&#39;(x)&quot;, &quot;---&quot;, 0, &quot;+++&quot;)) points(0, 0, pch = 21, bg = 4) text(rep(0, 2), c(0.4, 0.2), c(&quot;Inflection&quot;, &quot;point&quot;), cex = 0.7) A.13.2.3.1 Second-derivative evaluation Suppose we do not have a graph or do not want to draw a graph of a function, but we want to locate its vertices or local extrema, then we can use second derivative to do this using what is called a second-derivate evaluation. Using this evaluation we can locate a function’s vertices called local maxima and minima. If a derivative at \\(h&#39;(a) = 0\\) and it’s second derivative \\(h&#39;&#39;(a) &gt; 0\\), then we know interval containing point \\(x = a\\) is positive and concave upwards. If left side of this derivative is negative while its right side is positive, then it is a local minimum. However, if at derivative \\(h&#39;(a) = 0\\) and its second derivative is \\(h&#39;&#39;(a) &lt; 0\\) then we know interval containing point \\(x = a\\) is negative and concave upwards. If left side of this derivative is positive while right side is negative then this would imply a local maximum. A point where derivative \\(h&#39;(a) = 0\\) and second derivative \\(h&#39;&#39;(a) = 0\\) tells us nothing, it could be a local extrema or a point of inflection hence we need to use first-derivative evaluation to determine its shape. Do take note, in our explanation above we are taking \\(a\\) as a critical value for \\(h(x)\\). Let’s look at an example to see how we bring together all these concepts of limits, interval and signs, first derivative (increasing/decreasing properties) and second derivative (concavity properties) to analyse or predict shape of a graph. Example Suppose we are given following graph of derivative of function \\(h\\) and asked to discuss and plot a possible graph of \\(h\\), what do we need to take into consideration to achieve this task. third &lt;- expression(x^3 - 2*x) third_vertices &lt;- c(-sqrt(2), -sqrt(2/3), sqrt(2/3), sqrt(2)) x &lt;- sort(c(third_vertices, seq(-2, 2, 0.01))) plot(c(-5, 5), c(-5, 5), type = &quot;n&quot;, ann = FALSE) abline(v = 0, h = 0, lty = &quot;dashed&quot;) lines(x, eval(third), col = 4) title(&quot;Derivative of function h&quot;, line = 1) title(xlab = &quot;x&quot;, ylab = &quot;h&#39;(x)&quot;, line = 2.5) points(c(third_vertices, 0), y = c(eval(third)[which(x %in% third_vertices)], 0), pch = 21, bg = 4) text(4, -4.2, labels = expression(paste(&quot;h(x) = &quot;, x^3 - 2*x)), cex = 0.9) text(third_vertices, rep(-4.5, 4), labels = c(expression(-sqrt(2)), expression(-sqrt(frac(2, 3))), expression(sqrt(frac(2, 3))), expression(sqrt(2))), cex = 0.7) From this plot we can see six intervals (\\(-\\infty, -\\sqrt{2}\\)), (\\(-\\sqrt{2}, -\\sqrt{2/3}\\)), (\\(-\\sqrt{2/3}, 0\\)), (\\(0, \\sqrt{2/3}\\)), (\\(\\sqrt{2/3}, \\sqrt{2}\\)), and (\\(\\sqrt{2}, \\infty\\)). From initial interval (\\(-\\infty, -\\sqrt{2}\\)), we note graph of \\(h\\) is negative and increasing suggesting a concave upward shape. Second interval (\\(-\\sqrt{2}, -\\sqrt{2/3}\\)) is positive and increasing suggesting graph of \\(h\\) at this interval is concave upward. Since first interval was negative and second is positive, then point \\(x = -\\sqrt{2}\\) is a local minimum. Third interval (\\(-\\sqrt{2/3}, 0\\)) is positive but decreasing suggesting graph of \\(h\\) at this interval is concave downward. Since there was no change of sign from second to third interval then point \\(x = -\\sqrt{2/3}\\) is not a local extremum but a point of inflection. Fourth interval (\\(0, \\sqrt{2/3}\\)) is negative and decreasing suggesting graph of \\(h\\) at this interval is concave downward. Since third interval was positive while this (fourth) interval is negative, then point \\(x = 0\\) is a local maximum. Fifth interval (\\(\\sqrt{2/3}, \\sqrt{2}\\)) is negative but increasing suggesting a concave upward shape. Given that there was no change in sign from fourth to fifth, then point \\(x \\sqrt {2/3}\\) is a point of inflection but not a local extremum. Sixth interval (\\(\\sqrt{2}, \\infty\\)) is positive and increasing suggesting a concave upward shape at this interval. Since fifth interval was negative and this interval is positive then point \\(x = \\sqrt{2}\\) is a local minimum. Table below summarizes all this. \\(x\\) \\(h&#39;(x)\\) Shape of \\(h\\) \\(-\\infty &lt; x &lt; -\\sqrt{2}\\) | Negative and increasing | Decreasing and concave upward \\(-\\sqrt{2}\\) | x-intercept | Local minimum \\(-\\sqrt{2} &lt; x &lt; -\\sqrt{2/3}\\)| Positive and increasing| Increasing and concave upward \\(-\\sqrt{2/3}\\) | Local maximum | Point of inflection \\(-\\sqrt{2/3} &lt; x &lt; 0\\) | Positive but decreasing | Increasing and concave downward 0 | x-intercept | Local maximum \\(0 &lt; x &lt; \\sqrt{2/3}\\) | Negative and decreasing | Decreasing and concave downward \\(\\sqrt{2/3}\\) | Local minimum | Inflection point \\(\\sqrt{2/3} &lt; x &lt; \\sqrt{2}\\) | Negative but increasing | Decreasing and concave upward \\(\\sqrt{2}\\) | x-intercept | Local minimum \\(\\sqrt{2} &lt; x &lt; \\infty\\) | Positive and increasing | Increasing and concave upward Based on information we have, and particularly our three local extrema (local minimum, local maximum and local minimum), most probable function of \\(h\\) is fourth-degree polynomial as shown below. h &lt;- expression(2*x^4 - 4*x^2 + x - 1) plot(-c(-2.5, 2.5), c(-4.2, 1.8), type = &quot;n&quot;, ann = FALSE, axes = FALSE, frame.plot = TRUE) extrema &lt;- c(-1.0574538, 0.1270510, 0.9304029) abline(v = extrema[2], lty = &quot;dashed&quot;) x &lt;- sort(c(seq(-1.7, 1.5, 0.001), extrema)) lines(x, eval(h), col = 4) x &lt;- sort(c(extrema, -0.5774538, 0.57705)) points(x, eval(h), pch = 21, bg = 4) axis(1, at = x, labels = rep(&quot;&quot;, length(x))) text(x = x, y = rep(-5.4, length(x)), labels = c(expression(-sqrt(2)), expression(-sqrt(frac(2,3))), 0, expression(sqrt(frac(2, 3))), expression(sqrt(2))), cex = c(rep(0.7, 2), 0.9, rep(0.7, 2)), xpd = TRUE) text(extrema[2], -6.3, labels = &quot;x&quot;, xpd = TRUE) text(2.6, -1, labels = &quot;Fourth-degree polynomial&quot;, srt = 90) title(&quot;Sketch of a possible graph of &#39;h&#39;&quot;) A.13.2.3.2 Curve Sketching Techniques A.13.2.3.2.1 Limits at infinity In this section we want to see what happens to a graph of a function at points where \\(x\\) increases or decreases without bound. This is a point where limit does not exist. For these we shall be looking at exponential functions, polynomial functions and rational functions. Limits at infinity for exponential functions Here we want to explore graph and limits of exponential functions as \\(x\\) increases and decreases without bound. Let’s begin by looking at exponential functions when \\(x\\) increases to an extreme value. We will do this with two exponential functions, \\(j(x) = x^2\\) and \\(h(x) = 1/x^2\\). Given \\(x\\) values one hundred, one thousand, ten thousand, and one million, let’s \\(j(x)\\) and \\(h(x)\\) to see what happens for high values of \\(x\\). x &lt;- c(100, 1000, 10000, 100000, 1000000) y1 &lt;- expression(x^2) y2 &lt;- expression(1/x^2) matrix(c(eval(y1), eval(y2)), nrow = 2, byrow = TRUE, dimnames = list(c(y1, y2), x = x)) ## x ## 100 1000 10000 1e+05 1e+06 ## x^2 1e+04 1e+06 1e+08 1e+10 1e+12 ## 1/x^2 1e-04 1e-06 1e-08 1e-10 1e-12 From this table we can see \\(x^2\\) increases as \\(x\\) increases to an unknown high value we referred to as infinity (\\(\\infty\\)). Therefore we can say as \\(x\\) increases to infinity so does \\(x^p\\). Where \\(p\\) stands for number \\(x\\) is raised to. Symbolically we can denote this as: \\[x \\to \\infty \\qquad{} \\qquad{} x^p \\to \\infty\\] or \\[\\lim_{x \\to \\infty} x^p = \\infty\\] From this table we also see that as \\(x\\) increases, \\(1/x^2\\) decreases to almost 0 but not really reaching it. Reasoning here is that dividing a constant with a very high value outputs a very small number approaching 0. Therefore we can say, as \\(x\\) increases to infinity, \\(1/x^p\\) decreases to almost 0. Symbolically we can denote this as: \\[x \\to \\infty \\qquad{} \\qquad{} \\frac{1}{x^p} \\to 0\\] or \\[\\lim_{x \\to \\infty} \\frac{1}{x^p} = 0\\] This is shown in figure below. plot(c(0, 2.1), c(0, 4.1), type = &quot;n&quot;, ann = FALSE, xaxt = &quot;n&quot;) axis(1, at = 0:2) x &lt;- seq(0, 2, 0.00001) lines(x, eval(y1), col = 4) text(1.8, 1.8^2 + 0.4, labels = expression(paste(&quot;j(x) = &quot;, x^2)), srt = 45) x &lt;- seq(0.5, 2, 0.00001) lines(x, eval(y2), col = &quot;chocolate2&quot;) text(1.8, 1/1.8^2 + 0.3, labels = expression(paste(&quot;h(x) = &quot;, 1/x^2)), srt = -15) title(&#39;Exponential functions for high &quot;x&quot; values&#39;) Note, for function \\(h\\), graph is approaching x-axis or where \\(y = 0\\). This line \\(y = 0\\) is what we refer to as horizontal asymptote as it is the value of y we are approaching as \\(x\\) increases unbound. exponential functions for decreasing \\(x\\) values has a similar pattern except no real value is obtained if \\(x\\) is negative and \\(0 &lt; p &lt; 1\\). Value of \\(p\\) also determines if \\(x^p\\) approaches \\(\\infty\\) or \\(-\\infty\\), for example, if \\(p\\) is even like 2, then it approaches \\(\\infty\\) if it is odd like 3, then it approaches \\(-\\infty\\). Other infinity limits for exponential functions are: \\[\\text{1. } \\lim_{x \\to -\\infty} \\frac{c}{x^p} = 0 \\qquad{} \\qquad{} \\text{2. } \\lim_{x \\to \\infty} \\frac{c}{x^p} = 0\\] \\[\\text{3. } \\lim_{x \\to -\\infty} cx^p = \\pm \\infty \\qquad{} \\qquad{} \\text{4. } lim_{x \\to \\infty} cx^p = \\pm \\infty\\] Where \\(p\\) is positive real number and \\(c\\) is any constant number. Limits at infinity for polynomial functions Given a polynomial, we can transform it to reciprocal for to make it convenient to note how they are when \\(x\\) approaches \\(\\infty\\) or \\(-\\infty\\). In that regard, given: \\[j(x) = 3x^5 - 2x^4 + 4x^2 + 6\\] We can factor first term hence outputting a reciprocal function \\[j(x) = 3x^5(1 - \\frac{2x^4}{3x^5} + \\frac{4x^2}{3x^5} + \\frac{6}{3x^5})\\] Limit for what in bracket thus becomes \\[\\lim_{x \\to \\infty}(1 - \\frac{2x^45}{3x^5} + \\frac{4x^2}{3x^5} + \\frac{6}{3x^5}) = 1 - 0 + 0 + 0 = 1\\] Here we used limit of exponential function of the form \\(1/x^p\\) and that of a constant discussed in our section on properties of limits. We can now say for increasing \\(x\\) towards infinity, what is in bracket is approximately 1 and when multiplied by factored term we get this factored term which is really our first term. This happens for decreasing values of \\(x\\) heading towards negative infinity. In conclusion, for any polynomial, with increasing or decreasing \\(x\\) heading towards infinity or negative infinity, its limit is the same as that of its first (highest) term. Given a polynomial of the form: \\[j(x) = a_nx^n + a_{n-1}x^{n-1} + ... + a_1x + a_0 \\qquad{} a_n \\ne 0, \\qquad{} n \\leqslant 1\\] We can symbolically represent its limit as: \\[\\lim_{x \\to \\infty} j(x) = \\lim_{x \\to \\infty} a_nx^n = \\pm \\infty\\] and \\[\\lim_{x \\to -\\infty} j(x) = \\lim_{x \\to -\\infty} a_nx^n = \\pm \\infty\\] It is useful to note that polynomials with degree greater than one cannot have a horizontal asymptote. This is because limits at infinity for a polynomial equals it’s first term which increases vertically or decreases vertically. Exception to this are polynomials of zero degrees which are constant functions with a linear graph. Limits at infinity and horizontal asymptotes for rational functions Rational functions are basically ratios of two polynomials which means we can determine their limits at infinity by getting ratio of limits of both polynomials. For example, given \\[h(x) = \\frac{3x^4 - 2x^3 + 6x + 2}{9x^4 + 4x^2 + 2x - 1}\\] we can get its limit by factoring highest term from both polynomials and reducing ratio of factored terms. \\[h(x) = \\frac{3x^4(1 - \\frac{2x^3}{3x^4} + \\frac{6x}{3x^4} + \\frac{2}{3x^4})}{9x^4(1 + \\frac{4x^2}{9x^4} + \\frac{2x}{9x^4} - \\frac{1}{9x^4})}\\] \\[\\lim_{x \\to \\infty} h(x) = \\lim_{x \\to \\infty}(\\frac{1}{3}.\\frac{1 - 0 + 0 + 0)}{1 + 0 + 0 + 0}) = \\frac{1}{3}\\] In general, for any rational function of the form: \\[h(x) = \\frac{a_mx^m + a_{m-1}x^{m-1} + ... + a_1x + a_0}{b_nx^n + b_{n-1}x^{n-1} + ... + b_1x + b_0} \\qquad{} a_m \\ne 0, \\quad{} b_n \\ne 0\\] then \\[\\lim_{x \\to infty} h(x) = \\lim_{x \\to \\infty}\\frac{a_mx^m}{b_nx^n} \\qquad{} \\text{ and } \\qquad{} \\lim_{x \\to -\\infty} h(x) = lim_{x \\to -infty} \\frac{a_mx^m}{b_nx^n}\\] From this generalization, there three possible outcomes, two of which have a horizontal asymptote. First outcome where \\(y = 0\\) or x-axis is a horizontal asymptote occurs if degree of numerator limit is less than denominator, that is \\(m &lt; n\\). Note, \\(\\lim_{x \\to \\infty} h(x) = \\lim_{x \\to -\\infty} h(x) = 0\\) Second outcome where degree of numerator is equal to denominator or \\(m = n\\) limit is given by \\(a_m/b_n\\), that is \\(\\lim_{x \\to \\infty} h(x) = \\lim_{x \\to -\\infty} h(x) = a_m/b_n\\). Line \\(y = a_m/b^n\\) is its horizontal asymptote. Third outcome is when degree of numerator is greater tan denominator or \\(m &gt; n\\), then each limit will either be \\(\\infty\\) or \\(\\infty\\) depending on values of \\(m, n, a_m, \\text{ and }, b_n\\). In this case there is no horizontal asymptote as it heads towards plus or minus infinity. Overall, a rational function can have at most one horizontal asymptote. A.13.2.3.3 Locating Vertical Asymptoes For a given point, a vertical asymptote occurs when denominator of a rational limit evaluates to zero while numerator evaluates to a value that is not zero. That is to say, if function \\[j(x) = \\frac{a(x)}{b(x)}\\] where \\(a\\) and \\(b\\) are continuous over point \\(x = h\\) and at this point denominator \\(b(x) = 0\\) while numerator \\(a(x) \\ne 0\\) then line \\(x = h\\) is a vertical asymptote of function \\(j\\). If denominator and numerator evaluate to zero, then limit is indeterminate hence need to use algebraic simplification. Example Given \\[h(x) = \\frac{x^2 + 2x + 3}{x^2 + x - 6}\\] Let \\(a(x)\\) be \\(x^2 + 2x + 3\\) and \\(b(x)\\) be \\(x^2 + x - 6\\). We can factor denominator to locate points where it evaluates to zero \\[x^ + x - 6 = (x - 2)(x + 3)\\] We have \\(x = 2\\) and \\(x = -3\\). Equating both values to our \\(a(x)\\) we note that they are both not equal to zero, hence \\(x = 2\\) and \\(x = -3\\) are vertical asymptote of \\(h\\). hx &lt;- expression((x^2 + 2*x + 3)/(x^2 + x - 6)) x &lt;- seq(-5, 5, 0.1) plot(x, eval(hx), type = &quot;l&quot;, col = 4, ann = FALSE) abline(v = c(-3, 2), lty = &quot;dashed&quot;) title(&quot;Vertical asymptotes&quot;, sub = expression(paste(&quot;hx = &quot;, (x^2 + 2*x + 3)/(x^2 + x - 6))), font.sub = 2) title(xlab = &quot;x&quot;, ylab = &quot;h(x)&quot;, line = 2) Remember, if at given point \\(a(x)\\) and \\(b(x)\\) evaluates to zero, then algebraic simplification would have to be been called for. A.13.2.3.4 Graphing Strategy In this section we summarize all we have been discussing as far as graphing and analyzing graphs is concerned. We do this in four steps which we refer to as a graphing strategy. Initial step requires us to analyze given function in terms of its domain values. These are real numbers which produce values for function. This step also requires us to locate intercepts, that is \\(y\\) and \\(x\\) intercepts. X-intercept is the point \\(x = 0\\) and y-intercept is the point where function evaluates to 0. Final step is to locate function’s asymptotes. Step two requires us to analyze derivative of a given function. We do this by identifying critical values and then constructing a sign chart for derivative function. Using this sign chart we can establish which intervals are positive and which are negative implying intervals of the function that are increasing and decreasing. Derivatives can also identify local minimum and maximum points. Step three requires us to analyze second derivative of given function. From its sign chart we can establish concavity of graph of given function as well as inflection points. Last step involves sketching graph of given function given known information Example As an example, let’s revisit our polynomial function \\(h\\) \\[h(x) = -x^5 - x^4 + 14x^3 + 6x^2 - 45x - 3\\] and see how much we can know about its graph even before plotting. Step 1 Going by our sketching strategy, initial step is to analyse \\(h\\) in terms of: a). its domain values, b). intercepts, and c). asymptotes a). Domain of \\(h\\) is a set of all real numbers \\(x\\) which produce real values for \\(h(x)\\). h &lt;- function(x) (-x)^5 - x^4 + 14*x^3 + 6*x^2 - 45*x - 3 Intercepts y-intercept or \\(h(0)\\) is equal to -3. x-intercepts are -3.49, -2.28, -0.07, 2.15 and 2.66. c). Asymptotes \\(h\\) is a polynomial function and therefore it has no horizontal nor vertical asymptote. Step 2: Analyse derivative of \\(h\\) Critical values of \\(h\\) are -3, -1.2, 1, 2.4, these are also partition numbers of its derivative \\[h&#39;(x) = -5x^4 - 4x^3 - 42x^2 + 12x - 45\\] h_p &lt;- expression((-5)*x^4 - 4*x^3 + 42*x^2 + 12*x - 45) x &lt;- c(-4, -2, 0, 2, 3) signs &lt;- ifelse(eval(h_p) &lt; 0, &quot;---&quot;, &quot;+++&quot;) Sign chart for this derivative is: plot(c(-1.2, 20), c(-2, 6), type = &quot;n&quot;, axes = FALSE, ann = FALSE) rect(0, 0, 4, 4, border = NA, col = &quot;lightblue&quot;) rect(4, 0, 8, 4, border = NA, col = &quot;chocolate2&quot;) rect(8, 0, 12, 4, border = NA, col = &quot;lightblue&quot;) rect(12, 0, 16, 4, border = NA, col = &quot;chocolate2&quot;) rect(16, 0, 20, 4, border = NA, col = &quot;lightblue&quot;) mid &lt;- c(2, 6, 10, 14, 18) text(mid, rep(3.1, length(mid)), labels = c(expression(paste(&quot;(-&quot;, infinity, &quot;, -3)&quot;)), &quot;(-3, -1.2)&quot;, &quot;(-1.2, 1)&quot;, &quot;(1, 2.4)&quot;, expression(paste(&quot;(2.4,&quot;, infinity,&quot;)&quot;))), cex = 0.9) text(c(-0.8, c(mid, 4, 8, 12, 16)), rep(2, 6), labels = c(&quot;h&#39;(x)&quot;, signs, rep(0, 4))) text(c(-0.8, mid), rep(0.5, 6), labels = c(&quot;h(x)&quot;, &quot;Decreasing&quot;, &quot;Increasing&quot;, &quot;Decreasing&quot;, &quot;Increasing&quot;, &quot;Decreasing&quot;), cex = c(1, rep(0.7, 5))) segments(0, 1.2, 20, 1.2) points(c(4, 8, 12, 16), rep(1.2, 4), pch = 20) text(20.3, 1.2, &quot;x&quot;, cex = 0.9, font = 2) text(c(4, 8, 12, 16), rep(0.5, 4), labels = round(extrema, 1), cex = 0.7, font = 2) Based on this sign chart, graph of \\(h\\) begins with a decrease then a local minimum, an increase then a local maximum, a decrease then a local minimum, an increase then a local maximum and finally a decrease. This suggests a fifth-degree polynomial. Step 3: Analyse second degree of \\(h\\) Second degree of \\(h\\) is \\[h&#39;&#39;(x) = -20x^3 -12x^2 + 8x + 12\\] Partition numbers are all x coordinates which make \\(h&#39;&#39;(x) = 0\\) and its local extrema. Below is a graph of derivative of \\(h\\) which shows partition point (approximately) -3.0, -2.3, -1.2, -0.1, 1.0, 1.8 and 2.4. x &lt;- seq(-4, 4, 0.0001) plot(c(-6, 6), c(-200, 100), type = &quot;n&quot;, ann = FALSE) abline(v = 0, h = 0, lty = &quot;dashed&quot;) lines(x, eval(h_p), col = 4) x_intercepts &lt;- c(-3, -1.23303, 1, 2.43304) points(x_intercepts, rep(0, 4), pch = 21, bg = 4) h_pp &lt;- expression((-5)*x^3 - 3*x^2 + 21*x + 3) extrema_pp &lt;- x &lt;- c(-2.30748, -0.1407, 1.84818) points(x, eval(h_p), pch = 21, bg = 4) Here is a sign chart showing concavity of graph of \\(h\\). op &lt;- par(c(&quot;mar&quot;, &quot;mai&quot;)) par(mar = rep(0, 4), mar = rep(0, 4)) p_num &lt;- sort(c(extrema, extrema_pp)) # Partition numbers x &lt;- c(-4, -2.5, -2, -1, 0, 1.5, 2, 3) # T-numbers signs &lt;- ifelse(eval(h_pp) &lt; 0, &quot;---&quot;, &quot;+++&quot;) plot(c(-1.2, 43), c(-2, 6), type = &quot;n&quot;, axes = FALSE, ann = FALSE) rect(0, 0, 5, 4, border = NA, col = &quot;chocolate2&quot;) rect(5, 0, 10, 4, border = NA, col = &quot;chocolate2&quot;) rect(10, 0, 15, 4, border = NA, col = &quot;lightblue&quot;) rect(15, 0, 20, 4, border = NA, col = &quot;lightblue&quot;) rect(20, 0, 25, 4, border = NA, col = &quot;chocolate2&quot;) rect(25, 0, 30, 4, border = NA, col = &quot;chocolate2&quot;) rect(30, 0, 35, 4, border = NA, col = &quot;lightblue&quot;) rect(35, 0, 40, 4, border = NA, col = &quot;lightblue&quot;) mid &lt;- seq(2.5, 37.5, 5) text(mid, rep(3.1, length(mid)), labels = c(expression(paste(&quot;(-&quot;, infinity, &quot;, -3)&quot;)), &quot;(-3, -2.3)&quot;, &quot;(-2.3, -1.2)&quot;, &quot;(-1.2, -0.1)&quot;, &quot;(-0.1, 1)&quot;, &quot;(1, 1.8)&quot;, &quot;(1.8 - 2.4)&quot;, expression(paste(&quot;(2.4,&quot;, infinity,&quot;)&quot;))), cex = 0.8) text(c(-1.2, c(mid, seq(5, 35, 5))), rep(2, length(mid)+7), labels = c(&quot;h&#39;&#39;(x)&quot;, signs, rep(0, 7))) text(mid, rep(0.5, 8), labels = ifelse(signs == &quot;---&quot;, &quot;Decreasing&quot;, &quot;Increasing&quot;), cex = rep(0.7, 8), srt = 45) segments(0, 1.2, 40, 1.2) points(seq(5, 35, 5), rep(1.2, 7), pch = 20) text(40.3, 1.2, &quot;x&quot;, cex = 0.9, font = 2) text(c(-1.1, seq(5, 35, 5)), rep(0.5, 4), labels = c(&quot;h&#39;(x)&quot;, round(p_num, 1)), cex = c(1, rep(0.7, 7)), font = c(1, rep(2, 7))) par(mar = op$mar, mai = op$mai) From sign chart of derivative of \\(h\\) we know interval (\\(-\\infty, 3\\)) is negative and it’s second derivative at this interval is increasing meaning shape of \\(h\\) is decreasing and concave upward at this interval. Initial interval (\\(-\\infty\\), -3) is negative on derivative of \\(h\\) and decreasing on second derivative. This suggests graph of \\(h\\) at this interval is decreasing with a concave upward shape. Interval (\\(-3, -1.2\\)) is positive on \\(h\\)’s derivative, on its second derivative it is increasing on interval (\\(-3, -2.3\\)) but decreasing on interval (\\(-2.3, -1.2\\)). This implies that graph of \\(h\\) is increasing on entire interval (\\(-3, -1.2\\)) but is concave upward on interval (-3, -2.3) while concave downward on interval (-2.3, -1.2). On interval (-1.2, 1), derivative of \\(h\\) is negative, on its second derivative, this interval is declining on interval (-1.2, -0.1) but rising on interval (-0.1, 1). This implies graph of \\(h\\) is decreasing on on entire interval (-1.2, 1) but concave downward on interval (-1.2, -0.1) while concave upward on interval (-0.1, 1) Interval (1, 2.4) is positive on derivative of \\(h\\), on its second derivative it is increasing on interval (1, 1.8) while decreasing on interval (1.8, 2.4). This suggests graph of \\(h\\) at this interval (1, 2.4) is increasing but concave upward on interval (1, 1.8) while concave downward on interval (1.8, 2.4). Final interval (2.4, \\(\\infty\\)) is negative on derivative of \\(h\\) and decreasing on its second derivative. This implies this interval is decreasing with a concave downward shape on graph of \\(h\\). Sketch graph of \\(h\\) Based on information we now have, we can plot graph of \\(h\\) as shown below. plot(rep(c(-5, 5), 2), c(rep(40, 2), rep(-40, 2)), type = &quot;n&quot;, ann = FALSE) abline(h = 0, v = 0, lty = &quot;dashed&quot;) x &lt;- xx lines(x, h(x), col = 4) x &lt;- extrema points(extrema, h(x), pch = 21, bg = 4) A.13.2.4 Optimization: Absolute Maxima and minima In statistics, one crucial activity is locating minimum and maximum quantities of given entities. For example given a cost model for construction of a school WASH (water sanitation and hygiene) project, we might be interested in minimum amount of funds we can use while maintaining quality. These points are referred to as absolute maxima and minima and locating them is problem of optimization. For function \\(j\\) with \\(x\\) as real numbers in its domain, absolute maximum is a point \\(j(a)\\) such that \\(j(a) \\geqslant j(x)\\). Similarly, an absolute minimum is a point \\(j(a)\\) such that \\(j(a) \\eqslant j(x)\\). Below is a graph with an absolute maxima. # Absolute maxima j &lt;- expression(-3*n^2) n &lt;- seq(-2, 2, 0.0001) plot(c(-5, 5), c(-10, 0.2), type = &quot;n&quot;, ann = FALSE) lines(n, eval(j), col = 4) points(0, 0, pch = 21, bg = 4) title(&quot;Plot with absolute maxima&quot;) text(5, -5, expression(paste(&quot;j(x) = &quot;, -3*x^2)), srt = 90) Here is a plot with absolute minima. # Absolute minima sixth &lt;- expression(x^6 - 7*x^4 + 14*x^2 - x - 5) x &lt;- sort(c(seq(-2.3, 2.3, 0.01), sixth_vertices)) plot(c(-5, 5), c(-5, 5), type = &quot;n&quot;, ann = FALSE) y &lt;- eval(sixth) lines(x, y, col = 4) points(x[which.min(y)], y[which.min(y)], pch = 21, bg = 4) title(&quot;Plot with absolute minima&quot;, line = 1) title(xlab = &quot;x&quot;, ylab = &quot;h(x)&quot;, line = 2) text(4.9, 0, labels = expression(paste(&quot;F(x) = &quot;, x^6 - 7*x^4 + 14*x^2 - x - 5)), cex = 0.8, srt = 90) Graph below has no absolute minima nor maxima but it has local extrema. # No absolute maximum or minimum but has local extrema extr1 &lt;- (2 - sqrt((-2)^2 - 4*3*(-14)))/6 extr2 &lt;- (2 + sqrt((-2)^2 - 4*3*(-14)))/6 x &lt;- sort(c(seq(-4, 4, 0.1), extr1, extr2)) ind &lt;- c(which(x == extr1), which(x == extr2)) jx &lt;- expression(x^3 - x^2 - 14*x + 11) plot(x, eval(jx), type = &quot;n&quot;, ylab = &quot;j(x)&quot;) abline(v = 0, h = 0, lty = &quot;dashed&quot;) lines(x, eval(jx), col = 4) points(c(extr1, extr2), eval(jx)[ind], pch = 21, bg = c(&quot;chocolate2&quot;, &quot;lightblue&quot;)) title(&quot;No absolute minima or maxima&quot;) text(c(extr1, extr2, 3.1), c(22, -10, 26), labels = c(&quot;Local Maximum&quot;, &quot;Local Minimum&quot;, jx), cex = c(0.7, 0.7, 0.8)) All three graphs above have an open interval for their domain, that is (\\(-\\infty &lt; x &lt; \\infty\\)). This means a graph can have absolute maxima or absolute minima. However, if function is continuous on a closed interval, then it must have an absolute maxima and minima. This is often the case with most statistical models, as they have intervals for which \\(x\\) should be contained for example human height and body temperature. Both absolute maxima and absolute minima occur at critical value (local extrema) or at end points. Both of these values are unique but both can occur at more that one point. Graph \\(j\\) below is continuous on closed interval [-2, 2.1]. It has one absolute minima at point \\(j(1.6) = -2.6\\) and two absolute maxima at \\(j(-1.6) = 4.6 = j(2.1)\\). fifth_vertices &lt;- c(-1.64443286, 1.64443286, 2.119174) x &lt;- sort(c(fifth_vertices, seq(-2, 2, 0.0001))) fifth &lt;- expression(x^5 - 5*x^3 + 4*x + 1) plot(c(5, -5), c(5, -5), type = &quot;n&quot;, xaxt = &quot;n&quot;, ann = FALSE) axis(side = 1, at = -5:5, labels = c(rep(&quot;&quot;, 3), -2, rep(&quot;&quot;, 3), 2.1, rep(&quot;&quot;, 3))) title(&quot;Absolute minima and maxima&quot;, line = 1) title(xlab = &quot;x&quot;, ylab = &quot;f(x)&quot;, line = 2) lines(x, eval(fifth), col = 4) x &lt;- c(-2, fifth_vertices) points(x, eval(fifth), pch = 21, bg = 4) text(c(-2, -0.9, 1.6, 2.8), c(0.1, 4.63, -3, 4.6), labels = c(&quot;j(-2) = 1&quot;, &quot;j(-1.6) = 4.6&quot;, &quot;j(1.6) = -2.6&quot;, &quot;j(2.1) = 4.6&quot;), cex = 0.7) text(c(-2, 2), rep(-5.5, 2), labels = c(&quot;[&quot;, &quot;]&quot;), xpd = TRUE, col = 4) segments(-2, -5.45, 2, -5.45, col = 4, xpd = TRUE) text(4.9, 0, labels = expression(paste(&quot;j(x) = &quot;, x^5 - 5*x^3 + 4*x + 1)), srt = 90) From this example it is clear to see that for a closed interval, locating absolute minima and maxima needs us to identify function’s critical values and end end points. From there we just need to identify minimum and maximum values. But take note this function must be continuous like a polynomial. Just to wrap this up, if we look at our function \\[x^5 - 5*x^3 + 4*x + 1\\] On a closed interval [-2, 2.1] and critical values -1.6, -0.5, 1.6, and 0.5, its absolute minimum is about -2.6 and largest value is about 4.6 which happens to fall on two places, at -1.6 and 2.1. A.13.2.5 Constant e Earlier in this chapter we mentioned number \\(e\\) as an irrational value which can be approximated by expression \\[[1 + (\\frac{1}{n})]^n\\] taken with high values of \\(n\\) Here we want to look at it from a limits perspective. That is, number \\(e\\) can be formally defined as: \\[e = \\lim_{n \\to \\infty}(1 + frac{1}{n})^n\\] Another definition of irrational number \\(e\\) is given by \\[e = \\lim_{s \\to 0}(1 + s)^(1/s)\\] At first glance one would expect limit of \\(s\\) approaching 0 to be 1 given that 1 plus any number close to 0 would output 1, however, this is not the case as taking small values of \\(s\\) approaching 0 output a value close to \\(e\\) or 2.7182818. e &lt;- expression((1 + s)^(1/s)) s &lt;- c(-0.5, -0.1, -0.01, -0.001, -0.0001, 0, 0.0001, 0.001, 0.01, 0.1, 0.5) lims &lt;- eval(e) lims[6] &lt;- exp(1) matrix(c(s, lims), nrow = 2, byrow = TRUE, dimnames = list(c(&quot;s&quot;, &quot;(1 + s)^(1/s)&quot;), rep(&quot;&quot;, length(s)))) ## ## s -0.5 -0.100000 -0.010000 -0.001000 -0.000100 0.000000 ## (1 + s)^(1/s) 4.0 2.867972 2.731999 2.719642 2.718418 2.718282 ## ## s 0.000100 0.001000 0.010000 0.100000 0.50 ## (1 + s)^(1/s) 2.718146 2.716924 2.704814 2.593742 2.25 This implies function \\((1 + s)^1/s\\) is discontinuous as s = 0. A.13.2.6 Derivative of Logarithmic and Exponential Functions Logarithms and exponents are widely applied in statistics, it is therefore important for us to discuss these functions as regards their derivatives and graphing techniques. Derivative of any exponent with base \\(e\\) is its own derivative, that is, \\[\\frac{d}{dx} e^x = e^x\\] Derivative of natural logarithm is given by \\[\\frac{d}{dx} ln x = \\frac{1}{x}\\] Let’s look at how derivative of natural logarithm is arrived at before looking at derivative of exponential function. For function \\(j\\) given as \\(j(x) = ln x\\) we can use definition of derivative \\[j&#39;(x) = \\lim_{h \\to 0} \\frac{j(x + h) - j(x)}{h}\\] to arrive at derivative of \\(j\\). We begin by simplifying our difference quotient \\[\\frac{j(x + h) - j(x)}{h} = \\frac{ln(x + h) - ln x)}{h}\\] Using sixth property of logarithms, we can change this to a fraction \\[ = \\frac{1}{h} ln \\frac{x + h}{x}\\] Multiplying by \\(x/x\\) which is simply 1 transforms this equation such that we can be able to use seventh property of logarithms. \\[ = \\frac{x}{x} . \\frac{1}{h} ln \\frac{x + h}{x}\\] Note, \\((x + h)/x\\) is the same as \\((x/x) + (h/x) = 1 + h/x\\), therefore \\[\\frac{1}{x} [\\frac{x}{h} ln(1 + \\frac{h}{x})]\\] Using seventh property we get \\[ = \\frac{1}{x} ln(1 + \\frac{h}{x})^x/h\\] Without going into locating limits and only basing this on our knowledge on deriving limits, we know \\[\\frac{d}{dx} ln x = \\lim_{h \\to 0} \\frac{j(x + h) - j(x)}{h}\\] will evaluate to \\[\\frac{d}{dx} ln x = \\frac{1}{x}\\] In the same way, we can show derivative of \\(e^x\\) is equal to itself (\\(e^x\\)). For exponential function \\(j\\) given by \\(j(x) = e^x\\), we can simplify its difference quotient as \\[\\frac{j(x + h) - j(x)}{h} = \\frac{e^(x + h) - e^x}{h}\\] Using first property of exponents we get \\[ = \\frac{e^xe^h - e^x}{h}\\] Factoring out \\(e^x\\) we get \\[ = e^x(\\frac{e^h - e^x}{h})\\] And from our computation of limits we know this will lead to \\[\\frac{d}{dx} e^x = e^x\\] This simplicity of \\(e^x\\) makes it widely applicable in many situations. If we are given the following function \\[j(x) = 3e^x - ln\\text{ }x^2\\] we can get its derivative by starting with derivative of each term. For first term \\(3e^x\\) we use derivative of a constant function rule, that is: \\[3\\frac{d}{dx}e^x\\] This should remain the same \\(3e^x\\). For our second term \\(ln\\text{ }x^2\\), seventh property of logarithms tells us logarithm of a exponential function is that exponent times log of that exponent, therefore becoming: \\[2 \\frac{d}{dx} ln \\text{ } x = \\frac{2}{x}\\] We can now say derivative of \\(j\\) is: \\[j&#39;(x) = 3e^x - \\frac{2}{x}\\] A.13.2.7 Graphing Techniques Graphing exponential and logarithmic functions is fairly simple than polynomial functions of higher degrees. However, we can use our knowledge on derivatives to get to know more about graphs of these two functions. From graph of \\(j\\) below, we can see graphs of exponential functions are increasing an concave upwards with x-axis as its horizontal asymptote because \\(\\lim_{x \\to -\\infty} e^x = 0\\). Graph \\(h\\) show us that graphs of logarithms are also increasing but at a decreasing rate thus they are concave downward. Since we cannot take logarithms of negative numbers, then domain are all positive real numbers. Logarithm of zero is a very high number approaching infinity, therefore graph of a logarithmic function has y-axis as its vertical asymptote, that is \\(\\lim_{x \\to 0^+} ln \\text{ } x = -\\infty\\). Looking at both graphs we note that they are a reflection of each other along line \\(x = y\\). It should also be evident that as \\(x\\) increases both graphs head to infinity. An interesting point to note is that, even though both graphs are increasing as x increases, graph of \\(j\\) increases more rapidly than \\(x\\) while that of \\(h\\) increases much slower as \\(x\\) increases. We therefore conclude by saying graph of exponential functions increases more rapidly than any positive exponent of x and logarithmic functions increase more slowly than any positive exponent of x. x &lt;- seq(-5, 5, 0.001) #log(x) plot(c(-5, 5), c(-5, 5), type = &quot;n&quot;, xaxt = &quot;n&quot;, ann = FALSE) axis(1, at = -5:5) abline(v = 0, h = 0, lty = &quot;dashed&quot;) lines(-5:5, -5:5, col = &quot;chocolate2&quot;, lty = &quot;dashed&quot;) lines(x[x &gt; 0], log(x[x &gt; 0]), col = 4) lines(x, exp(x), col = 4) text(1.1, exp(1.4), labels = expression(paste(&quot;j(x) = &quot;, e^x)), srt = 45) text(3.5, log(3.5)+.6, &quot;h(x) = ln x&quot;, srt = 15) A.13.3 Integration In our introduction we mentioned calculus has two main topics, differential calculus and integral calculus. We now have basics of differential calculus and therefore in this section we aim to introduce or re-introduce integral calculus. There are two types of integration, indefinite integral and definite integral. We will not go into too much detail as this chapter is aimed at re-introducing the basics of mathematics necessary for post-descriptive statistics. In this regard we shall discuss the following: Antiderivatives and Indefinite Integral Geometric-Numeric Introduction to Definite Integral Fundamental Theorem of Calculus A.13.3.1 Antiderivatives and Indefinite Integral Antiderivative involves reformulating a function from its derivative. Two functions can have similar derivatives with only difference being a constant. For example, derivative \\(6x^2\\) can have these antiderivatives: \\(2x^3\\) \\(2x^3 - 2\\) \\(2x^3 + 4\\) \\(2x^3 + \\sqrt{2}\\) Note our first antiderivative can be expressed as \\(2x^3 + 0\\) which means all four antiderivatives can be expressed as \\(2x^3 + K\\) where \\(K\\) stands for any real number. Given this fact, graphing an antiderivative of any derivative will yield in the shape of graph with only difference being a vertical shift. n &lt;- seq(-2, 2, 0.0001) j1 &lt;- expression(2*n^3) j2 &lt;- expression(2*n^3 - 2) j3 &lt;- expression(2*n^3 + 4) j4 &lt;- expression(2*n^3 + sqrt(2)) plot(c(-2.5, 2.5), c(-20, 20), type = &quot;n&quot;, ann = FALSE) lines(n, eval(j1), col = 4, lty = &quot;dashed&quot;, lwd = 2) lines(n, eval(j2), col = 5, lty = &quot;dashed&quot;, lwd = 2) lines(n, eval(j3), col = 6, lty = &quot;dashed&quot;, lwd = 2) lines(n, eval(j4), col = 7, lty = &quot;dashed&quot;, lwd = 2) title(expression(paste(&quot;Possible graphs of antiderivatives of &quot;, 6*x^2))) legend(&quot;bottomright&quot;, legend = c(j1, j2, j3, j4), lty = &quot;dashed&quot;, col = c(6:7, 4:5), lwd = 2) From preceding discussion we note antidifferentiation of a function can lead to more than one group of functions differing with just their constant. This fact can be expressed symbolically by what is referred to as indefinite integral. \\[\\int j(x) \\space{} dx\\] Symbol \\(\\int\\) is called an integral sign. For function \\(j(x)\\), we can write all groups of its antiderivatives as: \\[\\int j(x) = J(x) + K \\quad{} \\text{ if } \\quad{} J&#39;(x) = j(x)\\] Function \\(j(x)\\) here is called an integrand and symbol \\(dx\\) indicates that antiderivative is performed with respect to \\(x\\). Constant \\(K\\) is called constant of integration. For our earlier example, we can express symbolically with indefinite integral as: \\[\\int 6x^2 \\space{} dx = 2x^3 + K\\] This is because: \\[\\frac{d}{dx}(2x^3 + K) = 6x^2\\] To simplify process of getting indefinite integrals of frequently used functions four properties have been developed. These are: For a constant integral, indefinite integral is given by \\(\\int k \\space{} dx = kx + C\\) where \\(k\\) and \\(C\\) are constants. Example, \\(\\int 6 \\space{} dx = 6x + C\\) since \\(\\frac{d}{dy} (6x + C) = 6\\) When integrand is a exponential function, then its indefinite integral is given by \\(\\int x^n \\space{} dx = \\frac{x^{n+1}}{n+1}+K, \\quad{} n \\ne -1\\). Example \\(\\int x^3 \\space{} dx = \\frac{x^4}{4} + K\\). When integrand is composed of a constant and a function, then its indefinite integral is given by \\(\\int kj(x) \\space{} dx = k \\int j(x) dx\\). Example, \\(\\int 6x^2 \\space{} dx = 6 \\int x^2 = 6 . \\frac{x3}{3} = 2x^3\\). Do note, here we are moving our constant across our integral sign, this should not be done with variables like \\(\\int yx^2 \\space{} dx \\ne y \\int x^2 \\space{} dx\\). When integrand is composed of sum or difference of functions, then indefinite integral is given by \\(\\int [j(x) \\pm h(x)] \\space{}dx = \\int j(x) \\space{} dx \\pm \\int h(x) dx\\). Example \\(\\int (x^2 + x^3) \\space{} dx = \\int x^2 + \\int x^3 = \\frac{x^3}{3} + \\frac{x^4}{4}\\) Antiderivatives and indefinite integrals for exponential and logarithmic functions This functions add two more properties to our list of properties for indefinite integral properties. Fifth property is for indefinite integral for and exponential integrand. It is given by \\(\\int e^x \\space{} dx = e^x + K\\). Sixth is for a logarithmic intergrand which is \\(\\int \\frac{1}{x} \\space{} dx = ln|x| + c, \\quad{} x \\ne 0\\). Example, let us get indefinite integral of function below: \\[\\int (3e^x + \\frac{4}{x}) \\space{} dx\\] It should output this function \\[3e^x + 4 \\space{} ln|x| + K\\] One informative application of indefinite integrals is get functions given slope (derivative) and a point on its graph. For instance, if we are told a function \\(j\\) has a slope of \\(6x^2\\) and it passes through point (2, 10), then we can establish its function as \\(j(x) = 2x^3 - 6\\). This is because its indefinite integral is \\(2x^3 + K\\). We determined \\(k\\) as -6 since \\(2 * 2^2 = 16\\) and not 10. A.13.3.2 Introduction to Definite Integral To fully comprehend concept of probability which is a core concept in statistics we need to know how to determine area under a curve. Definite integral provides us with tools to handle these kind of issues. In particular, we will use definite integral to determine an area bounded by graph of a function, its x-axis and left and right vertical lines denoting start and end of area of interest as shown below x &lt;- seq(-2, 2, 0.0001) h &lt;- expression((-5)*x^2 + 13) plot(c(-3, 3), c(0.5, 14), type = &quot;n&quot;, xaxt = &quot;n&quot;, ann = FALSE) axis(1, at = c(-1, 1), labels = c(&quot;a&quot;, &quot;b&quot;)) left &lt;- seq(-1, 1, abs(-1 - 1)/50) rect(xleft = left[-length(left)], ybottom = rep(par(&quot;usr&quot;)[3], length(left)), xright = left[-1], ytop = (-5)*left[-length(left)]^2 + 13, border = NA, col = &quot;chocolate2&quot;) segments(c(-1, 1), c(par(&quot;usr&quot;)[3], par(&quot;usr&quot;)[3]), c(-1, 1), c(-5*(-1)^2+13, -5*(1)^2+13), col = 4, lwd = 2) segments(-1, 0, 1, 0, col = 4, lwd = 2) lines(x, eval(h), col = 4, lwd = 2) legend(&quot;topright&quot;, legend = expression(paste(&quot;h(x) = &quot;, -5*x^2 + 13)), bty = &quot;n&quot;) Shaded region can be symbolically represented by: \\[\\int_{a}^{b} h(x) \\space{} dx = (\\text{Area under curve from x = a to x = b})\\] We shall begin by introducing this concept in terms of a static area followed by a dynamic distance (with respect to time) and finally as a complete total change in area. In subsequent section we will see how definite integral is related to indefinite integral (antiderivatve) through a fundamental theorem of calculus. A.13.3.2.1 Area To get area under a curve or graph we need to divide that area into equal rectangles and total their areas. Since a graph’s curve could understate or overstate area when rectangles are used, then our objective is to find an optimal number of rectangles which cover area under curve without overestimating or underestimating it to much. We do this by defining an approximation error which is a difference between approximated number of rectangles and actual number of rectangles needed to cover area under consideration without any under or over estimation. To grasp this concept, we will begin this discussion by looking at a simple function which is positive (on right side of Cartesian coordinate plane) and is monotonic. Monotonic means it is either increasing or decreasing over an interval. Later we will look at an example of a graph which crosses x-axis, that is, it has positive and negative values and another that is positive but not monotonic (has increasing and decreasing intervals). General concept of area under a curve Suppose we have \\(j(x) = 2x^2 + 2\\) and we want to find area bounded by graph of this function, x-axis and vertical lines at x = 1 and 2.5 as shown below, basically we want to determine area of “chocolate” colored region. x &lt;- seq(0.1, 5, 0.0001) j &lt;- expression(2*x^2 + 2) plot(c(0.1, 4), c(0, 14.6), type = &quot;n&quot;, xaxt = &quot;n&quot;, ann = FALSE) axis(1, at = c(1, 2.5), labels = c(1, 2.5)) lines(x, eval(j), col = 4, lwd = 2) legend(&quot;topright&quot;, legend = expression(paste(&quot;j(x) = &quot;, 2*x^2 + 2))) xleft &lt;- seq(1, 2.5, (2.5-1)/60) rect(xleft = xleft[-length(xleft)], ybottom = rep(par(&quot;usr&quot;)[3], length(xleft)), xright = xleft[-1], ytop = 2*xleft[-length(xleft)]^2 + 2, border = NA, col = &quot;chocolate2&quot;) segments(1, par(&quot;usr&quot;)[3], 2.5, par(&quot;usr&quot;)[3], col = 4, lwd = 2) segments(x0 = c(1, 2.5), y0 = c(par(&quot;usr&quot;)[3], par(&quot;usr&quot;)[3]), x1 = c(1, 2.5), y1 = c(2*1^2 + 2, 2*2.5^2 + 2), col = 4, lwd = 2) As noted at start of this section, we can only do this using areas of rectangles to approximate this area given that there is no geometric function which can accomplish this. It is important to appreciate that this is an approximation an therefore not an exact area of given region. We therefore begin with approximation of given area with 5 equally spaced rectangles. To get equal rectangles we need to compute width of each rectangle such that all five rectangle are between x = 1 and x = 5. \\[\\frac{2.5 - 1}{5} = 0.3\\] Our computation gives us a width of 0.3, denoted by \\(\\Delta x\\). \\(\\Delta x\\) means change in x and is read as delta x. Height of these rectangles will be given by evaluating left hand side x value of each rectangle as shown below. plot(c(0.1, 4), c(0, 14.6), type = &quot;n&quot;, xaxt = &quot;n&quot;, ann = FALSE) axis(1, at = c(1, 2.5), labels = c(1, 2.5)) lines(x, eval(j), col = 4, lwd = 2) legend(&quot;topright&quot;, legend = expression(paste(&quot;j(x) = &quot;, 2*x^2 + 2))) xleft &lt;- seq(1, 2.5, (2.5-1)/5) rect(xleft = xleft[-length(xleft)], ybottom = rep(par(&quot;usr&quot;)[3], length(xleft)), xright = xleft[-1], ytop = 2*xleft[-length(xleft)]^2 + 2, border = &quot;chocolate&quot;, col = &quot;chocolate2&quot;) segments(1, par(&quot;usr&quot;)[3], 2.5, par(&quot;usr&quot;)[3], col = 4, lwd = 2) segments(x0 = c(1, 2.5), y0 = c(par(&quot;usr&quot;)[3], par(&quot;usr&quot;)[3]), x1 = c(1, 2.5), y1 = c(2*1^2 + 2, 2*2.5^2 + 2), col = 4, lwd = 2) Area of a rectangle is given by height times width, therefore to get area of colored region we will need to compute area of all rectangles and add them up. We will call this computation left sum because we used x values on the left hand side and denote it as \\(L_n\\) where \\(n\\) are the number of rectangles. In our case for each rectangle, height will be an evaluation left hand side x value while width would be 0.3. \\[L_{5} = j(1)*0.3 + j(1.3)*0.3 + j(1.6)*0.3 + j(1.9)*03 + j(2.2)*0.3\\] \\[= 1.2 + 1.614 + 2.136 + 2.766 + 3.504 = 11.22\\] We have arrived at an area of 11.22, but looking at our graph we can clearly see that we have fully covered colored region for which we sort to get. We have actually underestimated intended region. In general, if graph is increasing, then left sum would always underestimate given region. We can symbolically express this as: \\[11.22 = L_{5} &lt; \\int_{1}^{2.5} (2x^2 + 2) \\space{} dx = \\text{Area}\\] Since left side for an increasing function gave us an underestimate of area of interest, then we can try using right side. We will do this and superimpose on our graph to see difference. plot(c(0.1, 4), c(0, 14.6), type = &quot;n&quot;, xaxt = &quot;n&quot;, ann = FALSE) axis(1, at = c(1, 2.5), labels = c(1, 2.5)) lines(x, eval(j), col = 4, lwd = 2) legend(&quot;topright&quot;, legend = expression(paste(&quot;j(x) = &quot;, 2*x^2 + 2))) xleft &lt;- seq(1, 2.5, (2.5-1)/5) rect(xleft = xleft[-length(xleft)], ybottom = rep(par(&quot;usr&quot;)[3], length(xleft)), xright = xleft[-1], ytop = 2*xleft[-length(xleft)]^2 + 2, border = &quot;chocolate&quot;, col = &quot;chocolate2&quot;) rect(xleft = xleft[-length(xleft)], ybottom = rep(par(&quot;usr&quot;)[3], length(xleft)), xright = xleft[-1], ytop = 2*xleft[-1]^2 + 2, border = &quot;chocolate&quot;) segments(1, par(&quot;usr&quot;)[3], 2.5, par(&quot;usr&quot;)[3], col = 4, lwd = 2) segments(x0 = c(1, 2.5), y0 = c(par(&quot;usr&quot;)[3], par(&quot;usr&quot;)[3]), x1 = c(1, 2.5), y1 = c(2*1^2 + 2, 2*2.5^2 + 2), col = 4, lwd = 2) As we can see, now we have overestimated our region of interest. Its area is given by summing all areas of our rectangles with height computed using right x values. This summation is called right sum and denoted as \\(R_n\\) where \\(n\\) is number of rectangles. Right sum for five rectangles is: \\[R_5 = j(1.3)*0.3 + j(1.6)*0.3 + j(1.9)*0.3 + j(2.2)*0.3 + j(2.5)*0.3\\] \\[= 1.614 + 2.136 + 2.766 + 3.504 + 4.350 = 14.37\\] In general, if graph of function we are considering is increasing, then right sum would always be an overestimate of region of interest. Now, looking at left sum and right sum we see actual area is between these \\(L_5 = 11.22\\) and \\(R_5 = 14.37\\). We can represent this as: \\[11.22 = L_5 &lt; \\int_{1}^{2.5} (2x^2 + 2) \\space{} dx &lt; R_{5} = 14.37\\] When an unknown value is between two known values, then an average of known values can be a good approximation of unknown value. Therefore we can compute an approximate value by taking an average of \\(L_5\\) and \\(R_5\\). \\[\\text{Average} = \\frac{L_5 + R_5}{2} = \\frac{11.22 + 14.37}{2} \\approx 12.8\\] Using only five rectangles left us with an inefficient approximation of area under our curve. Suppose we increased this number (\\(n\\)) to 10 and to 100, visually we can see we are getting better approximation as \\(n\\) increases. \\[\\frac{2.5 -1}{10} = 0.15\\] plot(c(0.1, 4), c(0, 14.6), type = &quot;n&quot;, xaxt = &quot;n&quot;, ann = FALSE) axis(1, at = c(1, 2.5), labels = c(1, 2.5)) lines(x, eval(j), col = 4, lwd = 2) legend(&quot;topright&quot;, legend = c(expression(paste(&quot;j(x) = &quot;, 2*x^2 + 2)), &quot;n = 10&quot;), bty = &quot;n&quot;) xleft &lt;- seq(1, 2.5, (2.5-1)/10) rect(xleft = xleft[-length(xleft)], ybottom = rep(par(&quot;usr&quot;)[3], length(xleft)), xright = xleft[-1], ytop = 2*xleft[-length(xleft)]^2 + 2, border = &quot;chocolate&quot;, col = &quot;chocolate2&quot;) rect(xleft = xleft[-length(xleft)], ybottom = rep(par(&quot;usr&quot;)[3], length(xleft)), xright = xleft[-1], ytop = 2*xleft[-1]^2 + 2, border = &quot;chocolate&quot;) segments(1, par(&quot;usr&quot;)[3], 2.5, par(&quot;usr&quot;)[3], col = 4, lwd = 2) segments(x0 = c(1, 2.5), y0 = c(par(&quot;usr&quot;)[3], par(&quot;usr&quot;)[3]), x1 = c(1, 2.5), y1 = c(2*1^2 + 2, 2*2.5^2 + 2), col = 4, lwd = 2) \\[\\frac{2.5 - 1}{100} = 0.015\\] plot(c(0.1, 4), c(0, 14.6), type = &quot;n&quot;, xaxt = &quot;n&quot;, ann = FALSE) axis(1, at = c(1, 2.5), labels = c(1, 2.5)) lines(x, eval(j), col = 4, lwd = 2) legend(&quot;topright&quot;, legend = c(expression(paste(&quot;j(x) = &quot;, 2*x^2 + 2)), &quot;n = 100&quot;), bty = &quot;n&quot;) xleft &lt;- seq(1, 2.5, (2.5-1)/100) rect(xleft = xleft[-length(xleft)], ybottom = rep(par(&quot;usr&quot;)[3], length(xleft)), xright = xleft[-1], ytop = 2*xleft[-length(xleft)]^2 + 2, border = &quot;chocolate&quot;, col = &quot;chocolate2&quot;) rect(xleft = xleft[-length(xleft)], ybottom = rep(par(&quot;usr&quot;)[3], length(xleft)), xright = xleft[-1], ytop = 2*xleft[-1]^2 + 2, border = &quot;chocolate&quot;) segments(1, par(&quot;usr&quot;)[3], 2.5, par(&quot;usr&quot;)[3], col = 4, lwd = 2) segments(x0 = c(1, 2.5), y0 = c(par(&quot;usr&quot;)[3], par(&quot;usr&quot;)[3]), x1 = c(1, 2.5), y1 = c(2*1^2 + 2, 2*2.5^2 + 2), col = 4, lwd = 2) Computation-wise, we can get area of colored region with 16 rectangles as: \\[11.97375 = L_{10} &lt; \\int_{1}^{2.5} (2x^2 + 2) \\space{} dx &lt; R_{10} = 13.54875\\] Notice distance between \\(L_{10}\\) and \\(R_{10}\\) (approximately 1.575) is smaller than that of \\(L_5\\) and \\(R_5\\) (approximately 3.15). Average of \\(L_{10} \\text{ and } R_{10}\\) is approximately 12.8 For 100 rectangle, area is given as: \\[12.67136 = L_{100} &lt; \\int_{1}^{2.5}(2x^2 + 2) \\space{} dx &lt; R_{100} =12.82886\\] We can now see difference between \\(L_{100}\\) and \\(R_{100}\\) is much smaller (about 1.2) and has an average of 12.75011 (about 12.8). In conclusion, approximating area under a curve with \\(L_n\\) or \\(R_n\\) is bound to produce an error, these are what we see as white boxes in our graph. Therefore to make better approximation we need to set an allowable error of approximation which will be a difference between approximated value and actual value. In our subsequent section we reason out some formulas to compute error bounds for any approximation thereby enabling us to determine an ideal value of \\(n\\). Error in Approximation Let us begin by defining error from a positive monotone function which is decreasing over an interval [1, 2.5] before having a look at a positive monotone which is increasing over the same interval. For a monotone function, area under a graph or colored region is between \\(L_n\\) and \\(R_n\\). We are mentioning monotone in particular as functions that are not monotone (they are increasing or decreasing like most polynomials), will have a different method for determining approximation error. Graphically, this area between left sum and right sum is shown as uncolored rectangles. j &lt;- expression(-0.8*x^2 + 20) plot(c(0.1, 4), c(0.1, 21), type = &quot;n&quot;, xaxt = &quot;n&quot;, yaxt = &quot;n&quot;, ann = FALSE) axis(1, at = c(1, 2.5), labels = c(1, 2.5)) axis(2, at = c((-0.8)*2.5^2 + 20, (-0.8)*1^2 + 20), labels = c(&quot;b&quot;, &quot;a&quot;)) lines(x, eval(j), col = 4, lwd = 2) xleft &lt;- seq(1, 2.5, (2.5-1)/5) rect(xleft = xleft[-length(xleft)], ybottom = rep(par(&quot;usr&quot;)[3], length(xleft)), xright = xleft[-1], ytop = -0.8*xleft[-length(xleft)]^2 + 20, border = &quot;chocolate&quot;) rect(xleft = xleft[-length(xleft)], ybottom = rep(par(&quot;usr&quot;)[3], length(xleft)), xright = xleft[-1], ytop = -0.8*xleft[-1]^2 + 20, border = &quot;chocolate&quot;, col = &quot;chocolate2&quot;) segments(1, par(&quot;usr&quot;)[3], 2.5, par(&quot;usr&quot;)[3], col = 4, lwd = 2) segments(x0 = c(1, 2.5), y0 = c(par(&quot;usr&quot;)[3], par(&quot;usr&quot;)[3]), x1 = c(1, 2.5), y1 = c(-0.8*1^2 + 20, -0.8*2.5^2 + 20), col = 4, lwd = 2) segments(x0 = c(par(&quot;usr&quot;)[1], par(&quot;usr&quot;)[1]), y0 = c((-0.8)*2.5^2 + 20, (-0.8)*1^2 + 20), x1 = c(2.5, 1), y1 = c((-0.8)*2.5^2 + 20, (-0.8)*1^2 + 20), lty = 2) Since \\(\\int_{1}^{2.5} j(x) \\space{}dx\\) is in between \\(L_n\\) and \\(R_n\\), then we know error is less than uncolored region. Numerically we can define errors as a value less than absolute difference between left sum \\(L_n\\) and right sum \\(R_n\\). Therefore, for this area \\(\\int_{1}^{2.5} j(x) \\space{}dx\\) with \\(L_n =\\) 26.712 and \\(R_n =\\) 25.452 Total uncolored area is 1.26. As this total area of all uncolored rectangles, then we can also compute it as height times width. Height is distance from \\(j(2.5)\\) to \\(j(1)\\) while width is equivalent to change in \\(x\\) for each rectangle, that is: \\[|j(2.5) - j(1)| * \\frac{2.5 - 1}{5} = 1.26\\] In general, we can denote our second \\(x\\) value as \\(b\\) and our first as \\(a\\) and therefore express this area as: \\[|j(b) - j(a)|\\Delta{x}\\] where \\[\\Delta{x} = \\frac{b-a}{n}\\] \\(n = \\text{ number of rectangles}\\) From this, we note that error is less or equal (if n is high enough) to total area between \\(L_n\\) and \\(R_n\\) or total area of uncolored region. \\[\\text{Error} \\leqslant |R_n - L_n| = |j(b) - j(a)|\\Delta{x}\\] Given what we now know of error, then we can also note that absolute difference between area under curve and left or right sum would always be less or equal to \\(|R_n - L_n|\\) or \\(|j(b) - j(a)|\\Delta{x}\\). If we took an average of \\(L_n\\) and \\(R_n\\), then we know this value would be less or equal to half uncolored region or half absolute difference between \\(L_n\\) and \\(R_n\\). We can summarize all this with some formulas for monotonic functions. For a closed interval [a, b], we can denote area under a curve \\(\\int_{a}^{b} j(x) \\space{}dx\\) as \\(I\\) and average of \\(L_n\\) and \\(R_n\\) as \\(A_n\\) and therefore state: \\[|I - L_n| \\leqslant |j(b) - j(a)|\\frac{b-a}{n}\\] \\[|I - R_n| \\leqslant |j(b) - j(a)|\\frac{b-a}{n}\\] \\[|I - A_n| \\leqslant |j(b) - j(a)| \\frac{b-a}{2n}\\] We now have a complete basics of what we mean by error bounds for monotonic left and right sums, as well as their averages. Now let us look at positive monotonic increasing function and see how we use what we have discussed. Example We are given \\[h(x) = 0.5x^2 + 2 \\qquad{} 0 \\leqslant x \\leqslant 6\\] and we are asked to compute error bounds of \\(L_5\\), \\(R_5\\) and \\(A_5\\). We are also asked to determine \\(n\\) for approximation of \\(\\int_{1}^{4} (0.25x^2 + 2) \\space{} dx\\) to be within 0.05 of true value. Let us begin by graphing this function including right and left rectangles. Remember, for an increasing monotone, right rectangles underestimate area under graph while left overestimate this area, hence we need to color our left and not right rectangles for us to see difference between \\(L_n\\) and \\(R_n\\). x &lt;- seq(0, 6, 0.0001) h &lt;- expression(0.5*x^2 + 2) plot(c(1, 5.5), c(0.2, 15), type = &quot;n&quot;, xaxt = &quot;n&quot;, ann = FALSE) axis(1, at = c(1, 4), labels = c(1, 4)) lines(x, eval(h), col = 4, lwd = 2) deltax &lt;- (4-1)/5 xleft &lt;- seq(1, 4, deltax) rect(xleft = xleft[-length(xleft)], ybottom = rep(par(&quot;usr&quot;)[3], length(xleft)), xright = xleft[-1], ytop = 0.5*xleft[-length(xleft)]^2 + 2, border = &quot;chocolate&quot;, col = &quot;chocolate2&quot;) rect(xleft = xleft[-length(xleft)], ybottom = rep(par(&quot;usr&quot;)[3], length(xleft)), xright = xleft[-1], ytop = 0.5*xleft[-1]^2 + 2, border = &quot;chocolate&quot;) segments(1, par(&quot;usr&quot;)[3], 4, par(&quot;usr&quot;)[3], col = 4, lwd = 2) segments(x0 = c(1, 4), y0 = c(par(&quot;usr&quot;)[3], par(&quot;usr&quot;)[3]), x1 = c(1, 4), y1 = c(0.5*1^2 + 2, 0.5*4^2 + 2), col = 4, lwd = 2) Now we can make these computations $L_5 = $ 14.34 $R_5 = $ 18.84 $A_5 = $ 16.59 Error bound for \\(L_5\\) and \\(R_5\\) would be similar, that is \\[\\text{Error} \\leqslant |h(4) - h(1)| \\Delta{x} = |10-2.5|(0.6) = 4.5\\] Error bound for \\(A_5\\) would be: \\[\\text{Error} \\leqslant \\frac{4.5}{2} = 2.25\\] Last part of our challenge requires us to determine \\(n\\) for any \\(L_n\\) and \\(R_n\\) such that Error \\(\\leqslant 0.05\\) (5%). In this regard what we seek is total area of uncolored region to less than or equal to 0.05. \\[|h(b) - h(a)| \\frac{b - a}{n} \\leqslant 0.05\\] In putting information we know we get: \\[|10 - 2.5|*\\frac{4-1}{n} \\leqslant 0.05\\] \\[7.5*\\frac{3}{n} \\leqslant 0.05 \\] We can solve for n as \\[22.5 \\leqslant 0.05n \\qquad{} \\therefore n \\geqslant 450\\] Area under a curve for a graph crossing x-axis Graphs crossing x-axis have negative heights for intervals below x-axis, this means these intervals will have a negative area. To get total area for region below and above x-axis, their areas are summed up but after region below x-axis is multiplied by a negative to convert them to positive. As an example, let us look at \\[\\int_{1}^{3} (x^2 - 4) \\space{} dx \\] for \\(n = 5\\) and domain \\(0 \\leqslant x \\leqslant 6\\). Graph below shows two colored regions (A and B) for which we seek to get their area. x &lt;- seq(0, 6, 0.0001) jan &lt;- function(z) z^2 - 4 plot(c(0, 5), c(-5, 15), type = &quot;n&quot;, xaxt = &quot;n&quot;, ann = FALSE) axis(1, at = c(1, 3), labels = c(1, 3)) abline(h = 0, lty = 2) lines(x, jan(x), col = 4, lwd = 2) deltax &lt;- (3-1)/5 xleft &lt;- sort(c(seq(1, 3, deltax), 2)) # Rectangles below x-axis rect(xleft = xleft[1:3], ybottom = jan(xleft[1:3]), xright = xleft[2:4], ytop = c(0, 0, 0), border = &quot;chocolate&quot;) rect(xleft = xleft[2:4], ybottom = jan(xleft[2:4]), xright = xleft[1:3], ytop = c(0, 0, 0), border = &quot;chocolate&quot;, col = &quot;chocolate2&quot;) # Rectangles above x-axis rect(xleft = xleft[4:6], ybottom = c(0, 0, 0), xright = xleft[5:7], ytop = jan(xleft[4:6]), border = &quot;chocolate&quot;, col = &quot;chocolate2&quot;) rect(xleft = xleft[5:7], ybottom = c(0, 0, 0), xright = xleft[4:6], ytop = jan(xleft[5:7]), border = &quot;chocolate&quot;) segments(x0 = c(1, 1.8), c(0, 0), c(1.8, 3), c(0, 0), col = 4, lwd = 2) segments(x0 = c(1, 3), y0 = c(jan(1), 0), x1 = c(1, 3), y1 = c(0, jan(3)), col = 4, lwd = 2) text(c(1.2, 2.8), c(-1.4, 1.9), labels = c(&quot;A&quot;, &quot;B&quot;)) We begin by computing change in x or width of each rectangle $ = $ 0.4 Now let us get area under graph above x-axis (positive area) $L_{5^+} = $ 1.44 $R_{5^+} = $ 3.44 $A_{5^+} = $ 2.44 For area below graph we get $L_{5^-} = $ -2.32 $R_{5^-} = $ -1.12 $A_{5^-} = $ -1.72 As mentioned, this area is negative given its negative height. Therefore to compute total area under graph of \\(x^2 -4\\) from x = 1 to x = 3, we multiply area under x-axis with a negative sign and add it to area above x-axis. \\[\\int_{1}^{3} j(x) \\space{} dx = -A + B = -(-1.72) + 2.44 = 4.16\\] Error bound and estimation of \\(n\\) are done as we did for graphs that are all positive or negative. Area under a curve for a non-monotonic graph For positive or negative non-monotonic graphs (are increasing and decreasing), approximations from \\(L_n\\) and \\(R_n\\) would not be accurate as area under graph may not be between these two values. For this reason error estimated from such approximations would also not be accurate. In such case then, it would be good to subdivide graph into intervals such that each interval is monotonic (it is increasing or decreasing). This is where a bit of knowledge on derivatives comes in hand. A.13.3.2.2 Determining Distance Central to calculus is reversing or undoing derivatives, and in this case we want to determine distance given average rate. Recall in our introduction to derivatives, we mentioned that total distance covered at any one point in time can be obtained using instantaneous rate of change, given by difference quotient: \\[\\frac{dp}{dt} = \\lim_{\\Delta{t} \\to 0}\\frac{p(t - \\Delta{t}) - p(t)}{\\Delta{t}}\\] Where \\(p\\) is position of a moving object at time \\(t\\) and average rate is measured as: \\[\\text{Average rate }= \\frac{\\text{Total distance}}{\\text{Elapsed time}}\\] From this equation we can make distance our subject such that \\[\\text{Total distance } = \\frac{\\text{Average rate}}{\\text{Elapsed time}}\\] Using this formula, we want revisit our example on jogging rate and and distance. This time we want to establish distance covered between two intervals. Now, suppose our jogger has been jogging for about 5 hours at an average speed of 8 kilometers per hour and we want to know distance covered between her first and fourth hour, we can use our knowledge on definite integral to establish this. Joggers average rate of jogging can be expressed as \\[j(t) = 8t\\] where \\(j(t)\\) is rate in hours at the end of \\(t\\) hours. This is an creasing function which as we saw earlier, area under a graph is underestimated by \\(L_n\\) but overestimated by \\(R_n\\). Therefore to establish \\(\\int_{1}^{4} j(t) \\space{} dx\\), we need to compute area as average between \\(L_n\\) and \\(R_n\\) and compute its error bound. In this regard, for \\(n = 5\\) we can get these estimates. hg &lt;- function(t) 8*t interval &lt;- c(1, 4) n &lt;- 5 delta_t &lt;- diff(interval)/n t &lt;- seq(interval[1], interval[2], by = delta_t) L6 &lt;- hg(t[1]) * delta_t + hg(t[2]) * delta_t + hg(t[3]) * delta_t + hg(t[4]) * delta_t + hg(t[5]) * delta_t R6 &lt;- hg(t[2]) * delta_t + hg(t[3]) * delta_t + hg(t[4]) * delta_t + hg(t[5]) * delta_t + hg(t[6]) * delta_t A6 &lt;- (L6 + R6)/2 height &lt;- hg(interval[2]) - hg(interval[1]) Error_A6 &lt;- abs(height) * (diff(interval)/(2 * n)) allowed_error &lt;- 2 n_Error &lt;- ((abs(height) * diff(interval))/2)/allowed_error Width of each rectangle will be: \\[\\Delta{t} = \\frac{b-a}{n} = \\frac{4 - 1}{5} = 0.6\\] \\[L_{5} = j(1)\\Delta{t} + j(1.6)\\Delta{t} + j(2.2)\\Delta{t} + j(2.8)\\Delta{t} + j(3.4)\\Delta{t} = 52.8\\] \\[R_5 = j(1.6)\\Delta{t} + j(2.2)\\Delta{t} + j(2.8)\\Delta{t} + j(3.4)\\Delta{t} + j(4)\\Delta{t}= 67.2\\] \\[A_5 = \\frac{L_5 + R_5}{2} = \\frac{52.8 + 67.2}{2} = 60\\text{km}\\] Approximated area covered by our jogger between 1 and 4 hours is 60km. Error bound for this approximation is given by: \\[\\text{Error} \\leqslant |j(4) - j(1)|\\frac{4-1}{2.5} = 7.2\\text{km}\\] This error bound tells us our jogger could have covered a distance between 52.8Km and 67.2Km. We can represent this as: \\[\\text{Distance traveled from t = 1 to t = 4} = \\int_{1}^{4} j(t)\\space{} dt = 60 \\pm 7.2\\text{km}\\] Suppose we wanted to get an estimated area with about 2km error, we would need to solve this in equality \\[|j(4) - j(1)| \\frac{4-1}{2n} \\leqslant 2\\] This gives us \\(n \\geqslant 18\\text{km}\\) A.13.3.2.3 Total change In most practical situations and more so those we will deal with in applied statistics requires estimating total rate of change. For example, for our jogger, suppose we know her speed increases up to a certain point and then decreases until she completes, one thing we might be interested in is total area covered during her decreasing interval. To determine interval where her speed is decreasing needs us to take derivative of her jogging function. Since we are assuming a real life situation, we take it that we do not have a function, instead we have a table showing instantaneous rate of increase and decrease (\\(j(t)\\)) for her 8km jog. jogger &lt;- matrix(c(15, 20, 25, 30, 25, 20, 15, 10, 5), nrow = 1, dimnames = list(&quot;j&#39;(t)&quot;, 0:8)) jogger ## 0 1 2 3 4 5 6 7 8 ## j&#39;(t) 15 20 25 30 25 20 15 10 5 This table tells us that at start of our joggers jogging session with no distance covered she will be increasing by 15km/h, for every 1 kilometer covered she would have increased by 20km/h, for every 2 kilometers covered she would have increased by 25km/h, for every 3 kilometers covered she would have increased by 30km/h. From 4 kilometer to 8 kilometer she would increasing at a decreasing rate, that is, for every 4 kilometers she would have increased by only 25km/h, for every 5 kilometers she would increase by only 20km/h, for every 6 kilometers she would have increased by 15km/h, for every 7 kilometers she would have increased by only 10km/h, and for every 8 kilometers she would have increased by only 5km/h. Given this data, we are interested in total distance covered when on a decreasing rate, this is between \\(t = 3\\text { and } t = 8\\). jog &lt;- function(t) -0.5*t^2 + 3.5*t + 15 t &lt;- seq(0, 8, 0.001) n_t &lt;- length(t) jogged &lt;- jog(t) pt_x &lt;- c(seq(0, 3.5, length.out = 4), seq(3.5, 8, length.out = 6)) pt_y &lt;- jog(pt_x) plot(c(0.3, 8.4), c(0, 23), type = &quot;n&quot;, xaxt = &quot;n&quot;, yaxt = &quot;n&quot;, xlab = &quot;Distance (km)&quot;, ylab = &quot;Rate of jogging&quot;) axis(1, pt_x[-4], labels = 0:(length(pt_x)-2)) axis(2, seq(1, 21, 2)) rect(xleft = t[which(t == 3.5):(n_t-1)], ybottom = rep(par(&quot;usr&quot;)[3]+0.2, length(which(t == 3.5):(n_t-1))), xright = t[t &gt; 3.5], ytop = jog(t[t &gt; 3.5]), border = &quot;chocolate&quot;, col = &quot;chocolate2&quot;) lines(t, jog(t), col = 4, lwd = 2) segments(x0 = c(3.5, 8), y0 = c(par(&quot;usr&quot;)[3], par(&quot;usr&quot;)[3]), x1 = c(3.5, 8), y1 = jog(c(3.5, 8)), col = 4, lwd = 2, lty = 2) points(pt_x, pt_y, pch = 21, bg = 4, xpd = TRUE) interval &lt;- c(3, 8) n &lt;- 4 jogger &lt;- as.vector(jogger) delta_t &lt;- diff(interval)/n L4 &lt;- jogger[4]*delta_t + jogger[5]*delta_t + jogger[6]*delta_t + jogger[7]*delta_t + jogger[8]*delta_t R4 &lt;- jogger[5]*delta_t + jogger[6]*delta_t + jogger[7]*delta_t + jogger[8]*delta_t + jogger[9]*delta_t A4 &lt;- (L4 + R4)/2 Error_A4 &lt;- abs(jogger[9] - jogger[4])*(diff(interval)/(2*n)) We will approximate this total distance covered with\\(A_n\\) which is an average of \\(L_n\\) and \\(R_n\\). We will let \\(n\\) be 4 since we are given five values of rate of increase from 4 to 8km. We will denote jogging rate of increase with \\(j(t)\\). \\(\\Delta{t}\\) which is length of each rectangle is 1.25 kilometers. \\[L_4 = 30*\\Delta{t} + 25*\\Delta{t} + 20*\\Delta{t} + 15*\\Delta{t} + 10*\\Delta{t} = 125\\text{km} \\quad{} \\text{Overestimated distance}\\] \\[R_4 = 25*\\Delta{t} + 20*\\Delta{t} + 15*\\Delta{t} + 10*\\Delta{t} + 5*\\Delta{t} = 93.75 \\quad{} \\text{Underestimated distance}\\] \\[A_4 = \\frac{L_4 + R_4}{2} = \\frac{125+93.75}{2} = 109.375\\text{km} \\quad{} \\text{Approximated (average) distance}\\] Estimated error of approximation for this average distance is given by \\[\\text{Error} \\leqslant |j&#39;(8)-j&#39;(3)|\\frac{8-3}{2.n}=|5-30|\\frac{5}{2.4}=15.625\\text{km}\\] We can now conclude this by indicating that additional or total distance covered by our jogger while at a decreasing rate is \\[\\int_{3}^{8} j&#39;(t)\\space{} dt = 109.5\\text{km} \\pm 15.6\\text{km}\\] A.13.3.3 Fundamental Theorem of Calculus In this section we will get into a bit of formal definition of definite integral although we will not go in greater depth as this chapter is only meant to serve as refresher rather than a full length introduction to Mathematics or Calculus. For a continuous function \\(j\\) on a closed interval [a, b], we note: Closed interval [a, b] contain ordered values increasing from \\(a\\) to \\(b\\) such that \\(a = x_0 &lt; x_1 &lt; .... &lt; x_{n-1} &lt; x_n = b\\) where \\(n\\) is number of sub-intervals. Length of each subinterval is given by \\(\\Delta{x_k} = x_k - x_{k-1}\\) Where \\(k\\) is number of subinterval such that \\(k = 1, 2, 3, ..., n\\) When \\(n\\) becomes really high approaching infinity, length of each subinterval tends to 0, that is, \\(\\Delta{x_k} \\to 0 \\quad{}\\text{as}\\quad{} n \\to \\infty\\). Finally we select one point between the \\(n\\) sub-intervals for which we denote as \\(c_k\\). We represent this as \\(x_{k-1} \\leqslant c_k \\leqslant x_k\\) With that we can formally define definite integral as \\[\\int_{a}^{b} j(x)\\space{}dx = \\lim_{n \\to \\infty} \\sum^{n}_{k=1} j(c_k) \\Delta{x_k}\\] We call this a definite integral of \\(j\\) from \\(a \\text{ to } b\\). Here integrand is \\(j(x)\\) and lower limit is \\(a\\) while upper limit is \\(b\\). It would be good to also note other than \\(L_n\\) and \\(R_n\\), we can compute area under a graph using midpoint sum \\(M_n\\) which follows similar computation. Just like indefinite integral properties, definite integrals have handy properties which can be used to compute value of an integral. Properties of definite integrals Zero: When lower and upper limits are similar, that is \\(a = b\\), then area is 0; \\(\\int_{a}^{b} j(x)\\space{}dx = 0\\) Reverse limits: When upper and lower limits are revered such that \\(b &lt; a\\), then area is negative; \\(\\int_{a}^{b}j(x)\\space{}dx = -\\int_{b}^{a}j(x)\\space{dx}\\) Constant multiple: Like indefinite integral, definite integral of a constant and a function is equal to constant times integral of function; \\(\\int_{a}^{b}kj(x)\\space{}dx = k \\int_{a}^{b}j(x)\\space{}dx,\\) \\(k\\) is a constant Addition: Also like indefinite integrals, integral of two function added together is integral of each function; \\(\\int_{a}^{b}[j(x) \\pm h(x)] dx = \\int_{a}^{b}j(x)\\space{}dx + \\int_{a}^{b}h(x)\\space{}dx\\) Internal addition: If an integrand can be split into two, addition of those two integrals will be similar to original integral. This means for \\(c\\) between \\(a \\text{ and } b\\); \\(\\int_{a}^{b}j(x)\\space{}dx = \\int_{a}^{c}j(x)\\space{}dx + \\int_{c}^{b}j(x)\\space{}dx\\) Examples Graph of \\(j\\) below has 4 colored areas, we will use these areas to compute subsequent integrals. jg &lt;- function(x) -x^5 - x^4 + 14*x^3 + 6*x^2 - 45*x -3 vertices &lt;- c(-3, -1.23303, 1, 2.43303) x &lt;- sort(c(seq(-4, 3.5, 0.0001), vertices)) plot(c(-5, 5), c(-40, 40), type = &quot;n&quot;, ann = FALSE, xaxt = &quot;n&quot;, yaxt = &quot;n&quot;) y &lt;- jg(x) y0 &lt;- c(-3.49, -2.28, -0.07, 2.15, 2.66) lims &lt;- sort(c(vertices, y0)) axis(1, at = y0, labels = letters[1:length(y0)]) axis(2, at = 0) # Area areas &lt;- sapply(1:(length(lims)-1), function(i) x[x &gt;= lims[i] &amp; x &lt;= lims[i+1]]) n &lt;- sapply(areas, length) # Colouring area under graph and x-axis between given limits sapply(c(1, 5), function(i) rect(xleft = areas[[i]][-n[i]], ybottom = 0, xright = areas[[i]][-i], ytop = jg(areas[[i]][-n[i]]), border = &quot;lightblue&quot;, col = &quot;lightblue&quot;)) ## [[1]] ## NULL ## ## [[2]] ## NULL sapply(c(2, 6), function(i) rect(xleft = areas[[i]][-n[i]], ybottom = 0, xright = areas[[i]][-1], ytop = jg(areas[[i]][-1]), border = &quot;lightblue&quot;, col = &quot;lightblue&quot;)) ## [[1]] ## NULL ## ## [[2]] ## NULL sapply(c(3, 7), function(i) rect(xleft = areas[[i]][-n[i]], ybottom = 0, xright = areas[[i]][-1], ytop = jg(areas[[i]][-n[i]]), border = &quot;chocolate2&quot;, col = &quot;chocolate2&quot;)) ## [[1]] ## NULL ## ## [[2]] ## NULL sapply(c(4, 8), function(i) rect(xleft = areas[[i]][-n[i]], ybottom = 0, xright = areas[[i]][-1], ytop = jg(areas[[i]][-1]), border = &quot;chocolate2&quot;, col = &quot;chocolate2&quot;)) ## [[1]] ## NULL ## ## [[2]] ## NULL # Graph of j lines(x, y, col = 4) abline(h = 0) text(y0, 0, labels = &quot;|&quot;, font = 2) legend(&quot;topright&quot;, legend = &quot;y = j(x)&quot;, bty = &quot;n&quot;, text.font = 2) text(vertices, c(-10, 17, -10, 10), labels = LETTERS[1:length(vertices)]) We are told area of these regions under graph of \\(j\\) are Area of \\(A\\) is 3.21 #Actual values will be provided later Area of \\(B\\) is 3.56 Area of \\(C\\) is 3.61 Area of \\(D\\) is 0.6 We are asked to compute these integrals: \\(\\int_{a}^{b} j(x)\\space{}dx\\) \\(\\int_{b}^{a} j(x)\\space{}dx\\) \\(\\int_{b}^{d} j(x)\\space{}dx\\) \\(\\int_{b}^{b} j(x)\\space{}dx\\) \\(\\int_{d}^{e} 4j(x)\\space{}dx\\) \\(\\int_{c}^{d} \\frac{j(x)}{^-3}\\space{}dx\\) Using properties of definite integrals we should arrive at $ = A = -3.21$ $ = -A = 3.21$ $ = B + C = 3.56 + (-3.61) = -0.05$ $ = 0$ $ = 4* D = 4 * 0.6 = 2.4$ $= = 1.2 $ Relationship between definite and indefinite integral In this section we want to look at difference and connection between definite and indefinite integral which will lead us to fundamental theorem of calculus. Definite integrals differ from indefinite integral in that definite integrals outputs a real number while indefinite integral outputs a set of functions. We can also view definite integral as a geometric notion while indefinite integral as an algebraic notion. Finally on difference, these integrals apply to different sets of functions. Core connection between definite integral and indefinite integral is that for a continuous derivative function on a closed interval, its area can also be obtained from difference of its antiderivative evaluated at end point of its interval. This means for function \\(m\\) a continuous function on closed interval \\([j, h]\\) with its antiderivative \\(M\\), this holds \\[\\int_{j}^{h} m(x)\\space{}dx = M(x)|_{j}^{h} = M(h) - M(j)\\] where \\(M&#39;(x) = m(x)\\) and \\(M(x)|_{j}^{h}\\) represents total change in \\(M(x)\\) from \\(x = j\\) and \\(x = h\\) As an example, let us evaluate this definite integral \\[\\int_{1}^{5} (2e^x + 6x^2 + \\frac{2}{x} - 3)\\space{}dx\\] Property four of definite integral leads us \\[2\\int_{1}^{5}e^x\\space{}dx + 6\\int_{1}^{5}x^2\\space{}dx + 2\\int_{1}^{5}\\frac{1}{x}\\space{}dx - \\int_{1}^{5}3\\space{}dx\\] From fundamental theorem of calculus, we can get antiderivatives and evaluate them at limits. \\[2*e^x|_{1}^{5} + 6*\\frac{x^3}{3}|_{1}^{5} + 2*ln|x||_{1}^{5} - 3*x|_{1}^{5}\\] a &lt;- (2*exp(5) - 2*exp(1)) + (6*(5^3/3) - 6*(1^3/3)) + (2*log(5) - 2*log(1)) + (3*5 - 3*1) Which we should get $(2e^5 - 2e^{1}) + (6 - 6) + (2ln(5) - 2ln(1)) - (35 - 31) $ 554.6 References [1.]{#1} Barnett, Raymond A. Applied calculus for business, economics, life sciences, and social sciences Prentice-Hall Internations 7th ed [2.]{#2} https://en.wikipedia.org/wiki/Set_theory [3.]{#3} www.mathsisfun.com [4.]{#4} https://www.khanacademy.org/math/algebra/quadratics/solving-quadratics-by-completing-the-square/a/solving-quadratic-equations-by-completing-the-square "]
]
